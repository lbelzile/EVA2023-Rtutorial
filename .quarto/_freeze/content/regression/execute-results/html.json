{
  "hash": "b4e55d7578d5c65edf393117209d7241",
  "result": {
    "markdown": "---\ntitle: \"Regression models\"\npage-layout: full\ntitle-block-banner: false\ncache: true\n---\n\n\n\nMost data encountered display various forms of nonstationarity, including trends, seasonality and covariate effects, which the extreme value distributions cannot capture without modification.\nIn environmental applications, this may be partly attributed to different weather patterns, climate change, etc.\n\nThere are multiple strategies that one can consider for modelling. The first consists in fitting a regression for the whole data and perform extreme value analysis with the residuals, as before assuming stationarity [@Eastoe.Tawn:2009]. The other, proposed by @Davison.Smith:1990, tries to incorporate covariates in the parameters $\\mu$, $\\sigma$, etc. --- fixing the shape parameters is often recommended as it is hard to estimate.\n\nGeneral linear modelling would consist in regression models, e.g.,\n\\begin{align*}\n\\mu(\\mathbf{X}) = \\beta_0 + \\beta_1 \\mathrm{X}_1 + \\cdots \\beta_p \\mathrm{X}_p,\n\\end{align*}\nand estimate as before parameters by maximum likelihood. The difficulty now is that there are more parameters to estimate and the support restriction translates into up to $n$ inequality constraints, as they must be supported for every combination of covariates found in the database. These two facts mean numerical optimization is more difficult.\n\nIn models with a relatively large number of parameters, it is useful to  include additive penalty terms to the log likelihood: for example, generalized additive models for the parameters include smooth functions, typically splines, with a penalty that controls the wiggliness of the estimated predictor functions. The latter is typically evaluated using the second-order derivative of the basis functions. \n\nIn nonstationary models, risk measures of interest are defined conditionally on the value of covariates: for example, the $1-p$ conditional return level is [@Eastoe.Tawn:2009]\n\\begin{align*}\n\\Pr(Y_t  > y \\mid \\mathbf{X}_t =\\boldsymbol{x}_t) = p\n\\end{align*}\nand the corresponding unconditional return level,\n\\begin{align*}\n\\int_{\\mathcal{X}} \\Pr(Y_t  > y \\mid \\mathbf{X}_t =\\boldsymbol{x}_t) \\mathrm{d} P(\\boldsymbol{x}_t),\n\\end{align*}\n is obtained by averaging out over the distribution of covariates that are employed in the model. For future quantities, this may or not be a sensible risk summary to compute^[What does return levels mean in a nonstationary climate? See @Rootzen.Katz:2013 for an alternative.] and may prove tricky to obtain as it requires either knowledge about the future distribution of the covariates, or else a perhaps unrealistically strong stationary assumption.\n \nSome parametrizations are better suited than others for regression modelling: for the nonstationary case, the generalized Pareto model with varying scale and shape is not stationary unless, for any $v$ greater than the original threshold $u$, \n\\begin{align*}\n\\sigma_v(\\boldsymbol{x}_t) = \\sigma_u(\\boldsymbol{x}_t) + (v-u) \\xi(\\boldsymbol{x}_t)\n\\end{align*}\nwhich, even with constant shape $\\xi$ must imply a linear or constant functional form for $\\sigma_u$. Using the inhomogeneous Poisson point process representation avoids these problems.\n\n## Generalized additive models for extremes \n\nThe function `evgam` from the eponymous package allows one to specify smooth functional forms and objective estimation of the smoothing parameters using Laplace's methods [@Wood.Pya.Safken:2016], building on the `mgcv` package of Simon Wood [@Wood:2017:mgcv].\n\nThe setup is `evgam(formula, data, family, ...)`, where formula is a list of formula for parameters (in the order location, scale, shape) and `family` is the character string for the extreme value distribution. Choices include `gev`, `gpd`, `rlarg` and `ald` for asymmetric Laplace, used in quantile regression, among other.\n\n\n::: {.cell hash='regression_cache/html/evgam-setup_bfb9136faa7620af2f6a8ffc3e0adc29'}\n\n```{.r .cell-code}\nlibrary(evgam)\ndata(frwind, package = \"mev\")\nlyon <- with(frwind,\n             xts::xts(x = S2, order.by = date))\nymax <- xts::apply.yearly(lyon, max)\nyears <- unique(lubridate::year(lyon))\nopt_gev_spl <- evgam::evgam(\n  data = data.frame(\n    syear = scale(years),\n    ymax = ymax),\n  formula = list(ymax ~ s(syear, k = 5, bs = \"cr\"),\n                 ~ 1, \n                 ~ 1),\n  family = \"gev\")\n## Summary with coefficients\nsummary(opt_gev_spl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n** Parametric terms **\n\nlocation\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    36.62       0.66   55.46   <2e-16\n\nlogscale\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)     1.36       0.12   11.23   <2e-16\n\nshape\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    -0.13       0.13   -0.99    0.162\n\n** Smooth terms **\n\nlocation\n          edf max.df Chi.sq Pr(>|t|)\ns(syear) 2.25      4   5.62   0.0776\n```\n:::\n\n```{.r .cell-code}\n## Plot splines (if any)\nplot(opt_gev_spl)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-html/evgam-setup-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## Fitted value, depend on covariates\n# predict(opt_gev_spl)\n```\n:::\n\n\nGiven the fitted model is fitted with a quadratic penalty, we can view this under the Bayesian lens as a Gaussian prior. The posterior distribution isn't available in closed-form, but we can do a Gaussian approximation at the mode on a suitable scale (e.g., log-scale) and transform them back on the data scale. We can next simulate from the approximate posterior predictive to get samples for a new combination of covariates, or for the data matrix that was used as covariates for the fitted model (default).\n\n@fig-density-post-evgam shows the output density for each time period, which is the only one that varies in time: all other are drawn from the same marginal distribution, even if simulated samples are different for each parameter combination. If the model is severely overfitted, this will be visible because the posterior standard deviation will be tiny.\n\n\n::: {.cell layout-align=\"center\" hash='regression_cache/html/fig-density-post-evgam_f747f6b93923644d8651c0aa2cfd3679'}\n\n```{.r .cell-code}\n## Simulate from the posterior of parameters\npost_sim <- simulate(opt_gev_spl, nsim = 1000L, seed = 2023)\nlibrary(ggplot2)\nggplot(\n  data = data.frame(\n    location = c(post_sim$location),\n    year = factor(rep(1:length(years), \n               length.out = prod(dim(post_sim$location))))),\n  mapping = aes(x = location,\n                color = year,\n                group = year)) +\n  geom_density() +\n  theme_minimal() +\n  viridis::scale_color_viridis(discrete = TRUE) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![Density plots of 1000 posterior samples based on a normal approximation to the posterior of the location parameter of the generalized extreme value distribution, colored by year.](regression_files/figure-html/fig-density-post-evgam-1.png){#fig-density-post-evgam fig-align='center' width=80%}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}