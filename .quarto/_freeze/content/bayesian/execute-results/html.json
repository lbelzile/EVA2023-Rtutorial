{
  "hash": "daf5dc7d1b49e0e77a62576fe7eedd55",
  "result": {
    "markdown": "---\ntitle: \"Bayesian modelling\"\npage-layout: full\ntitle-block-banner: false\n---\n\n\nIn the frequentist paradigm, we consider inference for a fixed value of the parameter that generated the data, treated as random. In Bayesian inference, we consider inference conditional on the observed data, and treat the parameter as random. This can be understood as reflecting our uncertainty about the value that generated the data from the model. To achieve this, the likelihood of the random sample $\\boldsymbol{Y}$ is combined with prior distributions for the model parameters $\\boldsymbol{\\theta} = (\\theta_1,\\ldots,\\theta_m)^\\top \\in \\boldsymbol{\\Theta}$, with prior density  $p(\\boldsymbol{\\theta})$; we use the generic notation $p(\\ldots)$ for various conditional and unconditional densities and mass functions. \n\nThe posterior distribution,\n$$\n p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y}) = \\frac{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{ \\int p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\mathrm{d}\\boldsymbol{\\theta}},\n$$ {#eq-bayes}\n is proportional, as a function of $\\boldsymbol{\\theta}$, to the product of the likelihood and the priors in the numerator, but the integral appearing in the denominator of @eq-bayes is untractable in general.  In such cases, the posterior density  $p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y})$ usually does not correspond to any well-known distribution family, and posterior inferences about the components of $\\boldsymbol{\\theta}$ further involve marginalizing out the other components. \n \n \n For instance, to obtain the posterior density $p(\\theta_1\\mid \\boldsymbol{Y})$ of the first parameter in $\\boldsymbol{\\theta}$, we have to evaluate the $(m-1)$-dimensional integral $\\int p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y})\\,\\mathrm{d}(\\theta_2,\\ldots,\\theta_m)$. If we have posterior draws from $\\boldsymbol{\\theta}$, this amounts to picking out only entries that correspond to the particular parameter of interest, e.g., $\\theta_1$. \n \n \nMost of the field of Bayesian statistics revolves around the creation of algorithms that circumvent the calculation of the normalizing constant (or else provide accurate numerical approximation of the latter) or that allow for marginalizing out all parameters except for one.\n \n## Prior specification for extremes \n \nWe first consider priors for the model parameters of extreme value distributions. These should reflect the range of plausible values and can sometimes interpreted be interpreted as penalties: for example, normal parameters in mixed models shrink values towards the overall mean or slope vectors. The more concentrated the prior mode is, the more influence the prior has. Bernstein-von Mises theorem however guarantees that, as the sample size grows, the influence of the prior is washed away unless the prior imposes restrictions on the support $\\Theta$. For example, if we take a Beta prior $\\xi \\sim \\mathsf{Be}(a,b)$ prior on $[-0.5, 0.5]$, then the posterior for $\\xi$ will be restricted to this range and shrunk towards the prior mean.\n\nSuppose we fit a generalized extreme value distribution as before. The `revdbayes` package specifies a range of prior functions, see `?revdbayes::set_prior`. It is possible to set priors for, e.g., the quantile spacing, and then map them back to the GEV parameters $\\mu, \\sigma, \\xi$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(revdbayes)\nlibrary(ggplot2)\ndata(\"frwind\", package = \"mev\")\nlyon <- with(frwind,\n             xts::xts(x = S2, order.by = date))\n# Create series of yearly maximum\nymax <- as.numeric(xts::apply.yearly(lyon, max))\n# Fit a model with a trivariate normal prior for mu, log(sigma), xi\nprior1 <- set_prior(prior = \"mdi\", model = \"gev\")\nprior2 <- set_prior(prior = \"beta\", model = \"gev\")\nprior3 <- set_prior(prior = \"norm\", \n                model = \"gev\", \n                mean = c(mean(ymax), log(sd(ymax)), 0), \n                cov = diag(c(1000, 1000, 1)))\n```\n:::\n\n\nHaving specified our prior distributions, we can use software to obtain draws from the posterior. Here, we use `revdbayes` [@revdbayes] to get exact samples using the ratio-of-uniform algorithm [@Wakefield:1991]. To see what impact priors have, we plot the marginal posterior, obtaining simply by dropping the columns for the other model parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npost_1 <- revdbayes::rpost_rcpp(\n  n = 1e4L, \n  model = \"gev\",\n  data = ymax,\n  prior = prior1,\n  nrep = 100)\npost_samp1 <- post_1$sim_vals\npost_samp2 <- revdbayes::rpost_rcpp(\n  n = 1e4L, \n  model = \"gev\",\n  data = ymax,\n  prior = prior2)$sim_vals\n# Compute marginal posterior for shape\nggplot(data = data.frame(\n  shape = c(post_samp1[,'xi'],\n            post_samp2[,'xi']),\n  prior = rep(c(\"mdi\", \"beta\"), \n              each = nrow(post_samp1))),\n  mapping = aes(x = shape,\n                col = prior, \n                group = prior)) +\n  geom_density() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Marginal posterior of GEV shape parameter for different prior distributions.](bayesian_files/figure-html/fig-posteriorxi-1.png){#fig-posteriorxi fig-align='center' width=672}\n:::\n:::\n\n\nWe are not restricted to the default parametrization: appealing to invariance of the log likelihood, and thanks to max-stability we can directly compute the marginal posterior of the expectation of the 50 year maximum. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ngev_Nmean <- function(par, N){\n  # Map parameters via GEV max-stability\n  mu <- par[1] + par[2]*(N^par[3]-1)/par[3]\n  sigma <- par[2]*N^par[3]; \n  xi <- par[3]\n  # then use formula for GEV expectation\n  ifelse(xi > 1, \n         Inf, \n  mu - sigma/xi * (1 - N^xi * gamma(1 - xi)))\n}\n# For each combination of posterior draw\n# compute functional of interest\n\n# This years the posterior distribution of 50 year mean\npost_gev_mean <- apply(post_samp1, 1, gev_Nmean, N = 50)\n# Posterior quartiles\nquantile(post_gev_mean, c(0.25, 0.5, 0.75))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     25%      50%      75% \n59.31238 67.05540 81.09692 \n```\n:::\n\n```{.r .cell-code}\n# To get a 95% credible interval, simply compute quantiles\nquantile(post_gev_mean, c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     2.5%     97.5% \n 51.93082 153.86240 \n```\n:::\n:::\n\n\nWe can see that the credible intervals are quite asymmetric.\n\nMore generally, we may be interested in prediction, which in the Bayesian paradigm arises from the posterior predictive distribution. For each posterior draw $\\boldsymbol{\\theta}_b$, we simulate new observations from the generative model, here GEV.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost_pred_samp <- revdbayes::rgev(\n  n = nrow(post_samp1),\n  loc = post_samp1[,'mu'], \n  scale = post_samp1[,'sigma'],\n  shape = post_samp1[,'xi'],\n  m = 50L) # 50 year parameters\nsummary(post_pred_samp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  42.06   49.54   52.63   55.22   57.56  259.41 \n```\n:::\n:::\n\nAs part of the Bayesian workflow [@Gabry:2019], we can also check if our model is in line with expectations by computing a summary statistic on simulate datasets from the posterior predictive, and comparing it with that of the original data. If the value for the original sample lies far into the tails of the distribution of simulated samples, this provides evidence of model misspecification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(post_1, stat = median)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](bayesian_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Loss function\n\nIn the EVA 2023 data challenge, a custom loss function for the return levels $q$ was provided, of the form\n\\begin{align*}\nL(q, \\widehat{q}(\\theta)) = \n\\begin{cases}\n0.9(0.99q - \\widehat{q}), & 0.99q > \\widehat{q} \\\\\n0, & |q - \\widehat{q}| \\leq 0.01 q\\\\\n0.1(\\widehat{q} - 1.01q), & 1.01q < \\widehat{q}.\n\\end{cases}\n\\end{align*}\nIn the Bayesian paradigm, we compute the average loss over the posterior distribution of the parameters, for given value of the return level $q_0$:\n\\begin{align*}\n r(q_0) = \\int_{\\boldsymbol{\\Theta}}L(q(\\boldsymbol{\\theta}), q_0) p (\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\nand then we seek to minimize the risk $\\mathrm{min}_{q_0 \\in \\mathbb{R}_{+}} r(q_0)$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngev_retlev <- function(par, N, p = 0.368){\n  # Map parameters via GEV max-stability\n  mu <- par[1] + par[2]*(N^par[3]-1)/par[3]\n  sigma <- par[2]*N^par[3]; \n  xi <- par[3]\n  # quantile of N-block maximum\n  mev::qgev(p = p, loc = mu, scale = sigma, shape = xi)\n}\n\n# Loss function\nloss <- function(qhat, q){\n    mean(ifelse(0.99*q > qhat,\n           0.99*(0.99*q-qhat),\n           ifelse(1.01*q < qhat,\n                  0.1*(qhat-1.01*q),\n                  0)))\n}\n# Compute the posterior of the return levels\nretlev_post <- apply(post_samp1, 1, gev_retlev, N = 50)\n# Create a grid of values over which to estimate the risk\nretlev_psi <- seq(\n  from = quantile(retlev_post, 0.2),\n  to = quantile(retlev_post, 0.99), \n  length.out = 101)\n# Create a container to store results\nrisk <- numeric(length = length(retlev_psi))\nfor(i in seq_along(risk)){\n  # Compute integral (Monte Carlo average over draws)\n risk[i] <- loss(q = retlev_post, qhat = retlev_psi[i])\n}\n# Plot loss function\nggplot(data = data.frame(\n  loss = risk, \n  retlev = retlev_psi), \n  mapping = aes(x = retlev, y = loss)) +\n  geom_line() +\n  geom_vline(xintercept = mean(retlev_post)) +\n  labs(x = \"return level\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](bayesian_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThe minimum of the loss function is returned for return levels values that are much higher than the posterior mean.\n\n\n",
    "supporting": [
      "bayesian_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}