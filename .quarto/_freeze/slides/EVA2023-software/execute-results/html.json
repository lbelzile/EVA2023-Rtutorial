{
  "hash": "1303ea6ebf9026d6433d1ead1179271d",
  "result": {
    "markdown": "---\ntitle: \"Tutorial on Statistical Computing for Extremes with **R**\"\nauthor: \"Léo Belzile\"\nsubtitle: \"Extreme Value Analysis\"\ndate: \"June 30, 2023\"\neval: true\necho: true\nformat:\n  revealjs:\n    slide-number: true\n    preview-links: auto\n    theme: [simple, hecmontreal.scss]\n    title-slide-attributes:\n      data-background-color: \"#002855\"\nbibliography: EVA2023-software.bib\n---\n\n\n\n\n\n## Plan for today {background-color=\"#92c1e9\" .white}\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n- Univariate extremes\n  - Maximum likelihood\n  - Bayesian\n  - Semiparametric methods\n- Regression models\n\n:::\n\n::: {.column width=\"40%\"}\n\n- Conditional extremes\n- Models for time series\n\n:::\n\n::::\n## Extremal type theorem\n\nConsider $Y_i$ $(i=1,2,\\ldots)$ i.i.d. with distribution $F$. \n\nIf there exist normalizing sequences $a_n>0$ and $b_n \\in \\mathbb{R}$ such that\n\\begin{align}\n \\lim_{n \\to \\infty} \\Pr\\left(\\frac{\\max_{i=1}^n Y_i - b_n}{a_n} \\leq x \\right) = G(x),\n\\label{eq:gevconv}\n\\end{align}\nfor $G$ a non-degenerate distribution, then $G$ must be **generalized extreme value** (GEV).\n\n## Generalized extreme value distribution\nWith location $\\mu \\in \\mathbb{R}$, scale $\\sigma \\in \\mathbb{R}_{+}$ and shape $\\xi \\in \\mathbb{R}$ parameters, the distribution function is\n\\begin{align*}\nG(x) =\\begin{cases}\n \\exp\\left\\{-\\left(1+\\xi \\frac{x-\\mu}{\\sigma}\\right)^{-1/\\xi}_{+}\\right\\}, & \\xi \\neq 0;\\\\\n\\exp\\left\\{-\\exp\\left(-\\frac{x-\\mu}{\\sigma}\\right)\\right\\}, & \\xi = 0,\n\\end{cases}\n\\end{align*}\nwhere $x_{+} = \\max\\{x, 0\\}$.\n\nThe support is $\\{x \\in \\mathbb{R}: \\xi(x-\\mu)/\\sigma > -1\\}$.\n\n## Max-stability property\n\nIf $Y_i \\sim \\mathsf{GEV}(\\mu, \\sigma, \\xi)$ are independent, then $$\\max_{i=1}^N Y_i \\sim \\mathsf{GEV}(\\mu_N, \\sigma_N, \\xi),$$ where\n\n- $\\mu_N = \\mu + \\sigma(N^\\xi-1)/\\xi$\n- $\\sigma_N = \\sigma N^\\xi, \\xi_N = \\xi)$\n\n(case $\\xi=0$ defined by continuity).\n\n\n\n\n\n## Block maximum\n\nWe can \n\n- partition data into blocks of roughly equal size $m$ and \n- fit a GEV distribution to the maximum of the blocks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mev)\nlibrary(xts)\nlibrary(lubridate)\ndata(frwinds, package = \"mev\")\nlyon <- with(frwind, \n             xts(x = S2, order.by = date))\n# Create series of yearly maximum\nymax <- apply.yearly(lyon, max)\n```\n:::\n\n\n## Basics of likelihoods\n\n- Denote by $\\boldsymbol{\\theta} \\in \\mathcal{S} \\subseteq \\mathbb{R}^p$ the parameter vector.\n- Assume data has joint density $f(\\boldsymbol{y}; \\boldsymbol{\\theta})$.\n- The log likelihood is $\\ell(\\boldsymbol{\\theta}) = \\log f(\\boldsymbol{y}; \\boldsymbol{\\theta})$.\n- If the $n$ observations are independent with density or mass function $f_i$, then $\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\log f_i(y_i; \\boldsymbol{\\theta})$.\n- The maximum likelihood estimate $\\widehat{\\boldsymbol{\\theta}}$ is found by maximizing (numerically) $\\ell(\\boldsymbol{\\theta})$.\n\n---\n\n\n\n## Fitting GEV using `mev` package\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopt_gev <- mev::fit.gev(xdat = ymax, show = TRUE)\n## Log-likelihood: -142 \n## \n## Estimates\n##     loc    scale    shape  \n## 36.1845   3.9429  -0.0112  \n## \n## Standard Errors\n##   loc  scale  shape  \n## 0.659  0.488  0.132  \n## \n## Optimization Information\n##   Convergence: successful \n##   Function Evaluations: 27 \n##   Gradient Evaluations: 11\nmle <- coef(opt_gev)\n```\n:::\n\n\n## Goodness-of-fit diagnostics\n\nCustom methods (`print`, `plot`, `coef`, etc.) are defined\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmethods(class = \"mev_gev\")\n## [1] anova  coef   logLik nobs   plot   print  vcov  \n## see '?methods' for accessing help and source code\npar(mfrow = c(1,2))\nplot(opt_gev)\n```\n\n::: {.cell-output-display}\n![](EVA2023-software_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n## Numerical tricks\n\n- Standardize observations (e.g., `scale`) to facilitate optimization --- the GEV is a location-scale family.\n- Even if the limit is continuous and well defined at $\\xi=0$, the log likelihood and it's derivatives involves terms of the form $\\log(1+\\xi x)$, which are numerically unstable when $\\xi \\to 0$.\n  - Pro tip: do not code the likelihood yourself! Otherwise,\n    - use high precision arithmetic, e.g., `log1p`\n    - replace the terms that blow up by Taylor series expansion near $\\xi=0$ (interpolation).\n\n## Why use maximum likelihood?\n\n- Easy to generalize to complex settings (nonstationarity, regression models, censoring, rounding, etc.)\n- Known to be asymptotically most efficient (Cramér-Rao bound), even if they can be biased.\n- Point estimators, etc. are invariant to reparametrization.\n\n## Invariance property of maximum likelihood\n\nIf $h$ is a mapping, then $h(\\widehat{\\boldsymbol{\\theta}})$ is the MLE of $h(\\boldsymbol{\\theta})$.\n\nFor example, the expected maximum for $n=50$ years of data, assuming annual maximum are exactly GEV and $\\xi < 1$, is  \\begin{align*}\\mathfrak{e}_N = h(\\mu, \\sigma, \\xi) = \\mu_N + \\sigma_N{\\Gamma(1-\\xi)-1\\}/\\xi.\n\\end{align*}\n\n\nThus, the MLE $\\widehat{\\mathfrak{e}}_N=h(\\widehat{\\mu}, \\widehat{\\sigma}, \\widehat{\\xi})$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MLE of expectation of maximum of 50 blocks\ngev.mle(xdat = ymax, args = \"Nmean\", N = 50)\n## Nmean \n##  53.4\n```\n:::\n\n\n\n\n\n## Score vector\n\nWhen the log likelihood is differentiable, the MLE is the root of the score equation, meaning $\\ell_{\\boldsymbol{\\theta}}(\\widehat{\\boldsymbol{\\theta}}) = \\partial \\ell(\\boldsymbol{\\theta}) / \\partial \\boldsymbol{\\theta} = \\boldsymbol{0}_p$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmev::gev.score(par = mle, dat = ymax) # score\n## [1]  4.24e-08 -5.94e-08 -1.91e-07\n```\n:::\n\n\n- Gradient-based algorithms exploit this feature for  optimization\n- but beware of support constraints!\n\nBest to reparametrize so that the parameter space is $\\mathbb{R}^p$ if possible.\n\n\n## Information matrix and standard errors\n\nWe can extract standard errors by taking the square root of the diagonal elements of the inverse of either\n\n- the Fisher information, $\\imath(\\boldsymbol{\\theta}) = \\mathsf{Cov}\\{\\ell_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta})\\}$ or\n- the observed information $\\jmath(\\boldsymbol{\\theta}) = - \\partial^2 \\ell(\\boldsymbol{\\theta})/ \\partial \\boldsymbol{\\theta} \\partial \\boldsymbol{\\theta}^\\top$,\n\nboth evaluated at the MLE $\\widehat{\\boldsymbol{\\theta}}$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute observed information matrix\njmat <- mev::gev.infomat(par = mle, dat = ymax)\n# Compute standard errors\nsqrt(diag(solve(jmat)))\n##   loc scale shape \n## 0.659 0.488 0.132\n# Compare with opt$std.err\n```\n:::\n\n\n## Some remarks\n\nWe may compute $j(\\widehat{\\boldsymbol{\\theta}})$ (the negative Hessian of log likelihood) numerically through finite differences.\n\nMany software implementations compute MLE via Nelder--Mead simplex algorithm: \n\n- check the gradient and/or\n- the log likelihood differences\n\nto make sure the optimisation was successful.\n\n## Properties of MLE\n\n\n- Maximum likelihood estimators are asymptotically Gaussian whenever $\\xi > -1/2$ with data in domain of attraction of extreme value distribution.\n- Consistency requires that one increases block size, etc. as $n$ increases at a particular rate depending on $F$.\n\n\n\n## Regularity conditions\n\nSome cumulants (moments of derivatives of the log likelihood) of extreme value models do not exist.\n\n- the MLE does not solve the score equation if $\\widehat{\\xi} \\leq -1$\n- MLE is not unique for $\\xi < -1$ (some combinations of $\\mu$ and $\\sigma$ yield infinite log likelihood). \n   - restrict the parameter space to $\\{\\boldsymbol{\\theta}: y_1, \\ldots, y_n \\in \\mathrm{supp}(\\boldsymbol{\\theta}), \\xi \\geq -1\\}$\n   - For GEV, MLE at boundary is $(\\widehat{\\mu}=\\overline{y}, \\widehat{\\sigma} = \\max(y) - \\overline{y}, \\xi=-1)$.\n\n## Regularity conditions\n\n   \nIf $\\widehat{\\xi} < -1/2$, can not evaluate the information matrix.\n\n- Regularity assumptions do not apply! reported  std. errors are misleading.\n- Typically faster convergence, joint limit not asymptotically normal [@Smith:1985].\n\nIn applications, shape is typically close to zero, so authors sometimes restrict $\\xi \\in (-0.5, 0.5)$. \n\nPenalization of the shape helps ensure that we get reasonable estimates in small samples.\n\n## Profile log likelihood\n\nConsider a functional of interest $\\psi$ and other parameters $\\boldsymbol{\\lambda}$, treated as nuisance.\n\nWe reparametrize the log likelihood in terms of $(\\psi, \\boldsymbol{\\lambda})$ and compute  the profile log likelihood\n\\begin{align*}\n\\ell_{\\mathrm{p}}(\\psi) = \\max_{\\boldsymbol{\\lambda}} \\ell(\\psi, \\boldsymbol{\\lambda})\n\\end{align*}\n\n## Plot of profile\n\n\n::: {.cell hash='EVA2023-software_cache/revealjs/unnamed-chunk-8_f30e0aee8299b0e122d78558d80c6415'}\n\n```{.r .cell-code}\nprof <- mev::gev.pll(param = \"Nmean\", dat = ymax, N = 50)\n```\n\n::: {.cell-output-display}\n![](EVA2023-software_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## Confidence intervals\n\nUnder regularity conditions, the likelihood ratio statistic\n\\begin{align*}\n2 \\{\\ell_{\\mathrm{p}}(\\widehat{\\psi}) - \\ell_{\\mathrm{p}}(\\psi_0)\\} \\stackrel{\\cdot}{\\sim} \\chi^2_1\n\\end{align*}\nFor the hypothesis $\\psi = \\psi_0$, a $(1-\\alpha)$ confidence interval based on the profile likelihood ratio test is \n\\begin{align*}\n\\{\\psi: 2\\{\\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta}_{\\psi})\\} \\leq  \\chi^2_1(1-\\alpha)\\}.\n\\end{align*}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(confint(prof))\n## Estimate Lower CI Upper CI \n##     53.4     47.8     73.6\n```\n:::\n\n\n## Generalized Pareto\n\nIf extremal type theorem applies, then threshold exceedances $Y-u \\mid Y>u$ follow, as $u$ tends to the upper endpoint of $F$, a generalized Pareto distribution.\n\nThe generalized Pareto distribution is\n\\begin{align*}\nH(y; \\tau, \\xi) &= \n\\begin{cases}\n1-\\left(1+\\xi {y}/{\\tau}\\right)_{+}^{-1/\\xi}, & \\xi \\neq 0,\\\\ 1-\n\\exp \\left(-{y}/{\\tau}\\right)_{+},& \\xi = 0, \n\\end{cases} \\label{eq:gpdist}\n\\end{align*}\n\n\n\n## Preprocess data\n\n- Choose a threshold $u$ (either an order statistic or a fixed quantity) and extract exceedances\n- Use @Grimshaw:1993 algorithm to reduce the 2D optimization problem to a line search.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwindlyon <- with(frwind, S2[month(date) <= 4 | month(date) >= 9])\nqulev <- 1-100/nrow(windlyon)\nu <- quantile(windlyon, 1-100/length(windlyon))\n```\n:::\n\n\n## Fitting the generalized Pareto model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopt_gp <- mev::fit.gpd(\n  xdat = windlyon, threshold = u, show = TRUE)\n## Method: Grimshaw \n## Log-likelihood: -208 \n## \n## Threshold: 33.8 \n## Number Above: 90 \n## Proportion Above: 0.008 \n## \n## Estimates\n##  scale   shape  \n## 3.5786  0.0309  \n## \n## Standard Errors\n## scale  shape  \n## 0.609  0.134  \n## \n## Optimization Information\n##   Convergence: successful\n```\n:::\n\n\n## Modelling bulk\n\nThe generalized Pareto only describes what happens above the threshold, but we can use the empirical distribution below:\n\\begin{align*}\n\\widehat{\\Pr}(Y_i \\le x) = \\sum_{i=1}^n \\mathsf{I}(Y_i \\le x)/n, \\qquad x \\leq u.\n\\end{align*}\n\nMany **splicing models** propose a (semi)parametric model for the bulk; see `evmix` package for examples\n\n## Binomial - generalized Pareto model\n\n- The binomial-generalized Pareto model includes a likelihood contribution for $\\mathsf{I}(Y_i >u) \\sim \\mathsf{Bin}(1, \\zeta_u)$, where $\\zeta_u = \\Pr(Y_i >u)$.\n- This third parameter is orthogonal to the others, and there is a closed-form solution for the MLE. \n\n\n## Block maximum vs threshold exceedances\n\n- Suppose we fit a $\\mathsf{GP}(\\tau, \\xi)$ distribution  to exceedances above $u$.\n- If there are on average $N_y$ observations per year, the distribution of the $N$-year maximum conditional on exceeding $u$ is approximately $H^{\\zeta_uNN_y}$.\n\n\n## Threshold stability\n\nMathematical basis for **extrapolation**.\n\nIf \n\\begin{align*}\nY - u \\mid Y>u \\sim \\mathsf{GP}(\\tau, \\xi),\n\\end{align*}\nthen for $\\{v >u\\in \\mathbb{R}_{+}: \\tau+\\xi (u-v)>0\\}$,  \n\\begin{align*}\nY-v \\mid Y>v \\sim \\mathsf{GP}\\{\\tau + \\xi (u-v), \\xi\\},\n\\end{align*}\nand $\\zeta_v = \\{1+\\xi(v-u)/\\tau\\}^{-1/\\xi}\\zeta_u$.\n\n\n## Threshold stability plots\n\nAssuming data are exactly generalized Pareto, expect shape parameters to be constant (up to sampling variability).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuseq <- quantile(windlyon, seq(0.9, 0.99, by = 0.01))\ntstab.gpd(windlyon, \n          method = \"profile\",\n          thresh = useq)\n```\n:::\n\n\n---\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](EVA2023-software_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n## Inhomogeneous point process\n\nLet $X_i$ i.i.d. from $F$ with lower endpoint $x^*$. \n\nConsider $a_n>0$ and $b_n \\in \\mathbb{R}$ such that the distribution of the bidimensional point process\n\\begin{align*}\nP_n =\\left\\{ \\frac{i}{n}, \\frac{X_i-b_n}{a_n}, i = 1, \\ldots, n\\right\\}\n\\end{align*}\nconverges to an inhomogeneous Poisson point process on sets of the form $(a, b) \\times (z, \\infty)$ for $0  \\leq a \\leq b \\leq 1$ and  $z>z_*=\\lim_{n \\to \\infty} \\{(x_*-b_n)/a_n\\}$.\n\n## Intensity of inhomogeneous Poisson process\n\nThe intensity\nmeasure of the limiting point process,  which gives the expected number of points falling in a set is\n\\begin{align*}\n&\\Lambda\\{(a, b) \\times (z, \\infty)\\} \n\\\\&\\quad  = (b-a)\\left(1+ \\xi \\frac{z-\\mu}{\\sigma}\\right)_{+}^{-1/\\xi} \\label{eq:pp_conv}\n\\end{align*}\nfor $\\xi \\neq 0$.\n\n\n## Likelihood of the point process\n\n\\begin{align*}\n\\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{y}) &=  (c\\sigma)^{n_u} \\prod_{i=1}^{n_u} \\left(1+\\xi\\frac{y_i-\\mu}{\\sigma}\\right)^{-1-1/\\xi}_{+} \\\\& \\times \\exp\\left\\{- c \\left(1+ \\xi \\frac{u-\\mu}{\\sigma}\\right)^{-1/\\xi}_{+}\\right\\},\n\\end{align*}\nThe constant $c$ is introduced as a way to relate the parameters of the point process likelihood to those of the GEV fitted to blocks of size $m$ observations, e.g., $c=n/m$. \n\n@Moins:arxiv propose a orthogonal reparametrization.\n\n\n## Link between parametrizations\n\nUnder the  Poisson approximation to the binomial, the expected number of observations above the threshold is\n\\begin{align*}\nc \\left\\{1+ \\xi \\left( \\frac{u-\\mu}{\\sigma}\\right)\\right\\}^{-1/\\xi} \\approx n_u. \\end{align*}\nWe can thus relate $\\mathsf{GP}(\\tau, \\xi)$ with Poisson, where MLE is\n \\begin{align*}\n\\mu_0 & \\approx u - \\sigma_0\\{(n_u/c)^{-\\widehat{\\xi}}-1\\}/\\widehat{\\xi}, \\\\\\sigma_0 &\\approx \\widehat{\\sigma}_u (n_u/c)^{\\widehat{\\xi}}, \\qquad \\xi_0 = \\widehat{\\xi}.\n\\end{align*}\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nu <- useq[4] #95%\nopt.pp <- fit.pp(\n  xdat = windlyon, \n  threshold = u, \n  np = )\n```\n:::\n\n\n\n\n## Return levels\n\n- The probability $p_l$ that a $N$-year return level is exceeded $l$ times in $N$ years of independent annual maxima is $\\mathsf{Bin}(N, 1/N)$.\n- For large $N$, a Poisson approximation yields $p_0=p_1=0.368$, $p_2=0.184$, $p_3=0.061$, etc.\n  - The probability of at least one exceedance over $N$ years is in fact roughly $0.63$.\n  - The return level corresponds to the 0.368 quantile of the $N$-year maximum distribution.\n\n\n\n## References\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}