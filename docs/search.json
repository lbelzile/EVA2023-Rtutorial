[
  {
    "objectID": "content/bayesian.html",
    "href": "content/bayesian.html",
    "title": "Bayesian modelling",
    "section": "",
    "text": "In the frequentist paradigm, we consider inference for a fixed value of the parameter that generated the data, treated as random. In Bayesian inference, we consider inference conditional on the observed data, and treat the parameter as random. This can be understood as reflecting our uncertainty about the value that generated the data from the model. To achieve this, the likelihood of the random sample \\(\\boldsymbol{Y}\\) is combined with prior distributions for the model parameters \\(\\boldsymbol{\\theta} = (\\theta_1,\\ldots,\\theta_m)^\\top \\in \\boldsymbol{\\Theta}\\), with prior density \\(p(\\boldsymbol{\\theta})\\); we use the generic notation \\(p(\\ldots)\\) for various conditional and unconditional densities and mass functions.\nThe posterior distribution, \\[\np(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y}) = \\frac{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{ \\int p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\mathrm{d}\\boldsymbol{\\theta}},\n\\tag{1}\\] is proportional, as a function of \\(\\boldsymbol{\\theta}\\), to the product of the likelihood and the priors in the numerator, but the integral appearing in the denominator of Equation 1 is intractable in general. In such cases, the posterior density \\(p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y})\\) usually does not correspond to any well-known distribution family, and posterior inferences about the components of \\(\\boldsymbol{\\theta}\\) further involve marginalizing out the other components.\nFor instance, to obtain the posterior density \\(p(\\theta_1\\mid \\boldsymbol{Y})\\) of the first parameter in \\(\\boldsymbol{\\theta}\\), we have to evaluate the \\((m-1)\\)-dimensional integral \\(\\int p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y})\\,\\mathrm{d}(\\theta_2,\\ldots,\\theta_m)\\). If we have posterior draws from \\(\\boldsymbol{\\theta}\\), this amounts to picking out only entries that correspond to the particular parameter of interest, e.g., \\(\\theta_1\\).\nMost of the field of Bayesian statistics revolves around the creation of algorithms that circumvent the calculation of the normalizing constant (or else provide accurate numerical approximation of the latter) or that allow for marginalizing out all parameters except for one."
  },
  {
    "objectID": "content/bayesian.html#prior-specification-for-extremes",
    "href": "content/bayesian.html#prior-specification-for-extremes",
    "title": "Bayesian modelling",
    "section": "Prior specification for extremes",
    "text": "Prior specification for extremes\nWe first consider priors for the model parameters of extreme value distributions. These should reflect the range of plausible values and can sometimes interpreted be interpreted as penalties: for example, normal parameters in mixed models shrink values towards the overall mean or slope vectors. The more concentrated the prior mode is, the more influence the prior has. Bernstein-von Mises theorem however guarantees that, as the sample size grows, the influence of the prior is washed away unless the prior imposes restrictions on the support \\(\\Theta\\). For example, if we take a Beta prior \\(\\xi \\sim \\mathsf{Be}(a,b)\\) prior on \\([-0.5, 0.5]\\), then the posterior for \\(\\xi\\) will be restricted to this range and shrunk towards the prior mean.\nSuppose we fit a generalized extreme value distribution as before. The revdbayes package specifies a range of prior functions, see ?revdbayes::set_prior. It is possible to set priors for, e.g., the quantile spacing, and then map them back to the GEV parameters \\(\\mu, \\sigma, \\xi\\).\n\nlibrary(revdbayes)\nlibrary(ggplot2)\ndata(\"frwind\", package = \"mev\")\nlyon &lt;- with(frwind,\n             xts::xts(x = S2, order.by = date))\n# Create series of yearly maximum\nymax &lt;- as.numeric(xts::apply.yearly(lyon, max))\n# Fit a model with a trivariate normal prior for mu, log(sigma), xi\nprior1 &lt;- set_prior(prior = \"mdi\", model = \"gev\")\nprior2 &lt;- set_prior(prior = \"beta\", model = \"gev\")\nprior3 &lt;- set_prior(prior = \"norm\", \n                model = \"gev\", \n                mean = c(mean(ymax), log(sd(ymax)), 0), \n                cov = diag(c(1000, 1000, 1)))\n\nHaving specified our prior distributions, we can use software to obtain draws from the posterior. Here, we use revdbayes (Northrop, 2023) to get exact samples using the ratio-of-uniform algorithm (Wakefield et al., 1991). To see what impact priors have, we plot the marginal posterior, obtaining simply by dropping the columns for the other model parameters.\n\npost_1 &lt;- revdbayes::rpost_rcpp(\n  n = 1e4L, \n  model = \"gev\",\n  data = ymax,\n  prior = prior1,\n  nrep = 100)\npost_samp1 &lt;- post_1$sim_vals\npost_samp2 &lt;- revdbayes::rpost_rcpp(\n  n = 1e4L, \n  model = \"gev\",\n  data = ymax,\n  prior = prior2)$sim_vals\n# Compute marginal posterior for shape\nggplot(data = data.frame(\n  shape = c(post_samp1[,'xi'],\n            post_samp2[,'xi']),\n  prior = rep(c(\"mdi\", \"beta\"), \n              each = nrow(post_samp1))),\n  mapping = aes(x = shape,\n                col = prior, \n                group = prior)) +\n  geom_density() +\n  theme_minimal()\n\n\n\n\nFigure 1: Marginal posterior of GEV shape parameter for different prior distributions.\n\n\n\n\nWe are not restricted to the default parametrization: appealing to invariance of the log likelihood, and thanks to max-stability we can directly compute the marginal posterior of the expectation of the 50 year maximum.\n\ngev_Nmean &lt;- function(par, N){\n  # Map parameters via GEV max-stability\n  mu &lt;- par[1] + par[2]*(N^par[3]-1)/par[3]\n  sigma &lt;- par[2]*N^par[3]; \n  xi &lt;- par[3]\n  # then use formula for GEV expectation\n  ifelse(xi &gt; 1, \n         Inf, \n  mu - sigma/xi * (1 - N^xi * gamma(1 - xi)))\n}\n# For each combination of posterior draw\n# compute functional of interest\n\n# This years the posterior distribution of 50 year mean\npost_gev_mean &lt;- apply(post_samp1, 1, gev_Nmean, N = 50)\n# Posterior quartiles\nquantile(post_gev_mean, c(0.25, 0.5, 0.75))\n\n     25%      50%      75% \n59.21709 67.08840 81.21272 \n\n# To get a 95% credible interval, simply compute quantiles\nquantile(post_gev_mean, c(0.025, 0.975))\n\n     2.5%     97.5% \n 52.22253 159.91206 \n\n\nWe can see that the credible intervals are quite asymmetric.\nMore generally, we may be interested in prediction, which in the Bayesian paradigm arises from the posterior predictive distribution. For each posterior draw \\(\\boldsymbol{\\theta}_b\\), we simulate new observations from the generative model, here GEV.\n\npost_pred_samp &lt;- revdbayes::rgev(\n  n = nrow(post_samp1),\n  loc = post_samp1[,'mu'], \n  scale = post_samp1[,'sigma'],\n  shape = post_samp1[,'xi'],\n  m = 50L) # 50 year parameters\nsummary(post_pred_samp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  41.53   49.65   52.69   55.38   57.42  792.15 \n\n\nAs part of the Bayesian workflow (Gabry et al., 2019), we can also check if our model is in line with expectations by computing a summary statistic on simulate datasets from the posterior predictive, and comparing it with that of the original data. If the value for the original sample lies far into the tails of the distribution of simulated samples, this provides evidence of model misspecification.\n\npp_check(post_1, stat = median)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "content/bayesian.html#loss-function",
    "href": "content/bayesian.html#loss-function",
    "title": "Bayesian modelling",
    "section": "Loss function",
    "text": "Loss function\nIn the EVA 2023 data challenge, a custom loss function for the return levels \\(q\\) was provided, of the form \\[\\begin{align*}\nL(q, \\widehat{q}(\\theta)) =\n\\begin{cases}\n0.9(0.99q - \\widehat{q}), & 0.99q &gt; \\widehat{q} \\\\\n0, & |q - \\widehat{q}| \\leq 0.01 q\\\\\n0.1(\\widehat{q} - 1.01q), & 1.01q &lt; \\widehat{q}.\n\\end{cases}\n\\end{align*}\\] In the Bayesian paradigm, we compute the average loss over the posterior distribution of the parameters, for given value of the return level \\(q_0\\): \\[\\begin{align*}\nr(q_0) = \\int_{\\boldsymbol{\\Theta}}L(q(\\boldsymbol{\\theta}), q_0) p (\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\\] and then we seek to minimize the risk, \\(\\mathrm{min}_{q_0 \\in \\mathbb{R}_{+}} r(q_0)\\)\n\ngev_retlev &lt;- function(par, N, p = 0.368){\n  # Map parameters via GEV max-stability\n  mu &lt;- par[1] + par[2]*(N^par[3]-1)/par[3]\n  sigma &lt;- par[2]*N^par[3]; \n  xi &lt;- par[3]\n  # quantile of N-block maximum\n  mev::qgev(p = p, loc = mu, scale = sigma, shape = xi)\n}\n\n# Loss function\nloss &lt;- function(qhat, q){\n    mean(ifelse(0.99*q &gt; qhat,\n           0.99*(0.99*q-qhat),\n           ifelse(1.01*q &lt; qhat,\n                  0.1*(qhat-1.01*q),\n                  0)))\n}\n# Compute the posterior of the return levels\nretlev_post &lt;- apply(post_samp1, 1, gev_retlev, N = 50)\n# Create a grid of values over which to estimate the risk\nretlev_psi &lt;- seq(\n  from = quantile(retlev_post, 0.2),\n  to = quantile(retlev_post, 0.99), \n  length.out = 101)\n# Create a container to store results\nrisk &lt;- numeric(length = length(retlev_psi))\nfor(i in seq_along(risk)){\n  # Compute integral (Monte Carlo average over draws)\n risk[i] &lt;- loss(q = retlev_post, qhat = retlev_psi[i])\n}\n# Plot loss function\nggplot(data = data.frame(\n  loss = risk, \n  retlev = retlev_psi), \n  mapping = aes(x = retlev, y = loss)) +\n  geom_line() +\n  geom_vline(xintercept = mean(retlev_post)) +\n  labs(x = \"return level\") +\n  theme_minimal()\n\n\n\n\nThe minimum of the loss function is returned for return levels values that are much higher than the posterior mean."
  },
  {
    "objectID": "content/conditionalextremes.html",
    "href": "content/conditionalextremes.html",
    "title": "Conditional extremes",
    "section": "",
    "text": "We begin by loading data and packages.\n\nlibrary(texmex)\n\nLoading required package: mvtnorm\n\n\nLoading required package: ggplot2\n\nlibrary(gridExtra)\nlibrary(GGally) #for exploratory plots\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\ndata(frwind, package = \"mev\")\n#?frwind\n\nWe will work with the standard implementation in the texmex package, which also comes with nice ggplot-graphics. Package information with vignettes for various modeling contexts can be found here.\nFirst, for exploration of data, we show bivariate scatter plots and linear correlation coefficients for the wind speed data.\n\nggpairs(frwind, columns = 2:ncol(frwind))\n\n\n\n\nLet us check chi and chi-bar coefficients to see if there is asymptotic dependence. We give an example for stations \\(S_2\\) and \\(S_3\\).\n\nchi23 = chi(frwind[, c(\"S2\", \"S3\")]) \nggplot(chi23, \n       main = c(\"Chi\"=\"Chi: S2 and S3\", \n              \"ChiBar\"=\"Chi-bar: S2 and S3\"))\n\n\n\n\nThe behavior of both curves suggest that there is asymptotic independence. We can also plot multivariate conditional Spearman’s correlation coefficients across a sliding window of values of the variables. Executing the following requires more time.\n\nbootmcs23 = bootMCS(\n  X = frwind[, c(\"S2\", \"S3\")], \n  p = seq(0.5, 0.99, by = 0.01), \n  R = 25, # nb of bootstrap replicates\n  trace = 1000) \nggplot(bootmcs23, main = \"MCS: S2 and S3\")\n\n\n\n\nThe behavior looks quite stable across different threshold levels."
  },
  {
    "objectID": "content/conditionalextremes.html#exploratory-analyses",
    "href": "content/conditionalextremes.html#exploratory-analyses",
    "title": "Conditional extremes",
    "section": "",
    "text": "We begin by loading data and packages.\n\nlibrary(texmex)\n\nLoading required package: mvtnorm\n\n\nLoading required package: ggplot2\n\nlibrary(gridExtra)\nlibrary(GGally) #for exploratory plots\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\ndata(frwind, package = \"mev\")\n#?frwind\n\nWe will work with the standard implementation in the texmex package, which also comes with nice ggplot-graphics. Package information with vignettes for various modeling contexts can be found here.\nFirst, for exploration of data, we show bivariate scatter plots and linear correlation coefficients for the wind speed data.\n\nggpairs(frwind, columns = 2:ncol(frwind))\n\n\n\n\nLet us check chi and chi-bar coefficients to see if there is asymptotic dependence. We give an example for stations \\(S_2\\) and \\(S_3\\).\n\nchi23 = chi(frwind[, c(\"S2\", \"S3\")]) \nggplot(chi23, \n       main = c(\"Chi\"=\"Chi: S2 and S3\", \n              \"ChiBar\"=\"Chi-bar: S2 and S3\"))\n\n\n\n\nThe behavior of both curves suggest that there is asymptotic independence. We can also plot multivariate conditional Spearman’s correlation coefficients across a sliding window of values of the variables. Executing the following requires more time.\n\nbootmcs23 = bootMCS(\n  X = frwind[, c(\"S2\", \"S3\")], \n  p = seq(0.5, 0.99, by = 0.01), \n  R = 25, # nb of bootstrap replicates\n  trace = 1000) \nggplot(bootmcs23, main = \"MCS: S2 and S3\")\n\n\n\n\nThe behavior looks quite stable across different threshold levels."
  },
  {
    "objectID": "content/conditionalextremes.html#modeling-conditional-extremes",
    "href": "content/conditionalextremes.html#modeling-conditional-extremes",
    "title": "Conditional extremes",
    "section": "Modeling conditional extremes",
    "text": "Modeling conditional extremes\nNow, we fit the multivariate conditional extremes model using mex. We here condition on the component \\(j=2\\). The mex function handles both marginal and dependence fits. We keep only those columns of frwind corresponding to wind speeds. Here, we use the same quantile (at level mqu) for marginal fits and dependence fits.\n\nmex2 = mex(frwind[, paste0(\"S\", 1:4)], \n           mqu = .95, # marginal quantile of cond.\n           penalty = \"none\", # penalized marginal estimation?\n           which = \"S2\") # conditioning variable\n\nAssuming same quantile for dependence thesholding as was used\n     to fit corresponding marginal model...\n\nmex2\n\nmex(data = frwind[, paste0(\"S\", 1:4)], which = \"S2\", mqu = 0.95, \n    penalty = \"none\")\n\n\nMarginal models:\n\nDependence model:\n\nConditioning on S2 variable.\nThresholding quantiles for transformed data: dqu = 0.9502586\nUsing laplace margins for dependence estimation.\nConstrained estimation of dependence parameters using v = 10 .\nLog-likelihood = -1411.812 -1533.625 -1802.116 \n\nDependence structure parameter estimates:\n      S1     S3     S4\na 0.1087 0.2592 0.4354\nb 0.2524 0.3842 0.4189\n\nsummary(mex2)\n\n\nMarginal models:\n                      S1     S2     S3     S4\nThreshold         48.600 23.760 39.600 29.160\nP(X &lt; threshold)   0.951  0.950  0.952  0.952\nsigma              7.987  5.366  8.351  4.523\nxi                -0.127 -0.126 -0.195 -0.129\nUpper end point  111.560 66.353 82.337 64.168\n\nDependence model:\n\n      S1      S3     S4\na  0.109 0.25921  0.435\nb  0.252 0.38417  0.419\nc  0.000 0.00000  0.000\nd  0.000 0.00000  0.000\nm -0.206 0.00981 -0.357\ns  0.940 0.92974  1.222\n\n\nHere are some diagnostic plots of the fitted model.\n\nggplot(mex2)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nAs assumed by the model, we do not see any notable dependence of residuals ($\\mathbf{Z}$) on the level of the conditioning variable.\nNote: to estimate only marginal parameters, we can use migpd; to estimate only the dependence with prespecified marginal estimates, we can use `mexDependence`. Here is an example of explicit two-step estimation.\n\nmarg = migpd(\n  data = frwind[,paste0(\"S\", 1:4)], \n  mqu = 0.95, \n  penalty = \"none\") \nmex2_twostep = mexDependence(marg, which = \"S2\")\n\nAssuming same quantile for dependence thesholding as was used\n     to fit corresponding marginal model...\n\nmex2_twostep\n\nmexDependence(x = marg, which = \"S2\")\n\n\nMarginal models:\n\nDependence model:\n\nConditioning on S2 variable.\nThresholding quantiles for transformed data: dqu = 0.95\nUsing laplace margins for dependence estimation.\nConstrained estimation of dependence parameters using v = 10 .\nLog-likelihood = -1411.812 -1533.625 -1802.116 \n\nDependence structure parameter estimates:\n      S1     S3     S4\na 0.1087 0.2592 0.4354\nb 0.2524 0.3842 0.4189\n\n\nIn the end, the result of mex2_twostep is exactly the same as that of mex2.\nVarious diagnostics can be used to explore or validate tuning parameters such as the threshold. For example: do we have an appropriate marginal threshold? We can check this using parameter stability plots and mean-excess plots.\n\nggplot(\n  gpdRangeFit(frwind$S2, \n              umin = quantile(frwind$S2, .90), \n              umax = quantile(frwind$S2, .995), \n              nint = 21)) \n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\nggplot(mrl(frwind$S2, \n           umin = quantile(frwind$S2, .90), \n           umax = quantile(frwind$S2, .995), \n           nint = 21))\n\n\n\n\nThe tail index estimates looks relatively stable across different thresholds, and the mean excess plots show a relatively nice linear slope, which correspond to the tail index.\nMoreover, we can explore if the estimated dependence model is sensible to the choice of the threshold. Note that running this analysis can be very time-consuming. Here, to avoid very long computations, we use a very small number of quantiles and a very low number R of bootstrap replicates, but for a serious application you should of course use larger numbers.\n\nmrf = mexRangeFit(\n  x = marg, \n  which = \"S2\", \n  quantiles = c(0.95, 0.96, 0.97), \n  R = 3) \nggplot(mrf)\n\n\n\n\nFor the chosen quantiles, and given the uncertainty in the bootstrap estimates, we do not see any strong dependence of parameters on the threshold level."
  },
  {
    "objectID": "content/conditionalextremes.html#simulation-and-prediction",
    "href": "content/conditionalextremes.html#simulation-and-prediction",
    "title": "Conditional extremes",
    "section": "Simulation and prediction",
    "text": "Simulation and prediction\nWe can simulate from the fitted model but for a higher conditioning threshold than the one used during estimation.\n\nset.seed(1)\nnsim = 100\npred2 = predict(mex2, pqu=.99, nsim = nsim)\nsummary(pred2)\n\npredict.mex(object = mex2, pqu = 0.99, nsim = nsim)\n\nConditioned on S2 being above its 99th percentile.\n\n\nConditional Mean and Quantiles:\n\n     S2|S2&gt;Q99 S1|S2&gt;Q99 S3|S2&gt;Q99 S4|S2&gt;Q99\nmean      35.4      27.1     30.50     23.00\n5%        31.9      10.4      8.64      5.72\n50%       34.4      23.6     27.20     22.70\n95%       40.6      56.6     61.60     39.00\n\nConditional probability of threshold exceedance:\n\n P(S2&gt;23.76|S2&gt;Q99) P(S1&gt;48.6|S2&gt;Q99) P(S3&gt;39.6|S2&gt;Q99) P(S4&gt;29.16|S2&gt;Q99)\n                  1              0.08              0.29               0.39\n\nggplot(pred2)\n\n\n\n\nThere are also commands to estimate all four conditional extremes models (for \\(j=1, \\ldots, 4\\) here), and to conduct simulation from the estimated models.\n\nmAll = mexAll(frwind[,paste0(\"S\", 1:4)], mqu=0.95, dqu=rep(0.95,4))\nmAll\n\nmex(data = data, which = i, mqu = mqu, dqu = dqu[i])\n\n\nMarginal models:\n\nDependence model:\n\nConditioning on S1 variable.\nThresholding quantiles for transformed data: dqu = 0.95\nUsing laplace margins for dependence estimation.\nConstrained estimation of dependence parameters using v = 10 .\nLog-likelihood = -1351.649 -1417.491 -1e+40 \n\nDependence structure parameter estimates:\n      S2      S3   S4\na 0.1181  0.3110 0.01\nb 0.3425 -0.2607 0.01\n\n______\nConditioning on S2 variable.\nThresholding quantiles for transformed data: dqu = 0.95\nUsing laplace margins for dependence estimation.\nConstrained estimation of dependence parameters using v = 10 .\nLog-likelihood = -1411.555 -1533.564 -1802.01 \n\nDependence structure parameter estimates:\n      S1     S3     S4\na 0.1081 0.2596 0.4364\nb 0.2519 0.3852 0.4198\n\n______\nConditioning on S3 variable.\nThresholding quantiles for transformed data: dqu = 0.95\nUsing laplace margins for dependence estimation.\nConstrained estimation of dependence parameters using v = 10 .\nLog-likelihood = -1512.871 -1e+40 -1346.54 \n\nDependence structure parameter estimates:\n       S1   S2     S4\na 0.20410 0.01 0.4660\nb 0.04996 0.01 0.0997\n\n______\nConditioning on S4 variable.\nThresholding quantiles for transformed data: dqu = 0.95\nUsing laplace margins for dependence estimation.\nConstrained estimation of dependence parameters using v = 10 .\nLog-likelihood = -1540.897 -1e+40 -1654.156 \n\nDependence structure parameter estimates:\n       S1   S2     S3\na 0.03745 0.01 0.3048\nb 0.23540 0.01 0.2503\n\npred_sim =  mexMonteCarlo(nsim, mAll)\npairs(pred_sim$MCsample)\n\n\n\n\nWe can use the conditional extremes models to provide model-based joint exceedance curves (for a given probability) at very high levels. Let’s first plot empirical joint exceedance curves at relatively high observed levels. We here consider the two variables at \\(S_2\\) and \\(S_3\\).\n\nwind23 = frwind[, c(\"S2\", \"S3\")]\nj1 = JointExceedanceCurve(wind23, 0.1) \nj2 = JointExceedanceCurve(wind23, 0.05) \nj3 = JointExceedanceCurve(wind23, 0.025) \nggplot(wind23, aes(S2,S3)) + \n      geom_point(colour=\"dark blue\",alpha=0.5) + \n      geom_jointExcCurve(j1,colour=\"orange\") + \n      geom_jointExcCurve(j2,colour=\"orange\") + \n      geom_jointExcCurve(j3,colour=\"orange\")\n\n\n\n\nFinally, we estimate joint exceedance curves at much higher levels by combining Monte-Carlo simulations of the different conditional-extremes models. We here use a relatively small number of simulations (100) for reasons of computation time, but results can be made more precise by using a larger number of simulations. Joint exceedance curves are implemented for two variables.\n\npred_sim =  mexMonteCarlo(\n    nSample = 500, \n    mexList = mAll)\nj1 = JointExceedanceCurve(\n  Sample = pred_sim, \n  ExceedanceProb = 0.05, \n  which = c(\"S2\",\"S3\"))\nj2 = JointExceedanceCurve(\n  pred_sim, \n  0.025, \n  which = c(\"S2\",\"S3\"))\nj3 = JointExceedanceCurve(\n  pred_sim, \n  0.01, \n  which = c(\"S2\",\"S3\")) \nggplot(\n  data = as.data.frame(\n    pred_sim$MCsample[,c(\"S2\",\"S3\")]), \n  mapping = aes(S2, S3)) +\n      geom_point(col=\"light blue\", alpha = 0.5) + \n      geom_jointExcCurve(j1, aes(S2,S3), col=\"orange\") + \n      geom_jointExcCurve(j2, aes(S2,S3), col=\"orange\") + \n      geom_jointExcCurve(j3, aes(S2,S3), col=\"orange\")\n\n\n\n\nTo extrapolate joint exceedance curves towards very high levels using simulation at higher conditioning thresholds in \\(S_2\\), we can use the predict function. Simulated values in blue correspond to those where the conditioning component is largest, otherwise the color is orange. The orange line indicates where we have the same marginal quantile levels in the two variables.\n\npred2 = predict(mex2, nsim = 5000, pqu = 0.995)\ngg = ggplot(pred2, plot. = FALSE)\nj1 = JointExceedanceCurve(\n    Sample = pred2, \n    ExceedanceProb = 0.002, \n    which = c(\"S2\",\"S3\"))\nj2 = JointExceedanceCurve(\n  pred2, \n  0.001, \n  which = c(\"S2\",\"S3\"))\nj3 = JointExceedanceCurve(\n  pred2, \n  0.0005, \n  which = c(\"S2\",\"S3\"))\ngg[[1]] + \n  geom_jointExcCurve(j1, aes(S2, S3), col = \"purple\") + \n  geom_jointExcCurve(j2, aes(S2, S3), col = \"purple\") + \n  geom_jointExcCurve(j3, aes(S2, S3), col = \"purple\")"
  },
  {
    "objectID": "content/likelihood.html",
    "href": "content/likelihood.html",
    "title": "Likelihood-based inference",
    "section": "",
    "text": "The mev package provides gradient-based optimization routines for fitting univariate extreme value models, either block maxima or threshold exceedances, using one of four likelihoods: that of the generalized extreme value distribution, the generalized Pareto distribution, and the inhomogeneous Poisson point process and the \\(r\\)-largest order statistics.\nRelative to other packages such as evd or ismev, the package functions include analytic expressions for the score and observed informations, with careful interpolation when \\(\\xi \\approx 0\\). However, mev does not handle generalized linear or generalized additive models for the parameters, to avoid having as many inequality constraints in the optimization as there are observations times the number of covariates."
  },
  {
    "objectID": "content/likelihood.html#basic-theory",
    "href": "content/likelihood.html#basic-theory",
    "title": "Likelihood-based inference",
    "section": "Basic theory",
    "text": "Basic theory\nLet \\(\\ell(\\boldsymbol{y}; \\boldsymbol{\\theta})\\) denotes the log-likelihood of an \\(n\\) sample with a \\(p\\)-dimensional parameter \\(\\boldsymbol{\\theta}\\). The score vector is \\(\\ell_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta})=\\partial \\ell / \\partial \\boldsymbol{\\theta}\\), while the Fisher information is \\(\\imath(\\boldsymbol{\\theta})=\\mathrm{E}\\{\\ell_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta})\\ell_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta})^\\top\\}\\). Under regularity conditions, we also have \\(\\imath(\\boldsymbol{\\theta}) = - \\mathrm{E}(\\partial^2 \\ell / \\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^\\top)\\). The observed information is the negative Hessian \\(\\jmath(\\boldsymbol{\\theta})-\\partial^2 \\ell / \\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^\\top\\), evaluated at the maximum likelihood estimator \\(\\hat{\\boldsymbol{\\theta}}\\).\nBy definition, the maximum likelihood estimator solves the score equation, i.e. \\(\\ell_{\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})=\\boldsymbol{0}_p\\). If the maximum likelihood estimator is not available in closed-form, its solution is found numerically and this property can be used to verify that the optimization routine has converged or for gradient-based maximization algorithms."
  },
  {
    "objectID": "content/likelihood.html#statistical-inference",
    "href": "content/likelihood.html#statistical-inference",
    "title": "Likelihood-based inference",
    "section": "Statistical inference",
    "text": "Statistical inference\nThis section presents some test statistics that can easily be computed using some of the functionalities of mev, as well as confidence intervals for parameters and common functionals, based on the profile likelihood.\nThe three main type of test statistics for likelihood-based inference are the Wald, score and likelihood ratio tests. The three main classes of statistics for testing a simple null hypothesis \\(\\mathscr{H}_0: \\boldsymbol{\\theta}=\\boldsymbol{\\theta}_0\\) against the alternative \\(\\mathscr{H}_a: \\boldsymbol{\\theta} \\neq \\boldsymbol{\\theta}_0\\) are the likelihood ratio, the score and the Wald statistics, defined respectively as \\[\\begin{align*}\nw &= 2 \\left\\{ \\ell(\\hat{\\boldsymbol{\\theta}})-\\ell(\\boldsymbol{\\theta}_0)\\right\\},\\qquad\n\\\\w_{\\mathsf{score}} &= U^\\top(\\boldsymbol{\\theta}_0)i^{-1}(\\boldsymbol{\\theta}_0)\\ell_{\\boldsymbol{   heta}}(\\boldsymbol{\\theta}_0),\\qquad\n\\\\ w_{\\mathsf{wald}} &= (\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}_0)^\\top i(\\boldsymbol{\\theta}_0)(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}_0),\n\\end{align*}\\] where \\(\\hat{\\boldsymbol{\\theta}}\\) is the maximum likelihood estimate under the alternative and \\(\\boldsymbol{\\theta}_0\\) is the null value of the parameter vector. The statistics \\(w, w_{\\mathsf{score}}, w_{\\mathsf{wald}}\\) are all first order equivalent and asymptotically follow a \\(\\chi^2_p\\) distribution, where \\(q\\) is the difference between \\(p\\) and the number of parameters under the null hypothesis. Under the conditions of the Neyman–Pearson theorem, the likelihood ratio test is most powerful test of the lot. The score statistic \\(w_{\\mathsf{score}}\\) only requires calculation of the score and information under \\(\\mathscr{H}_0\\), which can be useful in problems where calculations under the alternative are difficult to obtain. The Wald statistic \\(w_{\\mathsf{wald}}\\) is not parametrization-invariant and typically has poor coverage properties.\nOftentimes, we are interested in a functional of the parameter vector \\(\\boldsymbol{\\theta}\\). The profile likelihood \\(\\ell_\\mathsf{p}\\), a function of \\(\\boldsymbol{\\psi}\\) alone, is obtained by maximizing the likelihood pointwise at each fixed value \\(\\boldsymbol{\\psi}=\\boldsymbol{\\psi}_0\\) over the nuisance vector \\(\\boldsymbol{\\lambda}_{\\psi_0}\\), \\[\\begin{align*}\n   \\ell_\\mathsf{p}(\\boldsymbol{\\psi})=\\max_{\\boldsymbol{\\lambda}}\\ell(\\boldsymbol{\\psi}, \\boldsymbol{\\lambda})=\\ell(\\boldsymbol{\\psi}, \\hat{\\boldsymbol{\\lambda}}_{\\boldsymbol{\\psi}}).\n\\end{align*}\\] We denote the restricted maximum likelihood estimator \\(\\hat{\\boldsymbol{\\theta}}_\\psi= (\\psi, \\hat{\\lambda}_{\\psi})\\).\nWe can define score and information in the usual fashion: for example, the observed profile information function is [j_() =- = {j^{}(, _{})}^{-1}. ]\nWe can turn tests and their asymptotic distribution into confidence intervals. For the hypothesis \\(\\psi = \\psi_0\\), a \\((1-\\alpha)\\) confidence interval based on the profile likelihood ratio test is \\(\\{ \\psi: 2\\{\\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta}_{\\psi})\\} \\leq \\chi^2_1(0.95)\\}\\).\n\nLikelihoods\nThere are four basic likelihoods for univariate extremes: the likelihood of the generalized extreme value (GEV) distribution for block maxima, the likelihood for the generalized Pareto distribution and that of the non-homogeneous Poisson process (NHPP) for exceedances above a threshold \\(u\\) and lastly the likelihood of the \\(r\\)-largest observations.\n\n\nGeneralized extreme value distribution\nThe generalized extreme value (GEV) distribution with location parameter \\(\\mu \\in \\mathbb{R}\\), scale parameter \\(\\sigma \\in \\mathbb{R}_{+}\\) and shape parameter \\(\\xi \\in \\mathbb{R}\\) is \\[\\begin{align*}\n  G(x)  =\n\\begin{cases}\n\\exp\\left\\{-\\left(1+\\xi \\frac{x-\\mu}{\\sigma}\\right)^{-1/\\xi}\\right\\}, &  \\xi \\neq 0,\\\\\n\\exp \\left\\{ -\\exp \\left(-\\frac{x-\\mu}{\\sigma}\\right)\\right\\},&  \\xi = 0,\n\\end{cases}\n\\end{align*}\\] defined on \\(\\{x \\in \\mathbb{R}: \\xi(x-\\mu)/\\sigma &gt; -1\\}\\) where \\(x_{+} = \\max\\{0, x\\}\\). The case \\(\\xi=0\\) is commonly known as the Gumbel distribution. We denote the distribution by \\(\\mathsf{GEV}(\\mu, \\sigma, \\xi)\\).\nThe max-stability property allows one to extrapolate the distribution beyond observed levels: one can show that the distribution of the maximum of a larger block (or \\(N\\) block maximum) would be also generalized extreme value if \\(Y_i \\sim \\mathsf{GEV}(\\mu, \\sigma, \\xi)\\) are independent, where \\(\\max_{i=1}^N Y_i \\sim \\mathsf{GEV}(\\mu_N, \\sigma_N, \\xi)\\) and \\(\\mu_N = \\mu + \\sigma(N^\\xi-1)/\\xi\\) and \\(\\sigma_N = \\sigma N^\\xi, \\xi_N = \\xi)\\) — the case \\(\\xi=0\\) is defined by continuity. In practice, we can partition data into \\(m\\) blocks of roughly equal size \\(n/m\\) and fit a GEV distribution to the maximum of the blocks.\nThe GEV distribution is suitable for maximum of a large number of observations: the larger the block size, the closer the approximation will be, but the smaller the sample size \\(m\\).in In practice, there is a natural block size (say yearly) over which to compute maximum. The advantage is that, even if data are not stationary, we can expect the maximum to occur roughly at the same time of the year (e.g., for temperature) and so even if the true block size is much lower than 365 days, we can still use our approach.\nThe fit.gev function includes two optimization routines: either use the PORT methods from nlminb, or Broyden-Fletcher-Goldfarb-Shanno algorithm (BFGS) inside a constrained optimization algorithm (augmented Lagrangian). The default option is nlminb, which sometimes returns diagnostics indicating false convergence when the model is near the maximum likelihood estimate.\nAs for other model, parameters can be fixed and nested models can be compared using the anova S3 method. For these, we distinguish between estimated coefficients (estimate) or with the coef method, and the full vector of parameters, param.\n\n\nNumerical example\nWe consider in the tutorial daily mean wind speed data, measured in km/h, at four weather stations located in the south of France. We first take the data for Lyon’s airport (station \\(S_2\\)) and compute the annual maximum.\n\nlibrary(mev)\n# library(xts)\n# library(lubridate)\ndata(frwind, package = \"mev\")\nlyon &lt;- with(frwind,\n             xts::xts(x = S2, order.by = date))\n# Create series of yearly maximum\nymax &lt;- xts::apply.yearly(lyon, max)\n\nWe can then fit a GEV distribution via maximum likelihood to the yearly maximum, extract the coefficients \\(\\widehat{\\boldsymbol{\\theta}}\\) and check convergence by computing the score \\(\\ell_{\\boldsymbol{\\theta}}(\\widehat{\\boldsymbol{\\theta}})\\) and comparing it to the zero vector.\n\nopt_gev &lt;- mev::fit.gev(xdat = ymax, show = TRUE)\n\nLog-likelihood: -141.6626 \n\nEstimates\n     loc     scale     shape  \n36.18449   3.94287  -0.01124  \n\nStandard Errors\n   loc   scale   shape  \n0.6589  0.4881  0.1318  \n\nOptimization Information\n  Convergence: successful \n  Function Evaluations: 27 \n  Gradient Evaluations: 11 \n\nmle &lt;- coef(opt_gev)\nisTRUE(all.equal(rep(0,3),\n                 mev::gev.score(par = mle, dat = ymax),\n                 check.attributes = FALSE,\n                 tolerance = 1e-5))\n\n[1] TRUE\n\n\nHaving found the MLE, we can compute the covariance matrix of the parameters from the observed information matrix. The standard errors are the square root of the elements on the diagonal. While mev uses exact formulae, these can be approximated by computing the hessian via finite differences.\n\n# Compute observed information matrix\njmat &lt;- mev::gev.infomat(par = mle, dat = ymax)\n# Compute standard errors\nse_mle &lt;- sqrt(diag(solve(jmat)))\n# Compare with 'mev' output\nisTRUE(all.equal(se_mle, opt_gev$std.err))\n\n[1] TRUE\n\n\nEven if we have parameter estimates, there is no guarantee that the model is adequate. Standard visual goodness-of-fit diagnostics can be obtained with the plot method. To see other methods, query methods(class = \"mev_gev\").\n\n# PP and QQ plots\npar(mfrow = c(1,2))\nplot(opt_gev)\n\n\n\ngraphics.off()\n\nThe Gumbel distribution, which corresponds to a GEV with shape \\(\\xi=0\\), can be estimated by restricting a parameter. We then do a likelihood ratio test since models are nested.\n\nopt_gumb &lt;- mev::fit.gev(xdat = ymax,\n                         fpar = list(shape = 0))\nanova(opt_gev, opt_gumb)\n\nAnalysis of Deviance Table\n\n         npar Deviance Df  Chisq Pr(&gt;Chisq)\nopt_gev     3   283.32                     \nopt_gumb    2   283.33  1 0.0073     0.9321\n\n\nNone of the parameters are of interest in themselves. We may be interested rather by a risk summary, which is a function of parameters. For example, we could get the parameters of the GEV for 50 years via max-stability and return the average if \\(\\widehat{\\xi}&lt;1\\), or quantiles — the most popular choices are the median and 0.368, which corresponds roughly to the 50 year return level for threshold exceedances.\nAll of these are invariant to reparametrization, so we can use the formula and plug-in the parameter values. For inference, we reparametrize the model in terms of this quantity, then vary over a grid of values of the 50-year average maximum and compute profile-likelihood-based confidence intervals at level 95%.\n\ngev.mle(xdat = ymax, args = \"Nmean\", N = 50)\n\n  Nmean \n53.4114 \n\n# Compute profile log-likelihood\nprof &lt;- mev::gev.pll(param = \"Nmean\", dat = ymax, N = 50)\n\n\n\n# Extract confidence intervals\n(confint(prof))\n\nEstimate Lower CI Upper CI \n53.41140 47.79346 73.63881 \n\n\n\n\nGeneralized Pareto distribution\nThe generalized Pareto (GP) distribution with scale \\(\\sigma \\in \\mathbb{R}_{+}\\) and shape \\(\\xi \\in \\mathbb{R}\\) is \\[\\begin{align*}\n  G(x)  =\n\\begin{cases}\n1-\\left(1+\\xi \\frac{x}{\\sigma}\\right)_{+}^{-1/\\xi}, &  \\xi \\neq 0,\\\\ 1-\n\\exp \\left(-\\frac{x}{\\sigma}\\right),&  \\xi = 0.\n\\end{cases}\n\\end{align*}\\] The range of the generalized Pareto distribution is \\([0, -\\sigma/\\xi)\\) if \\(\\xi &lt; 0\\) and is \\(\\mathbb{R}_{+}\\) otherwise. We denote the distribution by \\(\\mathsf{GP}(\\sigma, \\xi)\\). The default optimization algorithm for this model is that of Grimshaw (1993), which reduces the dimension of the optimization through profiling. The exponential distribution and the case \\(\\xi=-1\\) are handled separately. If the sample coefficient of variation is less than one, the global maximum lies on the boundary of the parameter space since there exists for any \\(\\xi&lt;-1\\) a value \\(\\sigma^*\\) such that \\(\\ell(\\sigma^*, \\xi) \\to \\infty\\): the search is thus restricted to \\(\\xi \\geq -1\\). These cases are more frequent in small samples due to the negative bias of the maximum likelihood estimator of the shape.\nExcept for this boundary case, the maximum likelihood estimator solves the score equation \\(\\partial \\ell(\\boldsymbol{\\theta}) / \\partial \\boldsymbol{\\theta} = \\boldsymbol{0}_2\\). We can thus check convergence by verifying that the score vanishes at the maximum likelihood estimate.\nIf \\(\\widehat{\\xi} &lt; -0.5\\), the asymptotic regime is nonstandard (Smith, 1985) and the standard errors obtained from the inverse information matrix are unreliable; as such, mev does not report them and prints an optional warning.\n\n\n\n\n\n\n\n\n\nThe figure shows the profile likelihood for \\(\\eta = -\\xi/\\sigma\\) for two datasets, one of which (leftmost) achieves its maximum at \\(\\widehat{\\xi} = -1\\) and \\(\\widehat{\\eta} = 1/\\max(\\boldsymbol{y})\\).\n\n# Only keep data from September to April\nwindlyon &lt;- with(\n  frwind, \n  S2[lubridate::month(date) &lt;= 4 | \n       lubridate::month(date) &gt;= 9])\n# Keep only 100 largest points (fewer because of ties)\nu &lt;- quantile(windlyon, \n              probs = 1-100/length(windlyon))\n# Fit generalized Pareto via ML\nfitted_gp &lt;- fit.gpd(\n  xdat = windlyon,\n  threshold = u,\n  show = TRUE)\n\nMethod: Grimshaw \nLog-likelihood: -207.5276 \n\nThreshold: 33.84 \nNumber Above: 90 \nProportion Above: 0.0079 \n\nEstimates\n  scale    shape  \n3.57863  0.03088  \n\nStandard Errors\n scale   shape  \n0.6091  0.1337  \n\nOptimization Information\n  Convergence: successful \n\n# P-P and Q-Q diagnostic plots \npar(mfrow = c(1, 2))\nplot(fitted_gp)\n\n\n\n\n\n\n\ngraphics.off()\n# Fit exponential by passing a list with a fixed parameter\nreduced_gp &lt;- fit.gpd(windlyon,\n                   threshold = u, \n                   fpar = list(shape = 0))\n# The MLE is sample mean of exceedances - check this\nisTRUE(coef(reduced_gp) == mean(windlyon))\n\n[1] TRUE\n\n# Compare nested models using likelihood ratio test\nanova(fitted_gp, reduced_gp)\n\nAnalysis of Deviance Table\n\n           npar Deviance Df  Chisq Pr(&gt;Chisq)    \nfitted_gp     2   415.06                         \nreduced_gp    1   502.50  1 87.448  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe mev package includes alternative routines for estimation, including the optimal bias-robust estimator of Dupuis (1999) and the approximate Bayesian estimators of Zhang & Stephens (2009) and Zhang (2010). The latter two are obtained by running a Markov chain Monte Carlo algorithm, but only the posterior mean and standard deviation are returned to reduce the memory footprint of the returned object, and these are calculated on the fly using running mean and variance estimators.\n\n# Bayesian point estimates (based on MAP)\nfit.gpd(windlyon, \n        threshold = u, \n        show = TRUE, \n        MCMC = TRUE,\n        method = \"zhang\")\n\n\nMethod: Zhang \n\nThreshold: 33.84 \nNumber Above: 90 \nProportion Above: 0.0079 \n\nApproximate posterior mean estimates\nscale  shape  \n3.456  0.066  \n\nPosterior mean estimates\n scale   shape  \n3.5594  0.0495  \n\nMonte Carlo standard errors\nscale  shape  \n0.453  0.130  \n\nEstimates based on an adaptive MCMC\n Runs:    10000 \n Burnin:  2000 \n Acceptance rate: 0.43 \n Thinning: 1 \n\n\nIf the sample is small, maximum likelihood estimators are biased for the generalized Pareto distribution (the shape parameter is negatively biased, regardless of the true value for \\(\\xi\\)). Bias correction methods includes the modified score of Firth, but the default method is the implicit correction (subtract), which solves the implicit equation \\[\\begin{align}\n   \\boldsymbol{\\tilde{\\theta}}=\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{b}(\\tilde{\\boldsymbol{\\theta}}). \\label{eq:implbias}\n\\end{align}\\] The point estimate \\(\\boldsymbol{\\tilde{\\theta}}\\) is obtained numerically as the root of this nonlinear system of equations. In the present case, the sample size is large and hence the first-order correction, derived through asymptotic arguments from the generalized Pareto distribution likelihood, is small. Note that the bias correction requires \\(\\xi &gt; -1/3\\), since it is based on third-order cumulants of the distribution.\n\n# First-order bias corrected estimates\ncorr_coef &lt;- gpd.bcor(par = coef(fitted_gp), \n                      dat = windlyon, \n                      corr = \"firth\")\n\nError in bcor.st$value : $ operator is invalid for atomic vectors\n\n\n\n\nInhomogeneous Poisson process\nLet \\(Y_{(1)} \\geq \\cdots \\geq Y_{(r)}\\) denote the \\(r\\) largest observations from a sample. The likelihood of the limiting distribution of the point process for the \\(r\\)-largest observations is, for \\(\\mu,\\xi\\in\\mathbb{R}, \\sigma&gt;0\\), [ (,,; ) -r() - (1+){j=1}^r (1 + ){+} - (1 + )^{-1/}_+. ] This likelihood can be used to model the \\(r\\)-largest observations per block or threshold exceedances where the threshold is the \\(r\\)th order statistic\nConsider a sample of \\(N\\) observations, of which \\(n_u\\) exceed \\(u\\) and which we denote by \\(y_1, \\ldots, y_{n_u}\\). The likelihood associated to the limiting distribution of threshold exceedances is, for \\(\\mu, \\xi \\in \\mathbb{R}, \\sigma &gt;0\\), \\[\\begin{align}\nL(\\mu, \\sigma, \\xi; \\boldsymbol{y}) = \\exp \\left[ - c \\left\\{1+ \\xi \\left( \\frac{u-\\mu}{\\sigma}\\right)\\right\\}^{-1/\\xi}_{+}\\right] (c\\sigma)^{-n_u}\\prod_{i=1}^{n_u} \\left\\{1+\\xi\\left( \\frac{y_i-\\mu}{\\sigma}\\right)\\right\\}^{-1/\\xi-1}_{+},\\label{eq:ppp_lik}\n\\end{align}\\] where \\((\\cdot)_{+} = \\max\\{0, \\cdot\\}\\). The quantity \\(c\\) is a tuning parameter whose role is described in 7.5 of Coles (2001). If we take \\(c=N/m\\), the parameters of the point process likelihood correspond to those of the generalized extreme value distribution fitted to blocks of size \\(m\\). The NHPP likelihood includes a contribution for the fraction of points that exceeds the threshold, whereas the generalized Pareto is a conditional distribution, whose third parameter is the normalizing constant \\(\\zeta_u=\\Pr(Y&gt;u)\\). Since the latter has a Bernoulli and \\(\\zeta_u\\) is orthogonal to the pair \\((\\sigma, \\xi)\\), it is often omitted from further analyses and estimated as the proportion of samples above the threshold.\nThe model includes additional arguments, np and npp (number of observations per period). If data are recorded on a daily basis, using a value of npp = 365.25 yields location and scale parameters that correspond to those of the generalized extreme value distribution fitted to block maxima. Alternatively, one can specify instead the number of periods np, akin to \\(n_y\\) in Eq. 7.8 of Coles (2001) — only the latter is used by the function, with npp*np theoretically equal to the number of exceedances.\nThe tuning parameters impact the convergence of the estimation since the dependence between parameters becomes very strong: Sharkey & Tawn (2017) suggest to pick a value of np that near-orthogonalize the parameters. Wadsworth:2011 recommended picking this to be the number of observations (so npp=1), but Moins et al. (2023) show that a better choice leads to orthogonalization.\nAnother option is to fit the generalized Pareto model: if the probability of exceeding threshold \\(u\\) is small, the Poisson approximation to binomial distribution implies [c {1+ ( )}^{-1/} n_u, ] where \\(n_u\\) is the number of threshold exceedances above \\(u\\) and \\(c\\) is the tuning parameter np. With the point estimates of the generalized Pareto model, say \\((\\widehat{\\sigma}_u, \\widehat{\\xi})\\), we thus use \\[\\begin{align*}\n\\mu_0 &= u - \\sigma_0\\{(n_u/c)^{-\\widehat{\\xi}}-1\\}/\\widehat{\\xi},\\\\\n\\sigma_0 &= \\widehat{\\sigma}_u\\times (n_u/c)^{\\widehat{\\xi}},\n\\end{align*}\\] and \\(\\xi_0=\\widehat{\\xi}\\) as starting values. Most of the time, these values are so close to the solution of the score equation that numerical convergence of the optimization routine is all but guaranteed in a few likelihood evaluations. If no starting value is provided and some fixed parameters are provided, the model will approximate the distribution of the vector of parameters by a multivariate Gaussian distribution and compute the best linear predictor of the remaining parameters given those are fixed. This method works well if the log-likelihood is near quadratic and the values are not too far from the maximum, but does not deal with the boundary constraints. In case these starting values are invalid, and an error message is returned.\nThe log likelihood of the \\(r\\) largest order statistics likelihood is derived from the inhomogenenous Poisson point process formulation. We normally consider a matrix of observations \\(m \\times r\\) containing the largest \\(r\\) order statistics of each block out of the \\(n\\) sample. The parameters of the \\(r\\)-largest likelihood are the same as the generalized extreme value distribution (a special case when \\(r=1\\)). This model can be used when we have access to order statistics, assuming that the observations within the block are independent.\n\\[\\begin{align*}\n\\ell(\\mu,\\sigma,\\xi; \\boldsymbol{y}) &= -rm\\log(\\sigma) - \\left(1+\\frac{1}{\\xi}\\right)\\sum_{i=1}^m\\sum_{j=1}^r \\log\\left(1 + \\xi\\frac{y_{i,(j)}-\\mu}{\\sigma}\\right)_{+} \\\\ &\\quad- \\left(1 + \\xi\\frac{y_{i,(r)}-\\mu}{\\sigma}\\right)^{-1/\\xi}_+, \\quad \\mu,\\xi\\in\\mathbb{R}, \\sigma&gt;0. \\label{eq:rlarglik}\n\\end{align*}\\] It’s not obvious how to chose \\(r\\), but Belzile & Davison (2022) shows the gain from considering larger values of \\(r\\) decreases quickly for the shape. The support constraints, typically arising for the minimum observation, means that finding good starting values is hard.\nWe can simulate from the \\(r\\) largest observations by drawing from a unit rate Poisson process \\(0&lt;U_1&lt;U_2&lt;\\cdots\\), where \\(U_j=E_1+\\cdots+E_j\\) and \\(E_j\\sim \\mathsf{Exp}(1)\\), and setting \\(Y_{j} = \\mu + \\sigma\\big(U_j^{-1/\\xi}-1\\big)/\\xi\\). Applying the inverse transformation \\(\\Lambda_{\\boldsymbol{\\theta}}(y) = \\left\\{ 1 + \\xi(y-\\mu)/\\sigma\\right\\}^{-1/\\xi}_+\\) evaluated at the MLE gives roughly independent exponential spacings, which can be used to create quantile-quantile plots."
  },
  {
    "objectID": "content/likelihood.html#risk-measures",
    "href": "content/likelihood.html#risk-measures",
    "title": "Likelihood-based inference",
    "section": "Risk measures",
    "text": "Risk measures\nTwo typical questions in extreme values are: given the intensity of an extreme event, what is its recurrence period? and what is a typical worst-case scenario over a given period of time? For the latter, suppose for simplicity that the daily observations are blocked into years, so that inference is based on \\(N\\) points for the \\(N\\) years during which the data were recorded. The return level is a quantile of the underlying distribution corresponding to an event of probability \\(p=1-1/T\\) for an annual maximum, which is interpreted as ``the level exceeded by an annual maximum on average every \\(T\\) years’’. If observations are independent and identically distributed, then we can approximate the probability that a return level is exceeded \\(l\\) times over a \\(T\\) year period using a binomial distribution with probability of success \\(1-1/T\\) and \\(T\\) trials. For \\(T\\) large, the return level is exceeded \\(l=0, 1, 2, 3, 4\\) times within any \\(T\\)-years period with approximate probabilities 36.8%, 36.8%, 18.4%, 6.1% and 1.5%. The probability that the maximum observation over \\(T\\) years is exceeded with a given probability is readily obtained from the distribution of the \\(T\\)-year maximum, leading (Cox et al., 2002, p. 3(b)) to advocate its use over return levels, among other quantities of interest such as the number of times a threshold \\(u\\) will be exceeded in \\(T\\) years or the average number of years before a threshold \\(u\\) is exceeded.\nQuantiles, mean and return levels of \\(T\\)-maxima: consider the distribution \\(H(x) = G^T(x)\\) of the maximum of \\(T\\) independent and identically distributed generalized extreme value variates with parameters \\((\\mu, \\sigma, \\xi)\\) and distribution function \\(G\\). By max-stability, the parameters of \\(H(x)\\) are \\(\\mu_T=\\mu-\\sigma(1-T^\\xi)/\\xi\\) and \\(\\sigma_T=\\sigma T^\\xi\\) when \\(\\xi \\neq 0\\). We denote the expectation of the \\(T\\)-observation maximum by \\(\\mathfrak{e}_T\\), the \\(p\\) quantile of the \\(T\\)-observation maximum by \\(\\mathfrak{q}_p = H^{-1}(p)\\) and the associated return level by \\(z_{1/T} = G^{-1}(1-1/T)\\). Then, any of these three quantities can be written as \\[\\begin{align*}\n\\begin{cases}\n\\mu-\\frac{\\sigma}{\\xi}\\left\\{1-\\kappa_{\\xi}\\right\\}, &  \\xi &lt;1, \\xi \\neq 0, \\\\\n\\mu+\\sigma\\kappa_0, &  \\xi =0,\n  \\end{cases}\n\\end{align*}\\] where \\(\\kappa_{\\xi}=T^\\xi\\Gamma(1-\\xi)\\) for \\(\\mathfrak{e}_T\\), \\(\\kappa_{\\xi}=T^\\xi\\log(1/p)^{-\\xi}\\) for \\(\\mathfrak{q}_p\\) and \\(\\kappa_{\\xi}=\\left\\{-\\log\\left(1-{1}/{T}\\right)\\right\\}^{-\\xi}\\) for \\(z_{1/T}\\). In the Gumbel case, we have \\(\\kappa_0=\\log(T)+\\gamma_{e}\\) for \\(\\mathfrak{e}_T\\), \\(\\kappa_0=\\log(T)-\\log\\{-\\log(p)\\}\\) for \\(\\mathfrak{q}_p\\) and \\(\\kappa_0=-\\log\\{-\\log(1-1/T)\\}\\) for \\(z_{1/T}\\)."
  },
  {
    "objectID": "content/regression.html",
    "href": "content/regression.html",
    "title": "Regression models",
    "section": "",
    "text": "Most data encountered in applications display various forms of nonstationarity, including trends, time-varying variance, seasonality and covariate effects. In environmental applications, these may be partly attributed to the presence of different weather patterns or regimes, to climate change, etc. Extreme value distributions cannot capture these phenomena without modification. There is no general theory for nonstationary extremes, and therefore there are multiple strategies that one can consider for modelling.\nThe first consists in fitting a regression for the whole data and perform extreme value analysis with the residuals, as before assuming stationarity (Eastoe & Tawn, 2009). The second, proposed by Davison & Smith (1990), tries to incorporate covariates in the parameters \\(\\mu\\), \\(\\sigma\\), etc. — fixing the shape parameters is often recommended as it is hard to estimate.\nGeneral linear modelling would consist in regression models, e.g., \\[\\begin{align*}\n\\mu(\\mathbf{X}) = \\beta_0 + \\beta_1 \\mathrm{X}_1 + \\cdots \\beta_p \\mathrm{X}_p,\n\\end{align*}\\] and estimate as before parameters by maximum likelihood. The difficulty now is that there are more parameters to estimate and the support restriction translates into up to \\(n\\) inequality constraints, as they must be supported for every combination of covariates found in the database. These two facts mean numerical optimization is more difficult.\nIn models with a relatively large number of parameters, it is useful to include additive penalty terms to the log likelihood: for example, generalized additive models for the parameters include smooth functions, typically splines, with a penalty that controls the wiggliness of the estimated predictor functions. The latter is typically evaluated using the second-order derivative of the basis functions.\nFor example, consider a function of covariates \\(f(x_j) = \\sum_{k=1}^K \\beta_k b_k(x_j)\\), where \\(b_k(\\cdot)\\) is a basis function, possibly with compact support. We typically penalize the squared second derivative of the function \\(f(x)\\) to control the wiggliness of the function. Since the regression coefficients \\(\\beta\\)’s are constants, the penalty can be expressed in terms of the second derivative of the basis functions concatenated in a smooth matrix \\(\\mathbf{S}\\) with \\((i,j)\\)th entry \\(S_{ij} = \\int b_i''(x) b_j''(x)\\mathrm{d} x\\), so the penalty can be written \\(\\lambda \\boldsymbol{\\beta}^\\top \\mathbf{S}\\boldsymbol{\\beta}\\) for some tuning parameter \\(\\lambda \\geq 0\\) that controls the tuning. The penalty can be viewed in the Bayesian paradigm as an improper Gaussian prior. The optimal value of \\(\\lambda\\) for smoothing is selected by maximizing the marginal likelihood: in high-dimensional settings, or when we include interactions through tensor products, etc., optimization of \\(\\boldsymbol{\\lambda}\\) is far from trivial.\nThe mgcv package in R allows for estimation of generalized additive models using the methods described above. There are multiple choice of basis functions that can be used, including the default thin-plate spline tp, cubic splines cs and cubic cyclic splines cc for cyclic covariates such as period of year. While most software for generalized additive models will allow for The smooths consisting of linear combination of basis functions\nIn nonstationary models, risk measures of interest are defined conditionally on the value of covariates: for example, the \\(1-p\\) conditional return level is (Eastoe & Tawn, 2009) \\[\\begin{align*}\n\\Pr(Y_t  &gt; y \\mid \\mathbf{X}_t =\\boldsymbol{x}_t) = p\n\\end{align*}\\] and the corresponding unconditional return level, \\[\\begin{align*}\n\\int_{\\mathcal{X}} \\Pr(Y_t  &gt; y \\mid \\mathbf{X}_t =\\boldsymbol{x}_t) \\mathrm{d} P(\\boldsymbol{x}_t),\n\\end{align*}\\] is obtained by averaging out over the distribution of covariates that are employed in the model. For future quantities, this may or not be a sensible risk summary to compute1 and may prove tricky to obtain as it requires either knowledge about the future distribution of the covariates, or else a perhaps unrealistically strong stationary assumption.\nSome parametrizations are better suited than others for regression modelling: for the nonstationary case, the generalized Pareto model with varying scale and shape is not stationary unless, for any \\(v\\) greater than the original threshold \\(u\\), \\[\\begin{align*}\n\\sigma_v(\\boldsymbol{x}_t) = \\sigma_u(\\boldsymbol{x}_t) + (v-u) \\xi(\\boldsymbol{x}_t)\n\\end{align*}\\] which, even with constant shape \\(\\xi\\) must imply a linear or constant functional form for \\(\\sigma_u\\). Using the inhomogeneous Poisson point process representation avoids these problems."
  },
  {
    "objectID": "content/regression.html#generalized-additive-models-for-extremes",
    "href": "content/regression.html#generalized-additive-models-for-extremes",
    "title": "Regression models",
    "section": "Generalized additive models for extremes",
    "text": "Generalized additive models for extremes\nThe function evgam from the eponymous package allows one to specify smooth functional forms and objective estimation of the smoothing parameters using Laplace’s methods (Wood et al., 2016), building on the mgcv package of Simon Wood (Wood, 2017).\nThe setup is evgam(formula, data, family, ...), where formula is a list of formula for parameters (in the order location, scale, shape) and family is the character string for the extreme value distribution. Choices include gev, gpd, rlarg and ald for asymmetric Laplace, used in quantile regression, among other.\n\nlibrary(evgam)\ndata(frwind, package = \"mev\")\nlyon &lt;- with(frwind,\n             xts::xts(x = S2, order.by = date))\nymax &lt;- xts::apply.yearly(lyon, max)\nyears &lt;- unique(lubridate::year(lyon))\nopt_gev_spl &lt;- evgam::evgam(\n  data = data.frame(\n    syear = scale(years),\n    ymax = ymax),\n  formula = list(ymax ~ s(syear, k = 5, bs = \"cr\"),\n                 ~ 1, \n                 ~ 1),\n  family = \"gev\")\n## Summary with coefficients\nsummary(opt_gev_spl)\n\n\n** Parametric terms **\n\nlocation\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    36.62       0.66   55.46   &lt;2e-16\n\nlogscale\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     1.36       0.12   11.23   &lt;2e-16\n\nshape\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    -0.13       0.13   -0.99    0.162\n\n** Smooth terms **\n\nlocation\n          edf max.df Chi.sq Pr(&gt;|t|)\ns(syear) 2.25      4   5.62   0.0776\n\n## Plot splines (if any)\nplot(opt_gev_spl)\n\n\n\n## Fitted value, depend on covariates\n# predict(opt_gev_spl)\n\nInterpreting the penalty as an improper Gaussian prior, we can view this model under the Bayesian lens. The posterior distribution isn’t available in closed-form, but we can do a Gaussian approximation at the mode on a suitable scale (e.g., log-scale), sample from this multivariate Gaussian approximation of \\(p(\\boldsymbol{\\theta}) \\stackrel{\\cdot}{\\sim}\\mathsf{No}_p\\) and transform them back on the parameter scale. Given draws of \\(\\boldsymbol{\\theta}\\), we can next simulate new realizations \\(p(Y \\mid \\boldsymbol{\\theta}, \\mathbf{X}_{\\text{new}})\\) from the approximate posterior predictive for new combinations of covariates, or even for the data matrix that was used as covariates for the fitted model (default).\nFigure 1 shows the output density for the location parameter for each time period in the records, as this is the only parameter that varies as a function of time (\\(\\log(\\sigma)\\) and \\(\\xi\\) are drawn from the same marginal distribution, even if simulated sample values are different for each time period combination). If the model is severely overfitted, this will be visible because the posterior standard deviation will be tiny.\n\n## Simulate from the posterior of parameters\npost_sim &lt;- simulate(opt_gev_spl, nsim = 1000L, seed = 2023)\nlibrary(ggplot2)\nggplot(\n  data = data.frame(\n    location = c(post_sim$location),\n    year = factor(rep(1:length(years), \n               length.out = prod(dim(post_sim$location))))),\n  mapping = aes(x = location,\n                color = year,\n                group = year)) +\n  geom_density() +\n  theme_minimal() +\n  viridis::scale_color_viridis(discrete = TRUE) +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 1: Density plots of 1000 posterior samples based on a normal approximation to the posterior of the location parameter of the generalized extreme value distribution, colored by year."
  },
  {
    "objectID": "content/regression.html#footnotes",
    "href": "content/regression.html#footnotes",
    "title": "Regression models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhat does return levels mean in a nonstationary climate? See Rootzén & Katz (2013) for an alternative.↩︎"
  },
  {
    "objectID": "content/semiparametric.html",
    "href": "content/semiparametric.html",
    "title": "Semiparametric methods and Hill estimation",
    "section": "",
    "text": "There are multiple alternative estimators of the shape parameter for heavy-tailed data, the most popular of which is the Hill (1975) estimator . Consider a random sample of order statistics \\(Y_{(1)} &gt; \\cdots &gt; Y_{(n)}\\). The latter is, for exceedances above \\(Y_{(n-k)} &gt;0\\), \\[\\begin{align}\nH_{n,k}(\\boldsymbol{Y}) = \\frac{1}{k}\\sum_{i=n-k+1}^{n} \\log Y_{(i)} - \\log Y_{(n-k)}, \\qquad (j=1, 2), \\label{eq:Hillest}\n\\end{align}\\]\nUnder a second order regular variation assumption and provided \\(\\lim_{k \\to \\infty} k^{1/2}A(n/k) = \\lambda\\in \\mathbb{R}\\), Hill’s estimator is asymptotically normal with \\[\\begin{align*}\nk^{1/2}(H_{n,k} - \\xi) \\to \\mathsf{No}\\{\\lambda/(1-\\rho), \\xi^2\\}, \\qquad \\xi&gt;0, \\rho \\leq 0;\n\\end{align*}\\] the asymptotic bias term is dictated by the rate at which the number of extreme observations grows relative to the total sample size and depends on the unknown second-order auxiliary function \\(A(\\cdot)\\) which is distribution-specific. The estimator is consistent for \\(\\xi\\) provided \\(k=k(n)\\) is an intermediate sequence satisfying \\(k/n \\to 0\\) as \\(k, n \\to \\infty\\) such that \\(\\lambda \\to 0\\). Note the asymptotic variance, to be constrasted with that of the maximum likelihood estimator of the shape for the generalized Pareto shape, which is \\((1+\\xi)^2\\)."
  },
  {
    "objectID": "content/semiparametric.html#threshold-selection-for-hills-estimator",
    "href": "content/semiparametric.html#threshold-selection-for-hills-estimator",
    "title": "Semiparametric methods and Hill estimation",
    "section": "Threshold selection for Hill’s estimator",
    "text": "Threshold selection for Hill’s estimator\n\ndata(frwind, package = \"mev\")\nlyon &lt;- sort(frwind$S2, decreasing = TRUE)\nremotes::install_github(\"lbelzile/rbm\", quiet = TRUE)\n# Fit Hill's estimator\nhill_est &lt;- rbm::hill(data_array = lyon,\n                      idx = 1:400)\nplot(hill_est)\n\n\n\n\nWe can see that the Hill estimator values are quite stable (which is seldom the case) and however around 0.1. However, maximum likelihood estimators of the shape parameter \\(\\xi\\) are much closer to zero and negative for other wind speed series. The noticeable tilted staircase pattern is an artefact of rounding.\nA simple graphical method for choosing the number of order statistics to keep is a plot of the rank against shape parameters, termed Hill plots. Practical recommendations are that (1) the number of order statistics should be restricted (say 20 to 500), (2) the graph is more easily interpreted when the \\(x\\)-axis shows normalized log ranks \\(\\log(k)/\\log(n)\\), and (3) parameters should be smoothed using a moving window estimator, as the sample path of the Hill estimator are analogous to a Brownian motion.\nNote that Hill estimator is not location invariant. The pointwise confidence intervals reported by must methods are based on exact Pareto tails, so are at best approximate.\n\nevmix::hillplot(data = lyon[1:300],\n                hill.type = \"SmooHill\", \n                r = 3L, \n                x.theta = TRUE)\n\n\n\n\nThere are multiple alternative estimators of the shape parameter: extensive simulation studies show that the threshold selection performance using the random block maxima estimator of Wager (2014) is competitive. The latter is a \\(U\\) statistic and has \\(\\mathcal{C}^{\\infty}\\) sample paths: the selection is based on empirical risk minimization using a finite-difference approximation to the squared derivative of the process, subject to a penalty term. Although the computational cost is higher than Hill’s estimator, it can be kept reasonable by restricting attention to only largest exceedances.\n\nrbm_est &lt;- rbm::rbm.point_estimate(lyon[1:300])\nplot &lt;- rbm::rbm.plot(lyon[1:300])\n\n\n\n\nOther threshold selection methods, including the minimization of the asymptotic mean squared error of the Hill estimator highlighted in Section 2 of Caeiro & Gomes (2016), also works well but can fail catastrophically in some settings. Here, the answer (in terms of the tail index \\(\\alpha=1/\\xi\\)), is similar to other packages\n\nest_AMSE &lt;- tea::dAMSE(lyon[1:300])\n\nMany such estimators are available from the tea package. Extensions that deal with censoring and conditional estimators (ReIns), time series (extremefit), etc. but we will not attempt to cover those."
  },
  {
    "objectID": "content/semiparametric.html#quantile-estimator",
    "href": "content/semiparametric.html#quantile-estimator",
    "title": "Semiparametric methods and Hill estimation",
    "section": "Quantile estimator",
    "text": "Quantile estimator\nGiven an estimate of a positive shape parameter, we can get quantile estimates through Weissman (1978) formula. The estimator of the quantile at level \\(1-p\\), for small \\(p\\), is \\[\\begin{align*}\nQ^W_{k,n}(1-p) = Y_{(n-k)} \\left\\{ \\frac{k+1}{p(n+1)} \\right\\}^{H_{k,n}},                                      \n\\end{align*}\\]\nwhere \\(H_{k,n}\\) is the Hill estimator of the shape parameter and \\(Y_{(n-k)}\\) is the \\((n-k)\\)th order statistic, acting as threshold. While there are software that return these quantities, including the ReIns, evt0 and extremefit packages, they are easily coded.\nBelow, we show how to estimate the 0.999 quantile of the distribution by extrapolating the shape\n\nqweissman &lt;- function(n, p, k, thresh, shape){\n  thresh * ((k + 1) / (1 - p)/ (n + 1))^shape\n}\nquants &lt;- qweissman(n = length(lyon), \n          p = seq(0.99, 0.999, length.out = 101), \n          thresh = est_AMSE$threshold,\n          k = est_AMSE$k0, \n          shape = 1/est_AMSE$tail.index)\n\nUncertainty statements, if any, could be obtained by bootstrap methods or using asymptotic normality, but given the sampling distribution of the quantile estimator is strongly asymmetric, Wald-type (symmetric) confidence intervals are bound to give poor coverage."
  },
  {
    "objectID": "content/timeseries.html",
    "href": "content/timeseries.html",
    "title": "Time series",
    "section": "",
    "text": "To begin, we load the R packages that we will use. Remember: to view the help page for a command in R, just type ?command with command replaced by the name of the function.\nLet’s first load the wind speed data from the mev package.\n\ndata(frwind, package = \"mev\")\n#?frwind"
  },
  {
    "objectID": "content/timeseries.html#exploratory-analyses",
    "href": "content/timeseries.html#exploratory-analyses",
    "title": "Time series",
    "section": "Exploratory analyses",
    "text": "Exploratory analyses\nNext, we explore the extremogram (also called auto tail dependence function), as implemented in the package extRemes. We set a probability level for the threshold corresponding to correspond to approximately one exceedance per month.\n\nprob = 1-1/30\nmaxlag = 7\natdf(obs, u = prob, lag.max = maxlag)\n\n\n\n\nClearly, extremal dependance decays quite fast, and the chi-bar coefficient (here called rhobar) points towards asymptotic independence for all positive time lags. Nevertheless, using the standard methods assuming asymptotic dependence can still be useful but may overestimate the strength of extremal correlation at very high quantile levels.\nWe can also perform a \\(\\overline{\\chi}\\)-based analysis using tsdep.plot in the POT package. If necessary, we can reset graphics device with graphics.off().\n\npar(mfrow = c(1,1))\nu = quantile(obs, prob)\ntsdep.plot(obs, u = u, lag.max = 7) # POT package\n\n\n\n\nThe confidence bounds are relatively large but they tend to be bounded away from 1 (implying asymptotic independence).\nA similar analysis can be done with the extremogram package. We here estimate the upper-tail extremogram. We first produce an extremogram plot. Confidence bounds can also be obtained.\n\next = extremogram1(\n    obs, \n    quant = prob, \n    type = 1, \n    maxlag = maxlag)\n\n\n\nbootconf1(obs, \n          R = 100, \n          l = 30, \n          maxlag = maxlag,\n          quant = prob, \n          type = 1,\n          par = 1, \n          alpha = 0.05)\npoints(x = 1:(maxlag-1), \n       y = ext[-1], \n       pch = 19, \n       col = \"blue\")\n\n\n\n\nWe note that there is still room for improvement of the output of these functions.\nIt is also possible to calculate cross-extremogram between two measurement stations, and corresponding confidence bounds.\n\nobs1 = frwind$S2\nobs2 = frwind$S3\next_cross = extremogram2(\n  cbind(obs1, obs2), \n  quant1 = prob, \n  quant2 = prob, \n  type = 1, \n  maxlag = maxlag)\n\n\n\nbootconf2(\n  cbind(obs1, obs2), \n  R = 100, \n  l = 30, \n  maxlag = maxlag, \n  quant1 = prob, \n  quant2 = prob, \n  type = 1, \n  par = 1, \n  alpha = 0.05)\npoints(x = 1:(maxlag-1),\n       y = ext_cross[-1], \n       pch = 19, \n       col = \"blue\")\n\n\n\n\nWe find that the extremal dependence across the two series is very weak here."
  },
  {
    "objectID": "content/timeseries.html#estimation-of-the-extremal-index",
    "href": "content/timeseries.html#estimation-of-the-extremal-index",
    "title": "Time series",
    "section": "Estimation of the extremal index",
    "text": "Estimation of the extremal index\nA large number of estimation approaches are available in the exdex package (type ?exdex” for the help package); see also the package vignettefor more detailed information.\nWe first explore estimation based on maxima for sliding blocks. Here, we use block sizes of approximately one month. We further obtain and visualize symmetric confidence intervals.\n\ntheta = spm(obs, 30)\nsummary(theta)\n\n\nCall:\nspm(data = obs, b = 30)\n\n                  Estimate Std. Error Bias adj.\nN2015, sliding      0.6755    0.02495  0.002101\nBB2018, sliding     0.7158    0.02356  0.002026\nBB2018b, sliding    0.6825    0.02356  0.035360\nN2015, disjoint     0.6830    0.02857  0.002387\nBB2018, disjoint    0.7233    0.02779  0.002331\nBB2018b, disjoint   0.6900    0.02779  0.035660\n\nconf = confint(theta)\nconf = confint(theta, interval_type = \"lik\")\nplot(conf)\n\n\n\n\nThe estimation of the extremal index can be sensitive to the block size. We check this for longer and shorter blocks.\n\ntheta2 = spm(obs, 14)\ntheta3 = spm(obs, 60)\ncbind(theta2$theta_sl, \n      theta$theta_sl, \n      theta3$theta_sl)\n\n             [,1]      [,2]      [,3]\nN2015   0.6335512 0.6754860 0.6845424\nBB2018  0.7187804 0.7158157 0.7038313\nBB2018b 0.6473518 0.6824824 0.6871646\n\n\nThere is some moderate variability in the estimates.\nMoreover, there is a tool to help choosing an optimal block size. The execution of the following command can take more than one minute. Here, we explore block sizes between one and ten weeks. Note that certain small b-values may be too small for the sampling variance of the sliding blocks estimator to be estimated.\n\nb_vals = 7*1:10\nres = choose_b(obs, b_vals)\nplot(res, ylim = c(0, 1))\n\n\n\n\nHere, even for a relatively small block size (\\(b=7\\)), the estimate is not significantly different from results for larger blocks, and it makes sense to use the estimate obtained for \\(b=7\\).\nAnother possibility is to estimate the extremal index based on threshold exceedances. We set a probability level for the threshold corresponding to correspond to approximately one exceedance per month. We here explore two possible estimator, the \\(K\\)-gaps and $D$-gaps estimators.\n\nprob = 1-1/30\nu = quantile(obs, prob)\ntheta = kgaps(obs, u, k = 1)\nsummary(theta)\n\n\nCall:\nkgaps(data = obs, u = u, k = 1)\n\n      Estimate Std. Error\ntheta   0.7973    0.01532\n\ntheta = dgaps(obs, u, D = 3)\nsummary(theta)\n\n\nCall:\ndgaps(data = obs, u = u, D = 3)\n\n      Estimate Std. Error\ntheta   0.7936    0.01902\n\n\nAgain, for the sake of choosing optimal tuning parameters, we have visual tool for the \\(K\\)-gaps estimator to explore different thresholds and \\(K\\)-values.\n\nres = choose_uk(obs, u = quantile(obs, 1-1/c(7, 14, 30, 60, 90, 180)), k = 1:7) \nplot(res, y = \"theta\")\n\n\n\n\nThe estimates look more stable only at relatively high thresholds. In practice, this lack of asymptotic stability makes threshold choice difficult. However, sometimes, there may be a specific application-relevant threshold that provides a natural choice. For example, the monthly return level with \\(K=3\\) leads to estimates similar to the blocks estimates.\nAnother implementation of an extremal-index estimator comes from the extRemes package.\n\nextremalindex(obs, u, method = \"runs\", run.length = 2) \n\n\n Runs Estimator for the Extremal Index\n    extremal.index number.of.clusters         run.length \n         0.7628319        431.0000000          2.0000000 \n\nextremalindex(obs, u, method = \"intervals\")\n\n\n Intervals Method Estimator for the Extremal Index\nNULL\n\n theta.tilde used because there exist inter-exceedance times &gt; 2.\n    extremal.index number.of.clusters         run.length \n         0.6547669        358.0000000          7.0000000 \n\n\nIn general, it is recommended to check sensitivity of estimates to different methods/thresholds."
  },
  {
    "objectID": "content/timeseries.html#peaks-over-threshold-with-declustering",
    "href": "content/timeseries.html#peaks-over-threshold-with-declustering",
    "title": "Time series",
    "section": "Peaks-over-threshold with declustering",
    "text": "Peaks-over-threshold with declustering\nHere, we exemplify the approach using the implementation in the extRemes package. We also provide a comparison to estimation without any prior declustering.\n\nobs_decluster = decluster(\n  x = obs, \n  threshold = u, \n  method = \"runs\", \n  run.length = 2)\nfit = fevd(\n  x = obs_decluster, \n  threshold = u, \n  type = \"GP\", \n  time.units = \"365/year\")\nprint(fit)\n\n\nfevd(x = obs_decluster, threshold = u, type = \"GP\", time.units = \"365/year\")\n\n[1] \"Estimation Method used: MLE\"\n\n\n Negative Log-Likelihood Value:  1313.137 \n\n\n Estimated parameters:\n     scale      shape \n 7.7617037 -0.1180915 \n\n Standard Error Estimates:\n    scale     shape \n0.4581816 0.0358274 \n\n Estimated parameter covariance matrix.\n            scale        shape\nscale  0.20993036 -0.011767376\nshape -0.01176738  0.001283602\n\n AIC = 2630.274 \n\n BIC = 2638.484 \n\nplot(fit)\n\n\n\nfit2 = fevd(x = obs, \n            threshold = u, \n            type = \"GP\", \n            time.units = \"365/year\")\nprint(fit2)\n\n\nfevd(x = obs, threshold = u, type = \"GP\", time.units = \"365/year\")\n\n[1] \"Estimation Method used: MLE\"\n\n\n Negative Log-Likelihood Value:  1624.892 \n\n\n Estimated parameters:\n     scale      shape \n 7.2300607 -0.1023352 \n\n Standard Error Estimates:\n     scale      shape \n0.38740656 0.03346132 \n\n Estimated parameter covariance matrix.\n             scale        shape\nscale  0.150083839 -0.009270985\nshape -0.009270985  0.001119660\n\n AIC = 3253.784 \n\n BIC = 3262.457 \n\nplot(fit2)\n\n\n\n\nAs suggested by theory, the estimated values are relatively close. The standard error estimates without declustering are lower but may be biased towards too low values."
  },
  {
    "objectID": "content/timeseries.html#likelihood-estimation-of-the-extremal-index-and-the-gpd-without-declustering",
    "href": "content/timeseries.html#likelihood-estimation-of-the-extremal-index-and-the-gpd-without-declustering",
    "title": "Time series",
    "section": "Likelihood estimation of the extremal index and the GPD without declustering",
    "text": "Likelihood estimation of the extremal index and the GPD without declustering\nThe lite package uses all exceedances for likelihood-based estimation with an adjustment to avoid biased uncertainty estimates. This avoids wasting information when we keep only cluster maxima. The \\(K\\)-gaps estimator is used for the extremal index. Remember that there is the function choose_uk(…)from above to help with the choice of \\(K\\) and the threshold. Finally, we can produce unbiased confidence intervals for parameters.\n\nfit = flite(obs, u = u, k = 3)\nsummary(fit)\n\n\nCall:\nflite(data = obs, u = u, k = 3)\n\n         Estimate Std. Error\np[u]      0.03283   0.001358\nsigma[u]  7.23000   0.377000\nxi       -0.10230   0.036500\ntheta     0.74510   0.016190\n\nconfint(fit)\n\nWaiting for profiling to be done...\nWaiting for profiling to be done...\nWaiting for profiling to be done...\n\n\n              2.5%       97.5%\npu      0.03024674  0.03556012\nsigmau  6.50184018  7.98516959\nxi     -0.15912634 -0.01711611\ntheta   0.71278235  0.77615902"
  },
  {
    "objectID": "content/timeseries.html#joint-modeling-of-margins-and-dependence-of-time-series-extremes",
    "href": "content/timeseries.html#joint-modeling-of-margins-and-dependence-of-time-series-extremes",
    "title": "Time series",
    "section": "Joint modeling of margins and dependence of time-series extremes",
    "text": "Joint modeling of margins and dependence of time-series extremes\nWe show how a first-order Markov chain model can be fitted using a censored likelihood thanks to the POT package. Here, we use the bivariate logistic distribution to define the Markov chain transitions densities. Some graphical output is available to diagnostic the fitted model. We first illustrate the estimation for simulated data where the true parameters are known.\n\nset.seed(2)\nobs_simulated = simmc(\n  n = 5000, \n  alpha = .5, \n  model = \"log\", \n  margin = \"gumbel\")\nfit_mc_simulated = fitmcgpd(\n  obs_simulated, \n  threshold = quantile(obs_simulated, 0.95),\n  model = \"log\")\nfit_mc_simulated\n\n\nCall: fitmcgpd(data = obs_simulated, threshold = quantile(obs_simulated,      0.95), model = \"log\") \nEstimator: MLE \nDependence Model and Strenght:\n    Model : Logistic \n    lim_u Pr[ X_1 &gt; u | X_2 &gt; u] = 0.565 \nDeviance: 1646.856 \n     AIC: 1652.856 \n\nThreshold Call: \nNumber Above: 250 \nProportion Above: 0.05 \n\nEstimates\n  scale    shape    alpha  \n0.90763  0.02984  0.52114  \n\nStandard Errors\n  scale    shape    alpha  \n0.10411  0.06264  0.03056  \n\nAsymptotic Variance Covariance\n       scale       shape       alpha     \nscale   0.0108383  -0.0036266  -0.0019415\nshape  -0.0036266   0.0039242  -0.0001326\nalpha  -0.0019415  -0.0001326   0.0009339\n\nOptimization Information\n  Convergence: successful \n  Function Evaluations: 26 \n  Gradient Evaluations: 8 \n\nplot(fit_mc_simulated)\n\n\n\n\n\n\n\nWarning in retlev.mcpot(x, opy = opy, exi = exi, main = mains[4], ...):\nArgument ``opy'' is missing. Setting it to 365.25.\n\n\n\n\n\n\n\n\nEstimates are quite close to simulated parameters (scale = 1, shape = 0, alpha = 0.5)\nNext, we apply the method to our wind speed data. We also compare its estimation of marginal parameters to a marginal fit that does not take into account the dependence.\n\nfit_mc = fitmcgpd(obs, u, model = \"log\")\nfit_mc\n\n\nCall: fitmcgpd(data = obs, threshold = u, model = \"log\") \nEstimator: MLE \nDependence Model and Strenght:\n    Model : Logistic \n    lim_u Pr[ X_1 &gt; u | X_2 &gt; u] = 0.18 \nDeviance: 7927.214 \n     AIC: 7933.214 \n\nThreshold Call: \nNumber Above: 565 \nProportion Above: 0.0328 \n\nEstimates\n   scale     shape     alpha  \n 7.31821  -0.07324   0.86402  \n\nStandard Errors\n  scale    shape    alpha  \n0.43003  0.03833  0.01356  \n\nAsymptotic Variance Covariance\n       scale       shape       alpha     \nscale   1.849e-01  -1.122e-02  -2.001e-03\nshape  -1.122e-02   1.469e-03  -4.220e-06\nalpha  -2.001e-03  -4.220e-06   1.838e-04\n\nOptimization Information\n  Convergence: successful \n  Function Evaluations: 47 \n  Gradient Evaluations: 12 \n\nplot(fit_mc)\n\n\n\n\n\n\n\nWarning in retlev.mcpot(x, opy = opy, exi = exi, main = mains[4], ...):\nArgument ``opy'' is missing. Setting it to 365.25.\n\n\n\n\n\n\n\nfit_gpd = fitgpd(obs, u, est = \"mle\")\nfit_gpd \n\nEstimator: MLE \n Deviance: 3249.784 \n      AIC: 3253.784 \n\nVarying Threshold: FALSE \n\n  Threshold Call: c(\"96.66667%\" = 51.84) \n    Number Above: 565 \nProportion Above: 0.0328 \n\nEstimates\n  scale    shape  \n 7.2300  -0.1023  \n\nStandard Error Type: observed \n\nStandard Errors\n  scale    shape  \n0.38740  0.03346  \n\nAsymptotic Variance Covariance\n       scale      shape    \nscale   0.150081  -0.009271\nshape  -0.009271   0.001120\n\nOptimization Information\n  Convergence: successful \n  Function Evaluations: 29 \n  Gradient Evaluations: 9 \n\n\nGood news: The marginal parameters estimates are very similar."
  },
  {
    "objectID": "content/timeseries.html#exercice",
    "href": "content/timeseries.html#exercice",
    "title": "Time series",
    "section": "Exercice",
    "text": "Exercice\nPerform a similar analysis for one or several of the other stations (wind speeds) or for temperature data, and interpret results and compare them across stations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Description",
    "section": "",
    "text": "Description\nThis satellite workshop will review R implementations of a variety of techniques for statistical analysis of extreme values. The focus of the first part of the workshop will be on univariate extremes, including likelihood-based, Bayesian and nonparametric methods for both peaks-over-threshold and block maxima approaches, with a foray into nonstationary extremes. The second part of the workshop will concentrate on conditional extremes and time series.\nThe tutorial takes place Friday, June 30th, 2023, from 14:00 until 18:15 in room Aula Info AS04.\n\n\nCourse content\n\nLikelihood-based modelling: slides, notes, code\nSemiparametric methods: notes, code\nBayesian inference: slides, notes, code\nNonstationary extremes: slides, notes, code\nTime series: slides, notes , code\nConditional extremes model: slides, notes , code\n\n\n\nExercices\n\nExercise sheet and some code to solve them.\n\n\n\nInstructions\nWe will be using multiple R packages from the Comprehensive R Archive Network, as well as development versions of some packages.\nIf you plan on using your own laptop, download and install R (current version 4.3.0, nicknamed “Already Tomorrow”) and an integrated development environment such as RStudio.\n\ndevpkgs &lt;- c(\"lbelzile/mev\", \"lbelzile/rbm\")\ncranpkgs &lt;- c(\"lite\", \"revdbayes\", \"texmex\", \"POT\", \n              \"coda\", \"gridExtra\", \"patchwork\",\n              \"GGally\", \"extRemes\", \"evd\", \"extremogram\", \n              \"threshr\", \"tea\", \"evt0\", \"remotes\",\n              \"xts\", \"lubridate\", \"ggplot2\", \"exdex\")\ninstall.packages(cranpkgs)\nremotes::install_github(devpkgs)\n\nYou may need to download the R tool chain, the clang and gfortran on Mac to build packages.\nIf you cannot download the package mev, use the CRAN version via install.packages(\"mev\") and download the data from here.\n\n\nInstructors\n\nThomas Opitz, INRAE\nLéo Belzile, HEC Montréal"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#bayesian-inference",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#bayesian-inference",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Bayesian inference",
    "text": "Bayesian inference\n\nCombine likelihood with a prior: posterior.\n\n\\[\np(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y}) = \\frac{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{ \\int p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\mathrm{d}\\boldsymbol{\\theta}},\n\\qquad(1)\\]\n\nPosterior is proper (if likelihood times prior is integrable).\nParameters asymptotically normal in large samples under general conditions.\nNeed to evaluate integral in denominator (normalizing constant that does not depend on \\(\\boldsymbol{\\theta}\\))."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#change-of-paradigm",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#change-of-paradigm",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Change of paradigm",
    "text": "Change of paradigm\nIn the Bayesian world,\n\ninference is conditional on observations,\nparameters are treated as random variables\n\n(reflecting uncertainty about their true value)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#inference",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#inference",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Inference",
    "text": "Inference\n\nIn all scenarios, we will have samples from the posterior distribution for parameters \\(\\boldsymbol{\\theta}\\): any summary can be computed from these via Monte Carlo methods.\ne.g., we can report the posterior mean or median\ncredible intervals (probability that parameter falls in) are simply quantiles of functionals"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#prediction",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#prediction",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Prediction",
    "text": "Prediction\nWe consider new realizations from posterior predictive \\[\\begin{align*}\np(Y_{\\text{new}} \\mid \\boldsymbol{Y}) =\n\\int_{\\Theta} p(Y_{\\text{new}}  \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} \\mid  \\boldsymbol{Y}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\\]\n\nFor every posterior draw \\(\\boldsymbol{\\theta}_b\\) \\((b=1, \\ldots, B)\\), sample new realization from generative model.\nMarginalization means discarding \\(\\boldsymbol{\\theta}_b\\)’s"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#why-bother",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#why-bother",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Why bother?",
    "text": "Why bother?\n\nPriors can act as penalty\n\nregularizing parameter estimates in small samples.\n\nNatural extensions via hierarchical modelling (e.g., random effects).\nExpert knowledge can be encoded through the priors."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#intractable-models",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#intractable-models",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Intractable models",
    "text": "Intractable models\nTrade optimization for integration problem!\nThe integral is intractable, so we need to resort to alternative schemes:\n\nNumerical integration via Laplace approximations (INLA)\nMonte Carlo methods to simulate (approximate) draws from the posterior"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#monte-carlo-methods",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#monte-carlo-methods",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Monte Carlo methods",
    "text": "Monte Carlo methods\n\nMost packages implement offer Markov chain Monte Carlo algorithms\n\ngives approximate correlated draws\nrequires tuning\nneed to check convergence\n\n\nFor models without covariates, Northrop (2023) offers exact draws from posterior via the ratio-of-uniform algorithm."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#bayesian-dispute",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#bayesian-dispute",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Bayesian dispute",
    "text": "Bayesian dispute\nOne objection to Bayesian inference is arbitrary selection of priors\n\nInfluence of the prior vanishes as sample size grows unless it imposes support constraints!\nLikelihood are also often used for convenience\nSame for lasso and ridge (\\(L_1\\) and \\(L_2\\)) penalties in regression."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#prior-selection",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#prior-selection",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Prior selection",
    "text": "Prior selection\n\nTypically, parameters are assumed independent apriori (so prior factorizes), even if posterior doesn’t\nChoosing proper priors (density functions) ensure proper posteriors.\nCare is needed, especially for shape parameter"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#priors-for-extreme-value-models-12",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#priors-for-extreme-value-models-12",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Priors for extreme value models (1/2)",
    "text": "Priors for extreme value models (1/2)\nPopular choices include\n\nimproper uniform priors\nvague or diffuse Gaussian priors, e.g., \\(\\xi \\sim \\mathsf{No}(0, 10)\\)\nJeffrey’s prior (invariant to reparametrization.\n\nfor generalized Pareto \\[\\sigma^{-1}(1+\\xi)^{-1}(1+2\\xi)^{-1/2}\\mathrm{I}_{\\xi &gt; -1/2}\\]\ninvalid for the GEV regardless of sample size(Northrop & Attalides, 2016)!"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#priors-for-extreme-value-models-22",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#priors-for-extreme-value-models-22",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Priors for extreme value models (2/2)",
    "text": "Priors for extreme value models (2/2)\nBetter choices penalize the shape, including\n\nMartins & Stedinger (2001) prior \\(\\xi + 0.5 \\sim \\mathsf{Beta}(6, 9)\\)\nMaximal data information (MDI): \\(\\sigma^{-1}\\exp\\{-a(1+\\xi)\\}\\mathrm{I}_{\\xi \\geq -1}\\) Northrop & Attalides (2016)\nPrior on quantiles (Coles & Tawn, 1996), back-transformed"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#impact-of-priors",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#impact-of-priors",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Impact of priors",
    "text": "Impact of priors\n\nFigure 1: Marginal posterior of GEV shape parameter for different prior distributions."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#ratio-of-uniform-sampling",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#ratio-of-uniform-sampling",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Ratio-of-uniform sampling",
    "text": "Ratio-of-uniform sampling\nThe revdbayes (Northrop, 2023) provides the state-of-the-art for stationary models (faster than MCMC, exact sampling).\n\nlibrary(revdbayes)\npost &lt;- revdbayes::rpost_rcpp(\n  n = 1e4L, # number of posterior samples\n  model = \"gev\", # extreme value distribution\n  data = ymax, # vector of yearly maximum\n  prior = set_prior(prior = \"mdi\", model = \"gev\"),\n  nrep = 100) # number of post. predictive samples"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#point-estimators-and-credible-intervals",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#point-estimators-and-credible-intervals",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Point estimators and credible intervals",
    "text": "Point estimators and credible intervals\nFor each combination of posterior draw, compute the functional of interest (e.g., the median of the 50-year maximum) and/or quantiles.\n\npost_gev_Nmed &lt;- apply(post$sim_vals, 1, function(x){\n  revdbayes::qgev(p = 0.5, loc = x[1], scale = x[2], \n                  shape = x[3], m = 50)\n})\n# Posterior mean\nmean(post_gev_Nmed)\n\n[1] 54.15799\n\n# To get a 95% credible interval, simply compute quantiles\nquantile(post_gev_Nmed, c(0.025, 0.975))\n\n    2.5%    97.5% \n47.98801 67.98414"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#posterior-predictive-samples",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#posterior-predictive-samples",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Posterior predictive samples",
    "text": "Posterior predictive samples\nHere, using max-stability to get a new observation (maximum of 50 yearly max) for each draw \\(\\boldsymbol{\\theta}\\).\n\npostpred &lt;- revdbayes::rgev(\n  n = nrow(post$sim_vals), \n  loc = post$sim_vals[,1],\n  scale = post$sim_vals[,2],\n  shape = post$sim_vals[,3],\n  m = 50)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#posterior-predictive",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#posterior-predictive",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Posterior predictive",
    "text": "Posterior predictive\n\nFigure 2: Density of posterior predictive (black) and posterior median (grey) of 50 year maximum."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#diagnostics-for-bayesian-workflow",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#diagnostics-for-bayesian-workflow",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Diagnostics for Bayesian workflow",
    "text": "Diagnostics for Bayesian workflow\nCan model capture summaries of data?\n\npp_check(post, stat = median)\n\n\nSee bayesplot and loo packages for more information and options about the Bayesian workflow (Gabry et al., 2019)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#regression-model",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#regression-model",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Regression model",
    "text": "Regression model\n\nlibrary(texmex)\npost_reg &lt;- texmex::evm(\n  y = ymax, \n  data = data.frame(\n    syear = scale(1976:2023), \n    ymax = ymax),\n  family = texmex::gev,\n  method = \"simulate\",\n  burn = 1000L,\n  chains = 4L,\n  iter  = 1.1e4,\n  proposal.dist = \"gaussian\",\n  mu = ~ syear,\n  verbose = FALSE)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#posterior-estimates",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#posterior-estimates",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Posterior estimates",
    "text": "Posterior estimates\n\nsummary(post_reg)\n\nFamily:       GEV \n\nPosterior summary:\n                 Posterior mean        SD\nmu: (Intercept)     36.30933484 0.7041629\nmu: syear           -0.66354673 0.7182214\nphi: (Intercept)     1.44282072 0.1293479\nxi: (Intercept)     -0.04462734 0.1352223\n\n\n\n## Default 'plot' method for texmex objects:\n# density, traceplots and correlograms plots\nlibrary(gridExtra)\nggplot(post_reg)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#monitoring-markov-chain-monte-carlo",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#monitoring-markov-chain-monte-carlo",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Monitoring Markov chain Monte Carlo",
    "text": "Monitoring Markov chain Monte Carlo\n\nrun multiple Markov chains from different starting values\ndiscard initial samples (burn-in)\n\ncheck convergence:\n\nare all chains converging to the same region\nare chains stationary?"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#effective-sample-size",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#effective-sample-size",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Effective sample size",
    "text": "Effective sample size\nWith multiple chains, it is easier to use output of coda package.\nFor summaries to be reliable (e.g., quantiles of posterior), the approximate number of independent samples from the total simulated should be large enough.\n\nchains &lt;- coda::as.mcmc.list(\n  lapply(post_reg$chains, coda::as.mcmc))\n# effective sample size is sufficient here\ncoda::effectiveSize(chains)\n\n    var1     var2     var3     var4 \n2763.133 3121.967 3024.125 3093.349 \n\n\nCan compare algorithms efficiency via effective sample size per second."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#traceplots",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#traceplots",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Traceplots",
    "text": "Traceplots\n\n## Default 'plot' method for texmex objects:\n# density, traceplots and correlograms plots\nplot(chains, density = FALSE)\n\n\nShould look like fat hairy catterpilars."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#algorithm-efficiency",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#algorithm-efficiency",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Algorithm efficiency",
    "text": "Algorithm efficiency\nWhile some MCMC algorithms are more costly, they yield samples that are less autocorrelated (so contain more information altogether).\n\nIs autocorrelation reasonable?\nRelated to acceptance rate (Goldilock principle)\nIdeally, algorithms sample parameters on unconstrained space\nGood proposals adapt to the local geometry and satisfy support constraints"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#correlograms",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#correlograms",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Correlograms",
    "text": "Correlograms\n\ncoda::acfplot(chains)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#summaries",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#summaries",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Summaries",
    "text": "Summaries\nWe can get directly summaries (e.g., posterior sample mean)\n\nstandard errors for functionals is more complicated due to the autocorrelation\n\nDue to autocorrelation, dedicated methods are employed to get standard errors.\n\nsummary(chains[[1]])\n\n       V1              V2                V3              V4          \n Min.   :33.91   Min.   :-3.1841   Min.   :1.064   Min.   :-0.46361  \n 1st Qu.:35.81   1st Qu.:-1.1247   1st Qu.:1.352   1st Qu.:-0.13404  \n Median :36.30   Median :-0.6561   Median :1.436   Median :-0.05592  \n Mean   :36.31   Mean   :-0.6523   Mean   :1.441   Mean   :-0.04522  \n 3rd Qu.:36.78   3rd Qu.:-0.1614   3rd Qu.:1.525   3rd Qu.: 0.04187  \n Max.   :38.73   Max.   : 1.7285   Max.   :1.910   Max.   : 0.41994"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#loss-function",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#loss-function",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Loss function",
    "text": "Loss function\nCustom loss function for the return levels \\(q\\) from EVA data challenge \\[\\begin{align*}\nL(q, \\widehat{q}(\\theta)) =\n\\begin{cases}\n0.9(0.99q - \\widehat{q}), & 0.99q &gt; \\widehat{q} \\\\\n0, & |q - \\widehat{q}| \\leq 0.01 q\\\\\n0.1(\\widehat{q} - 1.01q), & 1.01q &lt; \\widehat{q}.\n\\end{cases}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#bayesian-evaluation-of-loss-function",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#bayesian-evaluation-of-loss-function",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Bayesian evaluation of loss function",
    "text": "Bayesian evaluation of loss function\nFor given value of the return level \\(q_0\\): \\[\\begin{align*}\nr(q_0) = \\int_{\\boldsymbol{\\Theta}}L(q(\\boldsymbol{\\theta}), q_0) p (\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\\] and we seek to minimize the risk, \\(\\mathrm{min}_{q_0 \\in \\mathbb{R}_{+}} r(q_0)\\)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#loss-function-1",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#loss-function-1",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Loss function",
    "text": "Loss function\n\nFigure 3: Loss function for wind speed data, based on fitting a generalized extreme value distribution to annual maxima."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-bayesian.html#references",
    "href": "slides/EVA2023-Rsoftware-bayesian.html#references",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nColes, S. G., & Tawn, J. A. (1996). A Bayesian analysis of extreme rainfall data. Journal of the Royal Statistical Society. Series C (Applied Statistics), 45(4), 463–478. https://doi.org/10.2307/2986068\n\n\nGabry, J., Simpson, D., Vehtari, A., Betancourt, M., & Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society Series A: Statistics in Society, 182(2), 389–402. https://doi.org/10.1111/rssa.12378\n\n\nMartins, E. S., & Stedinger, J. R. (2001). Generalized maximum likelihood Pareto-Poisson estimators for partial duration series. Water Resources Research, 37(10), 2551–2557. https://doi.org/10.1029/2001WR000367\n\n\nNorthrop, P. J. (2023). revdbayes: Ratio-of-uniforms sampling for Bayesian extreme value analysis.\n\n\nNorthrop, P. J., & Attalides, N. (2016). Posterior propriety in Bayesian extreme value analyses using reference priors. Statistica Sinica, 26(2), 721–743. https://doi.org/10.5705/ss.2014.034"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#basics-of-likelihoods",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#basics-of-likelihoods",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Basics of likelihoods",
    "text": "Basics of likelihoods\n\nDenote by \\(\\boldsymbol{\\theta} \\in \\mathcal{S} \\subseteq \\mathbb{R}^p\\) the parameter vector.\nAssume data has joint density \\(f(\\boldsymbol{y}; \\boldsymbol{\\theta})\\).\nThe log likelihood is \\(\\ell(\\boldsymbol{\\theta}) = \\log f(\\boldsymbol{y}; \\boldsymbol{\\theta})\\).\nIf the \\(n\\) observations are independent with density or mass function \\(f_i\\), then \\(\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\log f_i(y_i; \\boldsymbol{\\theta})\\).\nThe maximum likelihood estimate \\(\\widehat{\\boldsymbol{\\theta}}\\) is found by maximizing (numerically) \\(\\ell(\\boldsymbol{\\theta})\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#why-use-maximum-likelihood",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#why-use-maximum-likelihood",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Why use maximum likelihood?",
    "text": "Why use maximum likelihood?\n\nEasy to generalize to complex settings (nonstationarity, regression models, censoring, rounding, etc.)\nKnown to be asymptotically most efficient (Cramér–Rao bound), even if they can be biased in small samples.\nPoint estimators, etc. are invariant to reparametrization."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#extremal-type-theorem",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#extremal-type-theorem",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Extremal type theorem",
    "text": "Extremal type theorem\nConsider \\(Y_i\\) \\((i=1,2,\\ldots)\\) i.i.d. with distribution \\(F\\).\nIf there exist normalizing sequences \\(a_n&gt;0\\) and \\(b_n \\in \\mathbb{R}\\) such that \\[\\begin{align}\n\\lim_{n \\to \\infty} \\Pr\\left(\\frac{\\max_{i=1}^n Y_i - b_n}{a_n} \\leq x \\right) = G(x),\n\\label{eq:gevconv}\n\\end{align}\\] for \\(G\\) a non-degenerate distribution, then \\(G\\) must be generalized extreme value (GEV)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#generalized-extreme-value-distribution",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#generalized-extreme-value-distribution",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Generalized extreme value distribution",
    "text": "Generalized extreme value distribution\nWith location \\(\\mu \\in \\mathbb{R}\\), scale \\(\\sigma \\in \\mathbb{R}_{+}\\) and shape \\(\\xi \\in \\mathbb{R}\\) parameters, the distribution function is \\[\\begin{align*}\nG(x) =\\begin{cases}\n\\exp\\left\\{-\\left(1+\\xi \\frac{x-\\mu}{\\sigma}\\right)^{-1/\\xi}_{+}\\right\\}, & \\xi \\neq 0;\\\\\n\\exp\\left\\{-\\exp\\left(-\\frac{x-\\mu}{\\sigma}\\right)\\right\\}, & \\xi = 0,\n\\end{cases}\n\\end{align*}\\] where \\(x_{+} = \\max\\{x, 0\\}\\).\nThe support is \\(\\{x \\in \\mathbb{R}: \\xi(x-\\mu)/\\sigma &gt; -1\\}\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#max-stability-property",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#max-stability-property",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Max-stability property",
    "text": "Max-stability property\nIf \\(Y_i \\sim \\mathsf{GEV}(\\mu, \\sigma, \\xi)\\) are independent, then \\[\\max_{i=1}^N Y_i \\sim \\mathsf{GEV}(\\mu_N, \\sigma_N, \\xi),\\] where\n\n\\(\\mu_N = \\mu + \\sigma(N^\\xi-1)/\\xi\\)\n\\(\\sigma_N = \\sigma N^\\xi\\)\n\n(case \\(\\xi=0\\) defined by continuity)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#block-maximum",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#block-maximum",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Block maximum",
    "text": "Block maximum\nWe can\n\npartition data into blocks of roughly equal size \\(m\\) and\nfit a GEV distribution to the maximum of the blocks.\n\n\nlibrary(mev)\nlibrary(xts)\nlibrary(lubridate)\ndata(frwinds, package = \"mev\")\nlyon &lt;- with(frwind, \n             xts(x = S2, order.by = date))\n# Create series of yearly maximum\nymax &lt;- apply.yearly(lyon, max)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#fitting-gev-using-mev-package",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#fitting-gev-using-mev-package",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Fitting GEV using mev package",
    "text": "Fitting GEV using mev package\n\nopt_gev &lt;- mev::fit.gev(xdat = ymax, show = TRUE)\n## Log-likelihood: -142 \n## \n## Estimates\n##     loc    scale    shape  \n## 36.1845   3.9429  -0.0112  \n## \n## Standard Errors\n##   loc  scale  shape  \n## 0.659  0.488  0.132  \n## \n## Optimization Information\n##   Convergence: successful \n##   Function Evaluations: 30 \n##   Gradient Evaluations: 14\nmle &lt;- coef(opt_gev)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#goodness-of-fit-diagnostics",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#goodness-of-fit-diagnostics",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Goodness-of-fit diagnostics",
    "text": "Goodness-of-fit diagnostics\nCustom methods (print, plot, coef, etc.) are defined\n\nmethods(class = \"mev_gev\")\n## [1] anova  coef   logLik nobs   plot   print  vcov  \n## see '?methods' for accessing help and source code\npar(mfrow = c(1,2))\nplot(opt_gev)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#numerical-tricks",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#numerical-tricks",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Numerical tricks",
    "text": "Numerical tricks\n\nStandardize observations (e.g., scale) to facilitate optimization — the GEV is a location-scale family.\nEven if the limit is continuous and well defined at \\(\\xi=0\\), the log likelihood and it’s derivatives involves terms of the form \\(\\log(1+\\xi x)\\), which are numerically unstable when \\(\\xi \\to 0\\).\n\nPro tip: do not code the likelihood yourself! Otherwise,\n\nuse high precision arithmetic, e.g., log1p\nreplace the terms that blow up by Taylor series expansion near \\(\\xi=0\\) (interpolation)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#score-vector",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#score-vector",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Score vector",
    "text": "Score vector\nWhen the log likelihood is differentiable, the MLE is the root of the score equation, meaning \\(\\ell_{\\boldsymbol{\\theta}}(\\widehat{\\boldsymbol{\\theta}}) = \\left.\\partial \\ell(\\boldsymbol{\\theta}) / \\partial \\boldsymbol{\\theta} \\right|_{\\boldsymbol{\\theta} = \\widehat{\\boldsymbol{\\theta}}} = \\boldsymbol{0}_p\\).\n\nmev::gev.score(par = mle, dat = ymax) # score\n## [1]  4.44e-08 -5.53e-08 -1.49e-07\n\n\nGradient-based algorithms exploit this feature for optimization\nbut beware of support constraints!\n\nBest to reparametrize so that the parameter space is \\(\\mathbb{R}^p\\) if possible."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#information-matrix-and-standard-errors",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#information-matrix-and-standard-errors",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Information matrix and standard errors",
    "text": "Information matrix and standard errors\nWe can extract standard errors by taking the square root of the diagonal elements of the inverse of either\n\nthe Fisher information, \\(\\imath(\\boldsymbol{\\theta}) = \\mathsf{Cov}\\{\\ell_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta})\\}\\) or\nthe observed information \\(\\jmath(\\boldsymbol{\\theta}) = - \\partial^2 \\ell(\\boldsymbol{\\theta})/ \\partial \\boldsymbol{\\theta} \\partial \\boldsymbol{\\theta}^\\top\\),\n\nboth evaluated at the MLE \\(\\widehat{\\boldsymbol{\\theta}}\\).\n\n# Compute observed information matrix\njmat &lt;- mev::gev.infomat(par = mle, dat = ymax)\n# Compute standard errors\nsqrt(diag(solve(jmat)))\n##   loc scale shape \n## 0.659 0.488 0.132\n# Compare with opt$std.err"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#some-remarks",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#some-remarks",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Some remarks",
    "text": "Some remarks\nWe may compute \\(j(\\widehat{\\boldsymbol{\\theta}})\\) (the negative Hessian of log likelihood) numerically through finite differences.\nMany software implementations compute MLE via Nelder–Mead simplex algorithm:\n\ncheck the gradient and/or\nthe log likelihood differences\n\nto make sure the optimisation was successful."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#properties-of-mle",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#properties-of-mle",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Properties of MLE",
    "text": "Properties of MLE\n\nMaximum likelihood estimators are asymptotically Gaussian whenever \\(\\xi &gt; -1/2\\) with data in domain of attraction of extreme value distribution.\nConsistency requires that one increases block size, etc. as \\(n\\) increases at a particular rate depending on \\(F\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#regularity-conditions",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#regularity-conditions",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Regularity conditions",
    "text": "Regularity conditions\nSome cumulants (moments of derivatives of the log likelihood) of extreme value models do not exist.\n\nthe MLE does not solve the score equation if \\(\\widehat{\\xi} \\leq -1\\)\nMLE is not unique for \\(\\xi &lt; -1\\) (some combinations of \\(\\mu\\) and \\(\\sigma\\) yield infinite log likelihood).\n\nrestrict the parameter space to \\(\\{\\boldsymbol{\\theta}: y_1, \\ldots, y_n \\in \\mathrm{supp}(\\boldsymbol{\\theta}), \\xi \\geq -1\\}\\)\nFor GEV, MLE at boundary is \\((\\widehat{\\mu}=\\overline{y}, \\widehat{\\sigma} = \\max(y) - \\overline{y}, \\xi=-1)\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#regularity-conditions-1",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#regularity-conditions-1",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Regularity conditions",
    "text": "Regularity conditions\nIf \\(\\widehat{\\xi} &lt; -1/2\\), cannot evaluate the information matrix.\n\nRegularity assumptions do not apply! reported std. errors are misleading.\nTypically faster convergence, joint limit not asymptotically normal (Smith, 1985).\n\nIn applications, shape is typically close to zero, so authors sometimes restrict \\(\\xi \\in (-0.5, 0.5)\\).\nPenalization of the shape helps ensure that we get reasonable estimates in small samples."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#invariance-property-of-maximum-likelihood",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#invariance-property-of-maximum-likelihood",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Invariance property of maximum likelihood",
    "text": "Invariance property of maximum likelihood\nIf \\(h\\) is a mapping, then \\(h(\\widehat{\\boldsymbol{\\theta}})\\) is the MLE of \\(h(\\boldsymbol{\\theta})\\).\nThe expected value of the \\(N=50\\)-years maximum for \\(\\xi &lt; 1\\), is \\[\\begin{align*}\\mathfrak{e}_N = h(\\mu, \\sigma, \\xi) = \\mu_N + \\sigma_N\\{\\Gamma(1-\\xi)-1\\}/\\xi.\n\\end{align*}\\]\nThus, the MLE \\(\\widehat{\\mathfrak{e}}_N=h(\\widehat{\\mu}, \\widehat{\\sigma}, \\widehat{\\xi})\\).\n\n# MLE of expectation of maximum of 50 blocks\ngev.mle(xdat = ymax, args = \"Nmean\", N = 50)\n## Nmean \n##  53.4"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#profile-log-likelihood",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#profile-log-likelihood",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Profile log likelihood",
    "text": "Profile log likelihood\nConsider a functional of interest \\(\\psi\\) and other parameters \\(\\boldsymbol{\\lambda}\\), treated as nuisance.\nWe reparametrize the log likelihood in terms of \\((\\psi, \\boldsymbol{\\lambda})\\) and compute the profile log likelihood \\[\\begin{align*}\n\\ell_{\\mathrm{p}}(\\psi) = \\max_{\\boldsymbol{\\lambda}} \\ell(\\psi, \\boldsymbol{\\lambda})\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#plot-of-profile",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#plot-of-profile",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Plot of profile",
    "text": "Plot of profile\n\nprof &lt;- mev::gev.pll(param = \"Nmean\", dat = ymax, N = 50)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#confidence-intervals",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#confidence-intervals",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nUnder regularity conditions, the likelihood ratio statistic \\[\\begin{align*}\n2 \\{\\ell_{\\mathrm{p}}(\\widehat{\\psi}) - \\ell_{\\mathrm{p}}(\\psi_0)\\} \\stackrel{\\cdot}{\\sim} \\chi^2_1\n\\end{align*}\\] For the hypothesis \\(\\psi = \\psi_0\\), a \\((1-\\alpha)\\) confidence interval based on the profile likelihood ratio test is \\[\\begin{align*}\n\\{\\psi: 2\\{\\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta}_{\\psi})\\} \\leq  \\chi^2_1(1-\\alpha)\\}.\n\\end{align*}\\]\n\n(confint(prof))\n## Estimate Lower CI Upper CI \n##     53.4     47.9     73.6"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#generalized-pareto",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#generalized-pareto",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Generalized Pareto",
    "text": "Generalized Pareto\nIf extremal type theorem applies, then threshold exceedances \\(Y-u \\mid Y&gt;u\\) follow, as \\(u\\) tends to the upper endpoint of \\(F\\), a generalized Pareto distribution.\nThe generalized Pareto distribution is \\[\\begin{align*}\nH(y; \\tau, \\xi) &=\n\\begin{cases}\n1-\\left(1+\\xi {y}/{\\tau}\\right)_{+}^{-1/\\xi}, & \\xi \\neq 0,\\\\ 1-\n\\exp \\left(-{y}/{\\tau}\\right)_{+},& \\xi = 0,\n\\end{cases} \\label{eq:gpdist}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#preprocess-data",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#preprocess-data",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Preprocess data",
    "text": "Preprocess data\n\nChoose a threshold \\(u\\) (either an order statistic or a fixed quantity) and extract exceedances\nUse Grimshaw (1993) algorithm to reduce the 2D optimization problem to a line search.\n\n\nwindlyon &lt;- with(frwind, S2[month(date) &lt;= 4 | month(date) &gt;= 9])\nqulev &lt;- 1-100/nrow(windlyon)\nu &lt;- quantile(windlyon, 1-100/length(windlyon))"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#fitting-the-generalized-pareto-model",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#fitting-the-generalized-pareto-model",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Fitting the generalized Pareto model",
    "text": "Fitting the generalized Pareto model\n\nopt_gp &lt;- mev::fit.gpd(\n  xdat = windlyon, threshold = u, show = TRUE)\n## Method: Grimshaw \n## Log-likelihood: -208 \n## \n## Threshold: 33.8 \n## Number Above: 90 \n## Proportion Above: 0.008 \n## \n## Estimates\n##  scale   shape  \n## 3.5786  0.0309  \n## \n## Standard Errors\n## scale  shape  \n## 0.609  0.134  \n## \n## Optimization Information\n##   Convergence: successful"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#modelling-bulk",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#modelling-bulk",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Modelling bulk",
    "text": "Modelling bulk\nThe generalized Pareto only describes what happens above the threshold, but we can use the empirical distribution below: \\[\\begin{align*}\n\\widehat{\\Pr}(Y_i \\le x) = \\sum_{i=1}^n \\mathsf{I}(Y_i \\le x)/n, \\qquad x \\leq u.\n\\end{align*}\\]\nMany splicing models propose a (semi)parametric model for the bulk; see evmix package for examples"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#binomial---generalized-pareto-model",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#binomial---generalized-pareto-model",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Binomial - generalized Pareto model",
    "text": "Binomial - generalized Pareto model\n\nThe binomial-generalized Pareto model includes a likelihood contribution for \\(\\mathsf{I}(Y_i &gt;u) \\sim \\mathsf{Bin}(1, \\zeta_u)\\), where \\(\\zeta_u = \\Pr(Y_i &gt;u)\\).\nThis third parameter is orthogonal to the others, and there is a closed-form solution for the MLE."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#block-maximum-vs-threshold-exceedances",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#block-maximum-vs-threshold-exceedances",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Block maximum vs threshold exceedances",
    "text": "Block maximum vs threshold exceedances\n\nSuppose we fit a \\(\\mathsf{GP}(\\tau, \\xi)\\) distribution to exceedances above \\(u\\).\nIf there are on average \\(N_y\\) observations per year, the distribution of the \\(N\\)-year maximum conditional on exceeding \\(u\\) is approximately \\(H^{\\zeta_uNN_y}\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#threshold-stability",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#threshold-stability",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Threshold stability",
    "text": "Threshold stability\nMathematical basis for extrapolation.\nIf \\[\\begin{align*}\nY - u \\mid Y&gt;u \\sim \\mathsf{GP}(\\tau, \\xi),\n\\end{align*}\\] then for \\(\\{v &gt;u\\in \\mathbb{R}_{+}: \\tau+\\xi (u-v)&gt;0\\}\\),\n\\[\\begin{align*}\nY-v \\mid Y&gt;v \\sim \\mathsf{GP}\\{\\tau + \\xi (u-v), \\xi\\},\n\\end{align*}\\] and \\(\\zeta_v = \\{1+\\xi(v-u)/\\tau\\}^{-1/\\xi}\\zeta_u\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#threshold-stability-plots",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#threshold-stability-plots",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Threshold stability plots",
    "text": "Threshold stability plots\nAssuming data are exactly generalized Pareto, expect shape parameters to be constant (up to sampling variability).\n\nuseq &lt;- quantile(windlyon, seq(0.9, 0.99, by = 0.01))\ntstab.gpd(windlyon, \n          method = \"profile\",\n          thresh = useq)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#threshold-stability-for-shape",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#threshold-stability-for-shape",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Threshold stability for shape",
    "text": "Threshold stability for shape\n\nFigure 1: Threshold stability plot for Lyon mean wind speed"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#inhomogeneous-point-process",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#inhomogeneous-point-process",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Inhomogeneous point process",
    "text": "Inhomogeneous point process\nLet \\(Y_i\\) i.i.d. from \\(F\\) with lower endpoint \\(x^*\\).\nConsider \\(a_n&gt;0\\) and \\(b_n \\in \\mathbb{R}\\) such that the distribution of the bidimensional point process \\[\\begin{align*}\nP_n =\\left\\{ \\frac{i}{n}, \\frac{Y_i-b_n}{a_n}, i = 1, \\ldots, n\\right\\}\n\\end{align*}\\] converges to an inhomogeneous Poisson point process on sets of the form \\((a, b) \\times (z, \\infty)\\) for \\(0 \\leq a \\leq b \\leq 1\\) and \\(z&gt;z_*=\\lim_{n \\to \\infty} \\{(x_*-b_n)/a_n\\}\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#intensity-of-inhomogeneous-poisson-process",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#intensity-of-inhomogeneous-poisson-process",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Intensity of inhomogeneous Poisson process",
    "text": "Intensity of inhomogeneous Poisson process\nThe intensity measure of the limiting point process, which gives the expected number of points falling in a set is \\[\\begin{align*}\n&\\Lambda\\{(a, b) \\times (z, \\infty)\\}\n\\\\&\\quad  = (b-a)\\left(1+ \\xi \\frac{z-\\mu}{\\sigma}\\right)_{+}^{-1/\\xi} \\label{eq:pp_conv}\n\\end{align*}\\] for \\(\\xi \\neq 0\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#likelihood-of-the-point-process",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#likelihood-of-the-point-process",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Likelihood of the point process",
    "text": "Likelihood of the point process\n\\[\\begin{align*}\n\\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{y}) &=  (c\\sigma)^{n_u} \\prod_{i=1}^{n_u} \\left(1+\\xi\\frac{y_i-\\mu}{\\sigma}\\right)^{-1-1/\\xi}_{+} \\\\& \\times \\exp\\left\\{- c \\left(1+ \\xi \\frac{u-\\mu}{\\sigma}\\right)^{-1/\\xi}_{+}\\right\\},\n\\end{align*}\\] The constant \\(c\\) is introduced as a way to relate the parameters of the point process likelihood to those of the GEV fitted to blocks of size \\(m\\) observations, e.g., \\(c=n/m\\).\nMoins et al. (2023) propose a orthogonal reparametrization."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#link-between-parametrizations",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#link-between-parametrizations",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Link between parametrizations",
    "text": "Link between parametrizations\nUnder the Poisson approximation to the binomial, the expected number of observations above the threshold is \\[\\begin{align*}\nc \\left\\{1+ \\xi \\left( \\frac{u-\\mu}{\\sigma}\\right)\\right\\}^{-1/\\xi} \\approx n_u. \\end{align*}\\] We can thus relate \\(\\mathsf{GP}(\\tau, \\xi)\\) with Poisson, where MLE is \\[\\begin{align*}\n\\mu_0 & \\approx u - \\sigma_0\\{(n_u/c)^{-\\widehat{\\xi}}-1\\}/\\widehat{\\xi}, \\\\\\sigma_0 &\\approx \\widehat{\\sigma}_u (n_u/c)^{\\widehat{\\xi}}, \\qquad \\xi_0 = \\widehat{\\xi}.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#return-levels",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#return-levels",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Return levels",
    "text": "Return levels\n\nThe probability \\(p_l\\) that a \\(N\\)-year return level is exceeded \\(l\\) times in \\(N\\) years of independent annual maxima is \\(\\mathsf{Bin}(N, 1/N)\\).\nFor large \\(N\\), a Poisson approximation yields \\(p_0=p_1=0.368\\), \\(p_2=0.184\\), \\(p_3=0.061\\), etc.\n\nThe probability of at least one exceedance over \\(N\\) years is in fact roughly \\(0.63\\).\nThe return level corresponds to the 0.368 quantile of the \\(N\\)-year maximum distribution."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#references",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#references",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nGrimshaw, S. D. (1993). Computing maximum likelihood estimates for the generalized Pareto distribution. Technometrics, 35(2), 185–191. https://doi.org/10.1080/00401706.1993.10485040\n\n\nMoins, T., Arbel, J., Girard, S., & Dutfoy, A. (2023). Reparameterization of extreme value framework for improved Bayesian workflow. Computational Statistics & Data Analysis, to appear. https://doi.org/https://doi.org/10.1016/j.csda.2023.107807\n\n\nSmith, R. L. (1985). Maximum likelihood estimation in a class of nonregular cases. Biometrika, 72(1), 67–90. https://doi.org/10.1093/biomet/72.1.67"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#nonstationarity",
    "href": "slides/EVA2023-Rsoftware-regression.html#nonstationarity",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Nonstationarity",
    "text": "Nonstationarity\nThere are various forms of nonstationarity, including\n\ntrends,\ntime-varying variance,\nseasonality,\n\ncovariate effects.\n\nNo general theory"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#solution-1-preprocessing",
    "href": "slides/EVA2023-Rsoftware-regression.html#solution-1-preprocessing",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Solution 1: preprocessing",
    "text": "Solution 1: preprocessing\n\ndetrend data to approximate stationary.\n\nfor financial time series, fit ARMA-GARCH model\nuse Box-Cox transformation with regression model\n\nanalyze residuals \\(R_i = Y_i - \\widehat{f}(\\mathbf{X}_i)\\), where the postulated model is such that \\(\\mathsf{E}(Y_i) = f(\\mathbf{X}_i)\\).\n\nBenefit: more data are available to estimate trends."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#estimating-probability-of-rare-events",
    "href": "slides/EVA2023-Rsoftware-regression.html#estimating-probability-of-rare-events",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Estimating probability of rare events",
    "text": "Estimating probability of rare events\n\\[\\begin{align*}\n&\\Pr(Y_i &gt; y \\mid \\mathbf{X}_i) \\\\\\quad &= \\Pr\\{R_i &gt; y - \\widehat{f}(\\mathbf{X}_i)\\mid \\mathbf{X}_i\\}\n\\\\\\quad & = \\Pr\\{R_i &gt; y - \\widehat{f}(\\mathbf{X}_i)\\mid \\mathbf{X}_i, R_i &gt; u\\}\\Pr(R_i &gt; u) \\\\& \\qquad + \\Pr\\{R_i &gt; y - \\widehat{f}(\\mathbf{X}_i)\\mid \\mathbf{X}_i, R_i\\leq u\\}\\Pr(R_i \\leq u)\n\\end{align*}\\] where the first term of the last line can be estimated using a generalized Pareto and the latter empirically."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#solution-2-hierarchical-models",
    "href": "slides/EVA2023-Rsoftware-regression.html#solution-2-hierarchical-models",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Solution 2: hierarchical models",
    "text": "Solution 2: hierarchical models\nWe can also incorporate parameters in the parameters of the extreme value distribution.\n\nModel \\(g_{\\mu}(\\mu; \\mathbf{X})=f_{\\mu}(\\mathbf{X}; \\boldsymbol{\\beta}_{\\mu})\\), \\(g_{\\sigma}(\\sigma; \\mathbf{X})=f_{\\sigma}(\\mathbf{X}; \\boldsymbol{\\beta}_{\\sigma})\\)\nLink functions \\(g\\), functions of covariates \\(f\\)\nTypically keep shape constant.\n\nFor explanatories \\(\\mathbf{X}\\), fit regression models of the form \\[\\begin{align*}\nf(\\mathbf{X}) = \\beta_0 + \\beta_1 \\mathrm{X}_1 + \\cdots \\beta_p \\mathrm{X}_p,\n\\end{align*}\\] and estimate parameters by maximum likelihood."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#generalized-additive-models",
    "href": "slides/EVA2023-Rsoftware-regression.html#generalized-additive-models",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Generalized additive models",
    "text": "Generalized additive models\nRather than linear effects or changepoints (if \\(\\mathrm{X}_j\\) is binary), we could consider smooths\n\\[\\begin{align*}\nf(\\mathrm{X}_j) = \\sum_{k=1}^K \\beta_k b_k(\\mathrm{X}_j)\n\\end{align*}\\] where \\(b_k\\) are (compactly-supported) basis functions.\nScale input first! More numerically stable."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#parsimony-is-key",
    "href": "slides/EVA2023-Rsoftware-regression.html#parsimony-is-key",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Parsimony is key",
    "text": "Parsimony is key\n\nThere are few exceedances or extremes…\nThe support restriction translates into up to \\(n\\) inequality constraints: difficult to optimize.\n\nTake home message: keep models simple!"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#penalized-regression",
    "href": "slides/EVA2023-Rsoftware-regression.html#penalized-regression",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Penalized regression",
    "text": "Penalized regression\nPenalization to enforce sparsity or regularize estimation.\n\nFor generalized additive models (GAM), penalize square of second derivative to control the wiggliness of the function.\nSmooth matrix \\(\\mathbf{S}\\) with entries \\(S_{ij} = \\int b_i''(x) b_j''(x)\\mathrm{d} x\\), so the penalty is \\(\\lambda \\boldsymbol{\\beta}^\\top \\mathbf{S}\\boldsymbol{\\beta}\\) for \\(\\lambda &gt; 0\\).\nThese correspond to improper (i.e., rank-deficient) Gaussian priors."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#tuning-parameter",
    "href": "slides/EVA2023-Rsoftware-regression.html#tuning-parameter",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Tuning parameter",
    "text": "Tuning parameter\nMaximization over \\[\\begin{align*}\n\\max_{\\lambda  \\in \\mathbb{R}_K^{+}} \\max_{\\boldsymbol{\\beta}} \\left\\{\\ell_{\\boldsymbol{\\lambda}}(\\boldsymbol{\\beta}) - \\frac{1}{2} \\boldsymbol{\\beta}^\\top\\mathbf{S}_{\\boldsymbol{\\lambda}}\\boldsymbol{\\beta}\\right\\}\n\\end{align*}\\] where \\(\\mathbf{S}_{\\boldsymbol{\\lambda}} = \\sum_{k=1}^K \\lambda_k \\mathbf{S}_k\\).\n\nOptimal value of \\(\\boldsymbol{\\lambda}\\) obtained by maximizing the marginal likelihood, integrating out \\(\\boldsymbol{\\beta}\\) using Laplace’s method (Wood et al., 2016)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#a-word-of-caution",
    "href": "slides/EVA2023-Rsoftware-regression.html#a-word-of-caution",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "A word of caution",
    "text": "A word of caution\n\nWith time-varying covariates, we need forecasts to compute risk measures in future.\nModel often fitted only at specific covariate values (careful at extrapolation beyond range).\nRather than linear trend in time, use covariates that incorporate said trends but are more natural (e.g., climate model output)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#return-levels-under-nonstationarity",
    "href": "slides/EVA2023-Rsoftware-regression.html#return-levels-under-nonstationarity",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Return levels under nonstationarity",
    "text": "Return levels under nonstationarity\nIn nonstationary models, risk measures of interest are defined conditionally on the value of covariates\nFor example, the \\(1-p\\) conditional return level is (Eastoe & Tawn, 2009) \\[\\begin{align*}\n\\Pr(Y_t  &gt; y \\mid \\mathbf{X}_t =\\boldsymbol{x}_t) = p\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#unconditional-return-levels",
    "href": "slides/EVA2023-Rsoftware-regression.html#unconditional-return-levels",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Unconditional return levels",
    "text": "Unconditional return levels\nThese can be obtained by averaging out over the distribution of covariates that are employed in the model.\n\\[\\begin{align*}\n\\Pr(Y_t  &gt; y)=\\int_{\\mathcal{X}} \\Pr(Y_t  &gt; y \\mid \\mathbf{X}_t =\\boldsymbol{x}_t) \\mathrm{d} P(\\boldsymbol{x}_t)=p.\n\\end{align*}\\]\nMay require information about future distribution of covariates if these are time-varying.\nAs such, return levels may be meaningless quantities."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#threshold-stability",
    "href": "slides/EVA2023-Rsoftware-regression.html#threshold-stability",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Threshold stability",
    "text": "Threshold stability\nThe generalized Pareto model with varying scale and shape is not threshold-stable unless, for any \\(v&gt;u\\), \\[\\begin{align*}\n\\sigma_v(\\boldsymbol{x}_t) = \\sigma_u(\\boldsymbol{x}_t) + (v-u) \\xi(\\boldsymbol{x}_t)\n\\end{align*}\\]\nRestrictive! For constant \\(\\xi\\), need \\(\\sigma\\) linear or constant (log scale) (Eastoe & Tawn, 2009).\nUsing the inhomogeneous Poisson point process representation avoids these problems.\nFor nonstationary threshold models \\(u(\\mathbf{X})\\), see Section 3.2.2 of Northrop & Jonathan (2011)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#software",
    "href": "slides/EVA2023-Rsoftware-regression.html#software",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Software",
    "text": "Software\nFunction evgam from the eponymous package (Youngman, 2022) builds on the mgcv package (Wood, 2017).\nThe setup is evgam(formula, data, family, ...), where\n\nfamily is the character string for the extreme value distribution (gev, gpd, rlarg and ald for asymmetric Laplace, used in quantile regression),\nformula is a list of formula for parameters (in the order location, scale, shape)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#formulas-and-smooth",
    "href": "slides/EVA2023-Rsoftware-regression.html#formulas-and-smooth",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Formulas and smooth",
    "text": "Formulas and smooth\nUse s for smooths and te for tensor products of smooths (interactions)\n\nk controls the number of breakpoints\nbs controls the basis function, e.g., thin-plate splines (tp), cubic regression splines (cr), cyclic cubic spline (cc).\nfx: control whether fixed degrees of freedom for regression spline or else penalized regression spline (default, FALSE)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#data-setup",
    "href": "slides/EVA2023-Rsoftware-regression.html#data-setup",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Data setup",
    "text": "Data setup\n\nlibrary(evgam)\nlibrary(lubridate)\ndata(frwind, package = \"mev\")\nlyon &lt;- with(frwind,\n             xts::xts(x = S2, order.by = date))\nymax &lt;- xts::apply.yearly(lyon, max)\nymax &lt;- ymax[-length(ymax)] # Remove 2023"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#example-with-evgam",
    "href": "slides/EVA2023-Rsoftware-regression.html#example-with-evgam",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Example with evgam",
    "text": "Example with evgam\n\nopt_gev &lt;- evgam::evgam(\n  data = data.frame(\n    year = year(ymax),\n    ymax = ymax),\n  formula = list(ymax ~ s(year, bs = \"cr\"),\n                 ~ 1, \n                 ~ 1),\n  family = \"gev\")"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#fitted-splines",
    "href": "slides/EVA2023-Rsoftware-regression.html#fitted-splines",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Fitted splines",
    "text": "Fitted splines\n\nplot(opt_gev)\n\n\nFigure 1: Fitted spline for location parameter of generalized extreme value distribution as a function of day of year (scaled to unit interval).Plot and generalized analysis of deviance suggests constant location."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#estimated-quantiles",
    "href": "slides/EVA2023-Rsoftware-regression.html#estimated-quantiles",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Estimated quantiles",
    "text": "Estimated quantiles\nFor given \\(p\\), solve nonlinear equation \\[\\prod_{i=1}^n F^N(z; \\boldsymbol{\\theta}_i, \\mathbf{X}_i) = p\\] using qev to give unconditional quantile."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#quantile-regression",
    "href": "slides/EVA2023-Rsoftware-regression.html#quantile-regression",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Quantile regression",
    "text": "Quantile regression\nWe can work the asymmetric Laplace distribution to build a working likelihood for quantile regression at probability level \\(\\tau\\).\nThe latter has density function \\[\\begin{align*}\nf(y; \\mu, \\sigma) = \\frac{\\tau(1-\\tau)}{\\sigma} \\exp\\left\\{-\\rho_\\tau \\left(\\frac{y-\\mu}{\\sigma}\\right)\\right\\},\n\\end{align*}\\] where \\(\\rho_\\tau(y) = y(\\tau-\\mathrm{I}_{y &lt;0})\\) is the check function."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#time-varying-threshold-with-quantile-regression",
    "href": "slides/EVA2023-Rsoftware-regression.html#time-varying-threshold-with-quantile-regression",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Time-varying threshold with quantile regression",
    "text": "Time-varying threshold with quantile regression\n\nqreg &lt;- evgam::evgam(\n  # Cubic cyclic splines\n  formula = list(wind ~ s(tday, bs = \"cc\"), \n                 ~ s(tday, bs = \"cc\")), \n  data = data.frame(wind = as.numeric(lyon),\n                    tday = yday(lyon)),\n  family = \"ald\", \n  ald.args = list(tau = 0.95))"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#temporal-trend",
    "href": "slides/EVA2023-Rsoftware-regression.html#temporal-trend",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Temporal trend",
    "text": "Temporal trend\n\nplot(qreg)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-regression.html#references",
    "href": "slides/EVA2023-Rsoftware-regression.html#references",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nEastoe, E. F., & Tawn, J. A. (2009). Modelling non-stationary extremes with application to surface level ozone. Journal of the Royal Statistical Society: Series C (Applied Statistics), 58(1), 25–45. https://doi.org/10.1111/j.1467-9876.2008.00638.x\n\n\nNorthrop, P. J., & Jonathan, P. (2011). Threshold modelling of spatially dependent non-stationary extremes with application to hurricane-induced wave heights. Environmetrics, 22(7), 799–809. https://doi.org/10.1002/env.1106\n\n\nWood, S. N. (2017). Generalized additive models: An introduction with R (2nd ed.). Chapman; Hall/CRC.\n\n\nWood, S. N., Pya, N., & Säfken, B. (2016). Smoothing parameter and model selection for general smooth models. Journal of the American Statistical Association, 111(516), 1548–1563. https://doi.org/10.1080/01621459.2016.1180986\n\n\nYoungman, B. D. (2022). evgam: An R package for generalized additive extreme value models. Journal of Statistical Software, 103(1), 1–26. https://doi.org/10.18637/jss.v103.i03"
  }
]