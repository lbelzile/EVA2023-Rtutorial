[
  {
    "objectID": "content/bayesian.html",
    "href": "content/bayesian.html",
    "title": "Bayesian modelling",
    "section": "",
    "text": "In the frequentist paradigm, we consider inference for a fixed value of the parameter that generated the data, treated as random. In Bayesian inference, we consider inference conditional on the observed data, and treat the parameter as random. This can be understood as reflecting our uncertainty about the value that generated the data from the model. To achieve this, the likelihood of the random sample \\(\\boldsymbol{Y}\\) is combined with prior distributions for the model parameters \\(\\boldsymbol{\\theta} = (\\theta_1,\\ldots,\\theta_m)^\\top \\in \\boldsymbol{\\Theta}\\), with prior density \\(p(\\boldsymbol{\\theta})\\); we use the generic notation \\(p(\\ldots)\\) for various conditional and unconditional densities and mass functions.\nThe posterior distribution, \\[\np(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y}) = \\frac{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{ \\int p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\mathrm{d}\\boldsymbol{\\theta}},\n\\tag{1}\\] is proportional, as a function of \\(\\boldsymbol{\\theta}\\), to the product of the likelihood and the priors in the numerator, but the integral appearing in the denominator of Equation 1 is untractable in general. In such cases, the posterior density \\(p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y})\\) usually does not correspond to any well-known distribution family, and posterior inferences about the components of \\(\\boldsymbol{\\theta}\\) further involve marginalizing out the other components.\nFor instance, to obtain the posterior density \\(p(\\theta_1\\mid \\boldsymbol{Y})\\) of the first parameter in \\(\\boldsymbol{\\theta}\\), we have to evaluate the \\((m-1)\\)-dimensional integral \\(\\int p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y})\\,\\mathrm{d}(\\theta_2,\\ldots,\\theta_m)\\). If we have posterior draws from \\(\\boldsymbol{\\theta}\\), this amounts to picking out only entries that correspond to the particular parameter of interest, e.g., \\(\\theta_1\\).\nMost of the field of Bayesian statistics revolves around the creation of algorithms that circumvent the calculation of the normalizing constant (or else provide accurate numerical approximation of the latter) or that allow for marginalizing out all parameters except for one."
  },
  {
    "objectID": "content/bayesian.html#prior-specification-for-extremes",
    "href": "content/bayesian.html#prior-specification-for-extremes",
    "title": "Bayesian modelling",
    "section": "Prior specification for extremes",
    "text": "Prior specification for extremes\nWe first consider priors for the model parameters of extreme value distributions. These should reflect the range of plausible values and can sometimes interpreted be interpreted as penalties: for example, normal parameters in mixed models shrink values towards the overall mean or slope vectors. The more concentrated the prior mode is, the more influence the prior has. Bernstein-von Mises theorem however guarantees that, as the sample size grows, the influence of the prior is washed away unless the prior imposes restrictions on the support \\(\\Theta\\). For example, if we take a Beta prior \\(\\xi \\sim \\mathsf{Be}(a,b)\\) prior on \\([-0.5, 0.5]\\), then the posterior for \\(\\xi\\) will be restricted to this range and shrunk towards the prior mean.\nSuppose we fit a generalized extreme value distribution as before. The revdbayes package specifies a range of prior functions, see ?revdbayes::set_prior. It is possible to set priors for, e.g., the quantile spacing, and then map them back to the GEV parameters \\(\\mu, \\sigma, \\xi\\).\n\nlibrary(revdbayes)\nlibrary(ggplot2)\ndata(\"frwind\", package = \"mev\")\nlyon &lt;- with(frwind,\n             xts::xts(x = S2, order.by = date))\n# Create series of yearly maximum\nymax &lt;- as.numeric(xts::apply.yearly(lyon, max))\n# Fit a model with a trivariate normal prior for mu, log(sigma), xi\nprior1 &lt;- set_prior(prior = \"mdi\", model = \"gev\")\nprior2 &lt;- set_prior(prior = \"beta\", model = \"gev\")\nprior3 &lt;- set_prior(prior = \"norm\", \n                model = \"gev\", \n                mean = c(mean(ymax), log(sd(ymax)), 0), \n                cov = diag(c(1000, 1000, 1)))\n\nHaving specified our prior distributions, we can use software to obtain draws from the posterior. Here, we use revdbayes (Northrop, 2023) to get exact samples using the ratio-of-uniform algorithm (Wakefield et al., 1991). To see what impact priors have, we plot the marginal posterior, obtaining simply by dropping the columns for the other model parameters.\n\npost_1 &lt;- revdbayes::rpost_rcpp(\n  n = 1e4L, \n  model = \"gev\",\n  data = ymax,\n  prior = prior1,\n  nrep = 100)\npost_samp1 &lt;- post_1$sim_vals\npost_samp2 &lt;- revdbayes::rpost_rcpp(\n  n = 1e4L, \n  model = \"gev\",\n  data = ymax,\n  prior = prior2)$sim_vals\n# Compute marginal posterior for shape\nggplot(data = data.frame(\n  shape = c(post_samp1[,'xi'],\n            post_samp2[,'xi']),\n  prior = rep(c(\"mdi\", \"beta\"), \n              each = nrow(post_samp1))),\n  mapping = aes(x = shape,\n                col = prior, \n                group = prior)) +\n  geom_density() +\n  theme_minimal()\n\n\n\n\nFigure 1: Marginal posterior of GEV shape parameter for different prior distributions.\n\n\n\n\nWe are not restricted to the default parametrization: appealing to invariance of the log likelihood, and thanks to max-stability we can directly compute the marginal posterior of the expectation of the 50 year maximum.\n\ngev_Nmean &lt;- function(par, N){\n  # Map parameters via GEV max-stability\n  mu &lt;- par[1] + par[2]*(N^par[3]-1)/par[3]\n  sigma &lt;- par[2]*N^par[3]; \n  xi &lt;- par[3]\n  # then use formula for GEV expectation\n  ifelse(xi &gt; 1, \n         Inf, \n  mu - sigma/xi * (1 - N^xi * gamma(1 - xi)))\n}\n# For each combination of posterior draw\n# compute functional of interest\n\n# This years the posterior distribution of 50 year mean\npost_gev_mean &lt;- apply(post_samp1, 1, gev_Nmean, N = 50)\n# Posterior quartiles\nquantile(post_gev_mean, c(0.25, 0.5, 0.75))\n\n     25%      50%      75% \n59.31238 67.05540 81.09692 \n\n# To get a 95% credible interval, simply compute quantiles\nquantile(post_gev_mean, c(0.025, 0.975))\n\n     2.5%     97.5% \n 51.93082 153.86240 \n\n\nWe can see that the credible intervals are quite asymmetric.\nMore generally, we may be interested in prediction, which in the Bayesian paradigm arises from the posterior predictive distribution. For each posterior draw \\(\\boldsymbol{\\theta}_b\\), we simulate new observations from the generative model, here GEV.\n\npost_pred_samp &lt;- revdbayes::rgev(\n  n = nrow(post_samp1),\n  loc = post_samp1[,'mu'], \n  scale = post_samp1[,'sigma'],\n  shape = post_samp1[,'xi'],\n  m = 50L) # 50 year parameters\nsummary(post_pred_samp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  42.06   49.54   52.63   55.22   57.56  259.41 \n\n\nAs part of the Bayesian workflow (Gabry et al., 2019), we can also check if our model is in line with expectations by computing a summary statistic on simulate datasets from the posterior predictive, and comparing it with that of the original data. If the value for the original sample lies far into the tails of the distribution of simulated samples, this provides evidence of model misspecification.\n\npp_check(post_1, stat = median)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "content/bayesian.html#loss-function",
    "href": "content/bayesian.html#loss-function",
    "title": "Bayesian modelling",
    "section": "Loss function",
    "text": "Loss function\nIn the EVA 2023 data challenge, a custom loss function for the return levels \\(q\\) was provided, of the form \\[\\begin{align*}\nL(q, \\widehat{q}(\\theta)) =\n\\begin{cases}\n0.9(0.99q - \\widehat{q}), & 0.99q &gt; \\widehat{q} \\\\\n0, & |q - \\widehat{q}| \\leq 0.01 q\\\\\n0.1(\\widehat{q} - 1.01q), & 1.01q &lt; \\widehat{q}.\n\\end{cases}\n\\end{align*}\\] In the Bayesian paradigm, we compute the average loss over the posterior distribution of the parameters, for given value of the return level \\(q_0\\): \\[\\begin{align*}\nr(q_0) = \\int_{\\boldsymbol{\\Theta}}L(q(\\boldsymbol{\\theta}), q_0) p (\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}\n\\end{align*}\\] and then we seek to minimize the risk \\(\\mathrm{min}_{q_0 \\in \\mathbb{R}_{+}} r(q_0)\\)\n\ngev_retlev &lt;- function(par, N, p = 0.368){\n  # Map parameters via GEV max-stability\n  mu &lt;- par[1] + par[2]*(N^par[3]-1)/par[3]\n  sigma &lt;- par[2]*N^par[3]; \n  xi &lt;- par[3]\n  # quantile of N-block maximum\n  mev::qgev(p = p, loc = mu, scale = sigma, shape = xi)\n}\n\n# Loss function\nloss &lt;- function(qhat, q){\n    mean(ifelse(0.99*q &gt; qhat,\n           0.99*(0.99*q-qhat),\n           ifelse(1.01*q &lt; qhat,\n                  0.1*(qhat-1.01*q),\n                  0)))\n}\n# Compute the posterior of the return levels\nretlev_post &lt;- apply(post_samp1, 1, gev_retlev, N = 50)\n# Create a grid of values over which to estimate the risk\nretlev_psi &lt;- seq(\n  from = quantile(retlev_post, 0.2),\n  to = quantile(retlev_post, 0.99), \n  length.out = 101)\n# Create a container to store results\nrisk &lt;- numeric(length = length(retlev_psi))\nfor(i in seq_along(risk)){\n  # Compute integral (Monte Carlo average over draws)\n risk[i] &lt;- loss(q = retlev_post, qhat = retlev_psi[i])\n}\n# Plot loss function\nggplot(data = data.frame(\n  loss = risk, \n  retlev = retlev_psi), \n  mapping = aes(x = retlev, y = loss)) +\n  geom_line() +\n  geom_vline(xintercept = mean(retlev_post)) +\n  labs(x = \"return level\") +\n  theme_minimal()\n\n\n\n\nThe minimum of the loss function is returned for return levels values that are much higher than the posterior mean."
  },
  {
    "objectID": "content/likelihood.html",
    "href": "content/likelihood.html",
    "title": "Likelihood-based inference",
    "section": "",
    "text": "The mev package provides gradient-based optimization routines for fitting univariate extreme value models, either block maxima or threshold exceedances, using one of four likelihoods: that of the generalized extreme value distribution, the generalized Pareto distribution, and the inhomogeneous Poisson point process and the \\(r\\)-largest order statistics.\nRelative to other packages such as evd or ismev, the package functions include analytic expressions for the score and observed informations, with careful interpolation when \\(\\xi \\approx 0\\). However, mev does not handle generalized linear or generalized additive models for the parameters, to avoid having as many inequality constraints in the optimization as there are observations times the number of covariates."
  },
  {
    "objectID": "content/likelihood.html#basic-theory",
    "href": "content/likelihood.html#basic-theory",
    "title": "Likelihood-based inference",
    "section": "Basic theory",
    "text": "Basic theory\nLet \\(\\ell(\\boldsymbol{y}; \\boldsymbol{\\theta})\\) denotes the log-likelihood of an \\(n\\) sample with a \\(p\\)-dimensional parameter \\(\\boldsymbol{\\theta}\\). The score vector is \\(\\ell_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta})=\\partial \\ell / \\partial \\boldsymbol{\\theta}\\), while the Fisher information is \\(i(\\boldsymbol{\\theta})=\\mathrm{E}\\{\\ell_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta})\\ell_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta})^\\top\\}\\). Under regularity conditions, we also have \\(i(\\boldsymbol{\\theta}) = - \\mathrm{E}(\\partial^2 \\ell / \\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^\\top)\\). The observed information is the negative Hessian \\(\\jmath(\\boldsymbol{\\theta})-\\partial^2 \\ell / \\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^\\top\\), evaluated at the maximum likelihood estimator \\(\\hat{\\boldsymbol{\\theta}}\\).\nBy definition, the maximum likelihood estimator solves the score equation, i.e. \\(\\ell_{\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})=\\boldsymbol{0}_p\\). If the maximum likelihood estimator is not available in closed-form, its solution is found numerically and this property can be used to verify that the optimization routine has converged or for gradient-based maximization algorithms."
  },
  {
    "objectID": "content/likelihood.html#statistical-inference",
    "href": "content/likelihood.html#statistical-inference",
    "title": "Likelihood-based inference",
    "section": "Statistical inference",
    "text": "Statistical inference\nThis section presents some test statistics that can easily be computed using some of the functionalities of mev, as well as confidence intervals for parameters and common functionals, based on the profile likelihood.\nThe three main type of test statistics for likelihood-based inference are the Wald, score and likelihood ratio tests. The three main classes of statistics for testing a simple null hypothesis \\(\\mathscr{H}_0: \\boldsymbol{\\theta}=\\boldsymbol{\\theta}_0\\) against the alternative \\(\\mathscr{H}_a: \\boldsymbol{\\theta} \\neq \\boldsymbol{\\theta}_0\\) are the likelihood ratio, the score and the Wald statistics, defined respectively as \\[\\begin{align*}\nw &= 2 \\left\\{ \\ell(\\hat{\\boldsymbol{\\theta}})-\\ell(\\boldsymbol{\\theta}_0)\\right\\},\\qquad\n\\\\w_{\\mathsf{score}} &= U^\\top(\\boldsymbol{\\theta}_0)i^{-1}(\\boldsymbol{\\theta}_0)\\ell_{\\boldsymbol{   heta}}(\\boldsymbol{\\theta}_0),\\qquad\n\\\\ w_{\\mathsf{wald}} &= (\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}_0)^\\top i(\\boldsymbol{\\theta}_0)(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}_0),\n\\end{align*}\\] where \\(\\hat{\\boldsymbol{\\theta}}\\) is the maximum likelihood estimate under the alternative and \\(\\boldsymbol{\\theta}_0\\) is the null value of the parameter vector. The statistics \\(w, w_{\\mathsf{score}}, w_{\\mathsf{wald}}\\) are all first order equivalent and asymptotically follow a \\(\\chi^2_p\\) distribution, where \\(q\\) is the difference between \\(p\\) and the number of parameters under the null hypothesis. Under the conditions of the Neyman–Pearson theorem, the likelihood ratio test is most powerful test of the lot. The score statistic \\(w_{\\mathsf{score}}\\) only requires calculation of the score and information under \\(\\mathscr{H}_0\\), which can be useful in problems where calculations under the alternative are difficult to obtain. The Wald statistic \\(w_{\\mathsf{wald}}\\) is not parametrization-invariant and typically has poor coverage properties.\nOftentimes, we are interested in a functional of the parameter vector \\(\\boldsymbol{\\theta}\\). The profile likelihood \\(\\ell_\\mathsf{p}\\), a function of \\(\\boldsymbol{\\psi}\\) alone, is obtained by maximizing the likelihood pointwise at each fixed value \\(\\boldsymbol{\\psi}=\\boldsymbol{\\psi}_0\\) over the nuisance vector \\(\\boldsymbol{\\lambda}_{\\psi_0}\\), \\[\\begin{align*}\n   \\ell_\\mathsf{p}(\\boldsymbol{\\psi})=\\max_{\\boldsymbol{\\lambda}}\\ell(\\boldsymbol{\\psi}, \\boldsymbol{\\lambda})=\\ell(\\boldsymbol{\\psi}, \\hat{\\boldsymbol{\\lambda}}_{\\boldsymbol{\\psi}}).\n\\end{align*}\\] We denote the restricted maximum likelihood estimator \\(\\hat{\\boldsymbol{\\theta}}_\\psi= (\\psi, \\hat{\\lambda}_{\\psi})\\).\nWe can define score and information in the usual fashion: for example, the observed profile information function is [j_() =- = {j^{}(, _{})}^{-1}. ]\nWe can turn tests and their asymptotic distribution into confidence intervals. For the hypothesis \\(\\psi = \\psi_0\\), a \\((1-\\alpha)\\) confidence interval based on the profile likelihood ratio test is \\(\\{ \\psi: 2\\{\\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta}_{\\psi})\\} \\leq \\chi^2_1(0.95)\\}\\).\n\nLikelihoods\nThere are four basic likelihoods for univariate extremes: the likelihood of the generalized extreme value (GEV) distribution for block maxima, the likelihood for the generalized Pareto distribution and that of the non-homogeneous Poisson process (NHPP) for exceedances above a threshold \\(u\\) and lastly the likelihood of the \\(r\\)-largest observations.\n\n\nGeneralized extreme value distribution\nThe generalized extreme value (GEV) distribution with location parameter \\(\\mu \\in \\mathbb{R}\\), scale parameter \\(\\sigma \\in \\mathbb{R}_{+}\\) and shape parameter \\(\\xi \\in \\mathbb{R}\\) is \\[\\begin{align*}\n  G(x)  =\n\\begin{cases}\n\\exp\\left\\{-\\left(1+\\xi \\frac{x-\\mu}{\\sigma}\\right)^{-1/\\xi}\\right\\}, &  \\xi \\neq 0,\\\\\n\\exp \\left\\{ -\\exp \\left(-\\frac{x-\\mu}{\\sigma}\\right)\\right\\},&  \\xi = 0,\n\\end{cases}\n\\end{align*}\\] defined on \\(\\{x \\in \\mathbb{R}: \\xi(x-\\mu)/\\sigma &gt; -1\\}\\) where \\(x_{+} = \\max\\{0, x\\}\\). The case \\(\\xi=0\\) is commonly known as the Gumbel distribution. We denote the distribution by \\(\\mathsf{GEV}(\\mu, \\sigma, \\xi)\\).\nThe max-stability property allows one to extrapolate the distribution beyond observed levels: one can show that the distribution of the maximum of a larger block (or \\(N\\) block maximum) would be also generalized extreme value if \\(Y_i \\sim \\mathsf{GEV}(\\mu, \\sigma, \\xi)\\) are independent, where \\(\\max_{i=1}^N Y_i \\sim \\mathsf{GEV}(\\mu_N, \\sigma_N, \\xi)\\) and \\(\\mu_N = \\mu + \\sigma(N^\\xi-1)/\\xi\\) and \\(\\sigma_N = \\sigma N^\\xi, \\xi_N = \\xi)\\) — the case \\(\\xi=0\\) is defined by continuity. In practice, we can partition data into \\(m\\) blocks of roughly equal size \\(n/m\\) and fit a GEV distribution to the maximum of the blocks.\nThe GEV distribution is suitable for maximum of a large number of observations: the larger the block size, the closer the approximation will be, but the smaller the sample size \\(m\\).in In practice, there is a natural block size (say yearly) over which to compute maximum. The advantage is that, even if data are not stationary, we can expect the maximum to occur roughly at the same time of the year (e.g., for temperature) and so even if the true block size is much lower than 365 days, we can still use our approach.\nThe fit.gev function includes two optimization routines: either use the PORT methods from nlminb, or Broyden-Fletcher-Goldfarb-Shanno algorithm (BFGS) inside a constrained optimization algorithm (augmented Lagrangian). The default option is nlminb, which sometimes returns diagnostics indicating false convergence when the model is near the maximum likelihood estimate.\nAs for other model, parameters can be fixed and nested models can be compared using the anova S3 method. For these, we distinguish between estimated coefficients (estimate) or with the coef method, and the full vector of parameters, param.\n\n\nNumerical example\nWe consider in the tutorial daily mean wind speed data, measured in km/h, at four weather stations located in the south of France. We first take the data for Lyon’s airport (station \\(S_2\\)) and compute the annual maximum.\n\nlibrary(mev)\n# library(xts)\n# library(lubridate)\ndata(frwind, package = \"mev\")\nlyon &lt;- with(frwind,\n             xts::xts(x = S2, order.by = date))\n# Create series of yearly maximum\nymax &lt;- xts::apply.yearly(lyon, max)\n\nWe can then fit a GEV distribution via maximum likelihood to the yearly maximum, extract the coefficients \\(\\widehat{\\boldsymbol{\\theta}}\\) and check convergence by computing the score \\(\\ell_{\\boldsymbol{\\theta}}(\\widehat{\\boldsymbol{\\theta}})\\) and comparing it to the zero vector.\n\nopt_gev &lt;- mev::fit.gev(xdat = ymax, show = TRUE)\n\nLog-likelihood: -141.6626 \n\nEstimates\n     loc     scale     shape  \n36.18449   3.94287  -0.01124  \n\nStandard Errors\n   loc   scale   shape  \n0.6589  0.4881  0.1318  \n\nOptimization Information\n  Convergence: successful \n  Function Evaluations: 27 \n  Gradient Evaluations: 11 \n\nmle &lt;- coef(opt_gev)\nisTRUE(all.equal(rep(0,3),\n                 mev::gev.score(par = mle, dat = ymax),\n                 check.attributes = FALSE,\n                 tolerance = 1e-5))\n\n[1] TRUE\n\n\nHaving found the MLE, we can compute the covariance matrix of the parameters from the observed information matrix. The standard errors are the square root of the elements on the diagonal. While mev uses exact formulae, these can be approximated by computing the hessian via finite differences.\n\n# Compute observed information matrix\njmat &lt;- mev::gev.infomat(par = mle, dat = ymax)\n# Compute standard errors\nse_mle &lt;- sqrt(diag(solve(jmat)))\n# Compare with 'mev' output\nisTRUE(all.equal(se_mle, opt_gev$std.err))\n\n[1] TRUE\n\n\nEven if we have parameter estimates, there is no guarantee that the model is adequate. Standard visual goodness-of-fit diagnostics can be obtained with the plot method. To see other methods, query methods(class = \"mev_gev\").\n\n# PP and QQ plots\npar(mfrow = c(1,2))\nplot(opt_gev)\n\n\n\ngraphics.off()\n\nThe Gumbel distribution, which corresponds to a GEV with shape \\(\\xi=0\\), can be estimated by restricting a parameter. We then do a likelihood ratio test since models are nested.\n\nopt_gumb &lt;- mev::fit.gev(xdat = ymax,\n                         fpar = list(shape = 0))\nanova(opt_gev, opt_gumb)\n\nAnalysis of Deviance Table\n\n         npar Deviance Df  Chisq Pr(&gt;Chisq)\nopt_gev     3   283.32                     \nopt_gumb    2   283.33  1 0.0073     0.9321\n\n\nNone of the parameters are of interest in themselves. We may be interested rather by a risk summary, which is a function of parameters. For example, we could get the parameters of the GEV for 50 years via max-stability and return the average if \\(\\widehat{\\xi}&lt;1\\), or quantiles — the most popular choices are the median and 0.368, which corresponds roughly to the 50 year return level for threshold exceedances.\nAll of these are invariant to reparametrization, so we can use the formula and plug-in the parameter values. For inference, we reparametrize the model in terms of this quantity, then vary over a grid of values of the 50-year average maximum and compute profile-likelihood-based confidence intervals at level 95%.\n\ngev.mle(xdat = ymax, args = \"Nmean\", N = 50)\n\n  Nmean \n53.4114 \n\n# Compute profile log-likelihood\nprof &lt;- mev::gev.pll(param = \"Nmean\", dat = ymax, N = 50)\n\n\n\n# Extract confidence intervals\n(confint(prof))\n\nEstimate Lower CI Upper CI \n53.41140 47.79346 73.63881 \n\n\n\n\nGeneralized Pareto distribution\nThe generalized Pareto (GP) distribution with scale \\(\\sigma \\in \\mathbb{R}_{+}\\) and shape \\(\\xi \\in \\mathbb{R}\\) is \\[\\begin{align*}\n  G(x)  =\n\\begin{cases}\n1-\\left(1+\\xi \\frac{x}{\\sigma}\\right)_{+}^{-1/\\xi}, &  \\xi \\neq 0,\\\\ 1-\n\\exp \\left(-\\frac{x}{\\sigma}\\right),&  \\xi = 0.\n\\end{cases}\n\\end{align*}\\] The range of the generalized Pareto distribution is \\([0, -\\sigma/\\xi)\\) if \\(\\xi &lt; 0\\) and is \\(\\mathbb{R}_{+}\\) otherwise. We denote the distribution by \\(\\mathsf{GP}(\\sigma, \\xi)\\). The default optimization algorithm for this model is that of Grimshaw (1993), which reduces the dimension of the optimization through profiling. The exponential distribution and the case \\(\\xi=-1\\) are handled separately. If the sample coefficient of variation is less than one, the global maximum lies on the boundary of the parameter space since there exists for any \\(\\xi&lt;-1\\) a value \\(\\sigma^*\\) such that \\(\\ell(\\sigma^*, \\xi) \\to \\infty\\): the search is thus restricted to \\(\\xi \\geq -1\\). These cases are more frequent in small samples due to the negative bias of the maximum likelihood estimator of the shape.\nExcept for this boundary case, the maximum likelihood estimator solves the score equation \\(\\partial \\ell(\\boldsymbol{\\theta}) / \\partial \\boldsymbol{\\theta} = \\boldsymbol{0}_2\\). We can thus check convergence by verifying that the score vanishes at the maximum likelihood estimate.\nIf \\(\\widehat{\\xi} &lt; -0.5\\), the asymptotic regime is nonstandard (Smith, 1985) and the standard errors obtained from the inverse information matrix are unreliable; as such, mev does not report them and prints an optional warning.\n\n\n\n\n\n\n\n\n\nThe figure shows the profile likelihood for \\(\\eta = -\\xi/\\sigma\\) for two datasets, one of which (leftmost) achieves its maximum at \\(\\widehat{\\xi} = -1\\) and \\(\\widehat{\\eta} = 1/\\max(\\boldsymbol{y})\\).\n\n# Only keep data from September to April\nwindlyon &lt;- with(\n  frwind, \n  S2[lubridate::month(date) &lt;= 4 | \n       lubridate::month(date) &gt;= 9])\n# Keep only 100 largest points (fewer because of ties)\nu &lt;- quantile(windlyon, \n              probs = 1-100/length(windlyon))\n# Fit generalized Pareto via ML\nfitted_gp &lt;- fit.gpd(\n  xdat = windlyon,\n  threshold = u,\n  show = TRUE)\n\nMethod: Grimshaw \nLog-likelihood: -207.5276 \n\nThreshold: 33.84 \nNumber Above: 90 \nProportion Above: 0.0079 \n\nEstimates\n  scale    shape  \n3.57863  0.03088  \n\nStandard Errors\n scale   shape  \n0.6091  0.1337  \n\nOptimization Information\n  Convergence: successful \n\n# P-P and Q-Q diagnostic plots \npar(mfrow = c(1, 2))\nplot(fitted_gp)\n\n\n\n\n\n\n\ngraphics.off()\n# Fit exponential by passing a list with a fixed parameter\nreduced_gp &lt;- fit.gpd(windlyon,\n                   threshold = u, \n                   fpar = list(shape = 0))\n# The MLE is sample mean of exceedances - check this\nisTRUE(coef(reduced_gp) == mean(windlyon))\n\n[1] TRUE\n\n# Compare nested models using likelihood ratio test\nanova(fitted_gp, reduced_gp)\n\nAnalysis of Deviance Table\n\n           npar Deviance Df  Chisq Pr(&gt;Chisq)    \nfitted_gp     2   415.06                         \nreduced_gp    1   502.50  1 87.448  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe mev package includes alternative routines for estimation, including the optimal bias-robust estimator of Dupuis (1999) and the approximate Bayesian estimators of Zhang & Stephens (2009) and Zhang (2010). The latter two are obtained by running a Markov chain Monte Carlo algorithm, but only the posterior mean and standard deviation are returned to reduce the memory footprint of the returned object, and these are calculated on the fly using running mean and variance estimators.\n\n# Bayesian point estimates (based on MAP)\nfit.gpd(windlyon, \n        threshold = u, \n        show = TRUE, \n        MCMC = TRUE,\n        method = \"zhang\")\n\n\nMethod: Zhang \n\nThreshold: 33.84 \nNumber Above: 90 \nProportion Above: 0.0079 \n\nApproximate posterior mean estimates\nscale  shape  \n3.456  0.066  \n\nPosterior mean estimates\n scale   shape  \n3.5594  0.0495  \n\nMonte Carlo standard errors\nscale  shape  \n0.453  0.130  \n\nEstimates based on an adaptive MCMC\n Runs:    10000 \n Burnin:  2000 \n Acceptance rate: 0.43 \n Thinning: 1 \n\n\nIf the sample is small, maximum likelihood estimators are biased for the generalized Pareto distribution (the shape parameter is negatively biased, regardless of the true value for \\(\\xi\\)). Bias correction methods includes the modified score of Firth, but the default method is the implicit correction (subtract), which solves the implicit equation \\[\\begin{align}\n   \\boldsymbol{\\tilde{\\theta}}=\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{b}(\\tilde{\\boldsymbol{\\theta}}). \\label{eq:implbias}\n\\end{align}\\] The point estimate \\(\\boldsymbol{\\tilde{\\theta}}\\) is obtained numerically as the root of this nonlinear system of equations. In the present case, the sample size is large and hence the first-order correction, derived through asymptotic arguments from the generalized Pareto distribution likelihood, is small. Note that the bias correction requires \\(\\xi &gt; -1/3\\), since it is based on third-order cumulants of the distribution.\n\n# First-order bias corrected estimates\ncorr_coef &lt;- gpd.bcor(par = coef(fitted_gp), \n                      dat = windlyon, \n                      corr = \"firth\")\n\nError in bcor.st$value : $ operator is invalid for atomic vectors\n\n\n\n\nInhomogeneous Poisson process\nLet \\(Y_{(1)} \\geq \\cdots \\geq Y_{(r)}\\) denote the \\(r\\) largest observations from a sample. The likelihood of the limiting distribution of the point process for the \\(r\\)-largest observations is, for \\(\\mu,\\xi\\in\\mathbb{R}, \\sigma&gt;0\\), [ (,,; ) -r() - (1+){j=1}^r (1 + ){+} - (1 + )^{-1/}_+. ] This likelihood can be used to model the \\(r\\)-largest observations per block or threshold exceedances where the threshold is the \\(r\\)th order statistic\nConsider a sample of \\(N\\) observations, of which \\(n_u\\) exceed \\(u\\) and which we denote by \\(y_1, \\ldots, y_{n_u}\\). The likelihood associated to the limiting distribution of threshold exceedances is, for \\(\\mu, \\xi \\in \\mathbb{R}, \\sigma &gt;0\\), \\[\\begin{align}\nL(\\mu, \\sigma, \\xi; \\boldsymbol{y}) = \\exp \\left[ - c \\left\\{1+ \\xi \\left( \\frac{u-\\mu}{\\sigma}\\right)\\right\\}^{-1/\\xi}_{+}\\right] (c\\sigma)^{-n_u}\\prod_{i=1}^{n_u} \\left\\{1+\\xi\\left( \\frac{y_i-\\mu}{\\sigma}\\right)\\right\\}^{-1/\\xi-1}_{+},\\label{eq:ppp_lik}\n\\end{align}\\] where \\((\\cdot)_{+} = \\max\\{0, \\cdot\\}\\). The quantity \\(c\\) is a tuning parameter whose role is described in 7.5 of Coles (2001). If we take \\(c=N/m\\), the parameters of the point process likelihood correspond to those of the generalized extreme value distribution fitted to blocks of size \\(m\\). The NHPP likelihood includes a contribution for the fraction of points that exceeds the threshold, whereas the generalized Pareto is a conditional distribution, whose third parameter is the normalizing constant \\(\\zeta_u=\\Pr(Y&gt;u)\\). Since the latter has a Bernoulli and \\(\\zeta_u\\) is orthogonal to the pair \\((\\sigma, \\xi)\\), it is often omitted from further analyses and estimated as the proportion of samples above the threshold.\nThe model includes additional arguments, np and npp (number of observations per period). If data are recorded on a daily basis, using a value of npp = 365.25 yields location and scale parameters that correspond to those of the generalized extreme value distribution fitted to block maxima. Alternatively, one can specify instead the number of periods np, akin to \\(n_y\\) in Eq. 7.8 of Coles (2001) — only the latter is used by the function, with npp*np theoretically equal to the number of exceedances.\nThe tuning parameters impact the convergence of the estimation since the dependence between parameters becomes very strong: Sharkey & Tawn (2017) suggest to pick a value of np that near-orthogonalize the parameters. Wadsworth:2011 recommended picking this to be the number of observations (so npp=1), but Moins et al. (2023) show that a better choice leads to orthogonalization.\nAnother option is to fit the generalized Pareto model: if the probability of exceeding threshold \\(u\\) is small, the Poisson approximation to binomial distribution implies [c {1+ ( )}^{-1/} n_u, ] where \\(n_u\\) is the number of threshold exceedances above \\(u\\) and \\(c\\) is the tuning parameter np. With the point estimates of the generalized Pareto model, say \\((\\widehat{\\sigma}_u, \\widehat{\\xi})\\), we thus use \\[\\begin{align*}\n\\mu_0 &= u - \\sigma_0\\{(n_u/c)^{-\\widehat{\\xi}}-1\\}/\\widehat{\\xi},\\\\\n\\sigma_0 &= \\widehat{\\sigma}_u\\times (n_u/c)^{\\widehat{\\xi}},\n\\end{align*}\\] and \\(\\xi_0=\\widehat{\\xi}\\) as starting values. Most of the time, these values are so close to the solution of the score equation that numerical convergence of the optimization routine is all but guaranteed in a few likelihood evaluations. If no starting value is provided and some fixed parameters are provided, the model will approximate the distribution of the vector of parameters by a multivariate Gaussian distribution and compute the best linear predictor of the remaining parameters given those are fixed. This method works well if the log-likelihood is near quadratic and the values are not too far from the maximum, but does not deal with the boundary constraints. In case these starting values are invalid, and an error message is returned.\nThe log likelihood of the \\(r\\) largest order statistics likelihood is derived from the inhomogenenous Poisson point process formulation. We normally consider a matrix of observations \\(m \\times r\\) containing the largest \\(r\\) order statistics of each block out of the \\(n\\) sample. The parameters of the \\(r\\)-largest likelihood are the same as the generalized extreme value distribution (a special case when \\(r=1\\)). This model can be used when we have access to order statistics, assuming that the observations within the block are independent.\n\\[\\begin{align*}\n\\ell(\\mu,\\sigma,\\xi; \\boldsymbol{y}) &= -rm\\log(\\sigma) - \\left(1+\\frac{1}{\\xi}\\right)\\sum_{i=1}^m\\sum_{j=1}^r \\log\\left(1 + \\xi\\frac{y_{i,(j)}-\\mu}{\\sigma}\\right)_{+} \\\\ &\\quad- \\left(1 + \\xi\\frac{y_{i,(r)}-\\mu}{\\sigma}\\right)^{-1/\\xi}_+, \\quad \\mu,\\xi\\in\\mathbb{R}, \\sigma&gt;0. \\label{eq:rlarglik}\n\\end{align*}\\] It’s not obvious how to chose \\(r\\), but (Belzile:2022?) shows the gain from considering larger values of \\(r\\) decreases quickly for the shape. The support constraints, typically arising for the minimum observation, means that finding good starting values is hard.\nWe can simulate from the \\(r\\) largest observations by drawing from a unit rate Poisson process \\(0&lt;U_1&lt;U_2&lt;\\cdots\\), where \\(U_j=E_1+\\cdots+E_j\\) and \\(E_j\\sim \\mathsf{Exp}(1)\\), and setting \\(Y_{j} = \\mu + \\sigma\\big(U_j^{-1/\\xi}-1\\big)/\\xi\\). Applying the inverse transformation \\(\\Lambda_{\\boldsymbol{\\theta}}(y) = \\left\\{ 1 + \\xi(y-\\mu)/\\sigma\\right\\}^{-1/\\xi}_+\\) evaluated at the MLE gives roughly independent exponential spacings, which can be used to create quantile-quantile plots."
  },
  {
    "objectID": "content/likelihood.html#risk-measures",
    "href": "content/likelihood.html#risk-measures",
    "title": "Likelihood-based inference",
    "section": "Risk measures",
    "text": "Risk measures\nTwo typical questions in extreme values are: given the intensity of an extreme event, what is its recurrence period? and what is a typical worst-case scenario over a given period of time? For the latter, suppose for simplicity that the daily observations are blocked into years, so that inference is based on \\(N\\) points for the \\(N\\) years during which the data were recorded. The return level is a quantile of the underlying distribution corresponding to an event of probability \\(p=1-1/T\\) for an annual maximum, which is interpreted as ``the level exceeded by an annual maximum on average every \\(T\\) years’’. If observations are independent and identically distributed, then we can approximate the probability that a return level is exceeded \\(l\\) times over a \\(T\\) year period using a binomial distribution with probability of success \\(1-1/T\\) and \\(T\\) trials. For \\(T\\) large, the return level is exceeded \\(l=0, 1, 2, 3, 4\\) times within any \\(T\\)-years period with approximate probabilities 36.8%, 36.8%, 18.4%, 6.1% and 1.5%. The probability that the maximum observation over \\(T\\) years is exceeded with a given probability is readily obtained from the distribution of the \\(T\\)-year maximum, leading (Cox et al., 2002, p. 3(b)) to advocate its use over return levels, among other quantities of interest such as the number of times a threshold \\(u\\) will be exceeded in \\(T\\) years or the average number of years before a threshold \\(u\\) is exceeded.\nQuantiles, mean and return levels of \\(T\\)-maxima: consider the distribution \\(H(x) = G^T(x)\\) of the maximum of \\(T\\) independent and identically distributed generalized extreme value variates with parameters \\((\\mu, \\sigma, \\xi)\\) and distribution function \\(G\\). By max-stability, the parameters of \\(H(x)\\) are \\(\\mu_T=\\mu-\\sigma(1-T^\\xi)/\\xi\\) and \\(\\sigma_T=\\sigma T^\\xi\\) when \\(\\xi \\neq 0\\). We denote the expectation of the \\(T\\)-observation maximum by \\(\\mathfrak{e}_T\\), the \\(p\\) quantile of the \\(T\\)-observation maximum by \\(\\mathfrak{q}_p = H^{-1}(p)\\) and the associated return level by \\(z_{1/T} = G^{-1}(1-1/T)\\). Then, any of these three quantities can be written as \\[\\begin{align*}\n\\begin{cases}\n\\mu-\\frac{\\sigma}{\\xi}\\left\\{1-\\kappa_{\\xi}\\right\\}, &  \\xi &lt;1, \\xi \\neq 0, \\\\\n\\mu+\\sigma\\kappa_0, &  \\xi =0,\n  \\end{cases}\n\\end{align*}\\] where \\(\\kappa_{\\xi}=T^\\xi\\Gamma(1-\\xi)\\) for \\(\\mathfrak{e}_T\\), \\(\\kappa_{\\xi}=T^\\xi\\log(1/p)^{-\\xi}\\) for \\(\\mathfrak{q}_p\\) and \\(\\kappa_{\\xi}=\\left\\{-\\log\\left(1-{1}/{T}\\right)\\right\\}^{-\\xi}\\) for \\(z_{1/T}\\). In the Gumbel case, we have \\(\\kappa_0=\\log(T)+\\gamma_{e}\\) for \\(\\mathfrak{e}_T\\), \\(\\kappa_0=\\log(T)-\\log\\{-\\log(p)\\}\\) for \\(\\mathfrak{q}_p\\) and \\(\\kappa_0=-\\log\\{-\\log(1-1/T)\\}\\) for \\(z_{1/T}\\)."
  },
  {
    "objectID": "content/regression.html",
    "href": "content/regression.html",
    "title": "Regression models",
    "section": "",
    "text": "Most data encountered display various forms of nonstationarity, including trends, seasonality and covariate effects, which the extreme value distributions cannot capture without modification. In environmental applications, this may be partly attributed to different weather patterns, climate change, etc.\nThere are multiple strategies that one can consider for modelling. The first consists in fitting a regression for the whole data and perform extreme value analysis with the residuals, as before assuming stationarity (Eastoe & Tawn, 2009). The other, proposed by Davison & Smith (1990), tries to incorporate covariates in the parameters \\(\\mu\\), \\(\\sigma\\), etc. — fixing the shape parameters is often recommended as it is hard to estimate.\nGeneral linear modelling would consist in regression models, e.g., \\[\\begin{align*}\n\\mu(\\mathbf{X}) = \\beta_0 + \\beta_1 \\mathrm{X}_1 + \\cdots \\beta_p \\mathrm{X}_p,\n\\end{align*}\\] and estimate as before parameters by maximum likelihood. The difficulty now is that there are more parameters to estimate and the support restriction translates into up to \\(n\\) inequality constraints, as they must be supported for every combination of covariates found in the database. These two facts mean numerical optimization is more difficult.\nIn models with a relatively large number of parameters, it is useful to include additive penalty terms to the log likelihood: for example, generalized additive models for the parameters include smooth functions, typically splines, with a penalty that controls the wiggliness of the estimated predictor functions. The latter is typically evaluated using the second-order derivative of the basis functions.\nIn nonstationary models, risk measures of interest are defined conditionally on the value of covariates: for example, the \\(1-p\\) conditional return level is (Eastoe & Tawn, 2009) \\[\\begin{align*}\n\\Pr(Y_t  &gt; y \\mid \\mathbf{X}_t =\\boldsymbol{x}_t) = p\n\\end{align*}\\] and the corresponding unconditional return level, \\[\\begin{align*}\n\\int_{\\mathcal{X}} \\Pr(Y_t  &gt; y \\mid \\mathbf{X}_t =\\boldsymbol{x}_t) \\mathrm{d} P(\\boldsymbol{x}_t),\n\\end{align*}\\] is obtained by averaging out over the distribution of covariates that are employed in the model. For future quantities, this may or not be a sensible risk summary to compute1 and may prove tricky to obtain as it requires either knowledge about the future distribution of the covariates, or else a perhaps unrealistically strong stationary assumption.\nSome parametrizations are better suited than others for regression modelling: for the nonstationary case, the generalized Pareto model with varying scale and shape is not stationary unless, for any \\(v\\) greater than the original threshold \\(u\\), \\[\\begin{align*}\n\\sigma_v(\\boldsymbol{x}_t) = \\sigma_u(\\boldsymbol{x}_t) + (v-u) \\xi(\\boldsymbol{x}_t)\n\\end{align*}\\] which, even with constant shape \\(\\xi\\) must imply a linear or constant functional form for \\(\\sigma_u\\). Using the inhomogeneous Poisson point process representation avoids these problems."
  },
  {
    "objectID": "content/regression.html#generalized-additive-models-for-extremes",
    "href": "content/regression.html#generalized-additive-models-for-extremes",
    "title": "Regression models",
    "section": "Generalized additive models for extremes",
    "text": "Generalized additive models for extremes\nThe function evgam from the eponymous package allows one to specify smooth functional forms and objective estimation of the smoothing parameters using Laplace’s methods (Wood et al., 2016), building on the mgcv package of Simon Wood (Wood, 2017).\nThe setup is evgam(formula, data, family, ...), where formula is a list of formula for parameters (in the order location, scale, shape) and family is the character string for the extreme value distribution. Choices include gev, gpd, rlarg and ald for asymmetric Laplace, used in quantile regression, among other.\n\nlibrary(evgam)\ndata(frwind, package = \"mev\")\nlyon &lt;- with(frwind,\n             xts::xts(x = S2, order.by = date))\nymax &lt;- xts::apply.yearly(lyon, max)\nyears &lt;- unique(lubridate::year(lyon))\nopt_gev_spl &lt;- evgam::evgam(\n  data = data.frame(\n    syear = scale(years),\n    ymax = ymax),\n  formula = list(ymax ~ s(syear, k = 5, bs = \"cr\"),\n                 ~ 1, \n                 ~ 1),\n  family = \"gev\")\n## Summary with coefficients\nsummary(opt_gev_spl)\n\n\n** Parametric terms **\n\nlocation\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    36.62       0.66   55.46   &lt;2e-16\n\nlogscale\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     1.36       0.12   11.23   &lt;2e-16\n\nshape\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    -0.13       0.13   -0.99    0.162\n\n** Smooth terms **\n\nlocation\n          edf max.df Chi.sq Pr(&gt;|t|)\ns(syear) 2.25      4   5.62   0.0776\n\n## Plot splines (if any)\nplot(opt_gev_spl)\n\n\n\n## Fitted value, depend on covariates\n# predict(opt_gev_spl)\n\nGiven the fitted model is fitted with a quadratic penalty, we can view this under the Bayesian lens as a Gaussian prior. The posterior distribution isn’t available in closed-form, but we can do a Gaussian approximation at the mode on a suitable scale (e.g., log-scale) and transform them back on the data scale. We can next simulate from the approximate posterior predictive to get samples for a new combination of covariates, or for the data matrix that was used as covariates for the fitted model (default).\nFigure 1 shows the output density for each time period, which is the only one that varies in time: all other are drawn from the same marginal distribution, even if simulated samples are different for each parameter combination. If the model is severely overfitted, this will be visible because the posterior standard deviation will be tiny.\n\n## Simulate from the posterior of parameters\npost_sim &lt;- simulate(opt_gev_spl, nsim = 1000L, seed = 2023)\nlibrary(ggplot2)\nggplot(\n  data = data.frame(\n    location = c(post_sim$location),\n    year = factor(rep(1:length(years), \n               length.out = prod(dim(post_sim$location))))),\n  mapping = aes(x = location,\n                color = year,\n                group = year)) +\n  geom_density() +\n  theme_minimal() +\n  viridis::scale_color_viridis(discrete = TRUE) +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 1: Density plots of 1000 posterior samples based on a normal approximation to the posterior of the location parameter of the generalized extreme value distribution, colored by year."
  },
  {
    "objectID": "content/regression.html#footnotes",
    "href": "content/regression.html#footnotes",
    "title": "Regression models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhat does return levels mean in a nonstationary climate? See Rootzén & Katz (2013) for an alternative.↩︎"
  },
  {
    "objectID": "content/semiparametric.html",
    "href": "content/semiparametric.html",
    "title": "Semiparametric methods and Hill estimation",
    "section": "",
    "text": "There are multiple alternative estimators of the shape parameter for heavy-tailed data, the most popular of which is the Hill (1975) estimator . Consider a random sample of order statistics \\(Y_{(1)} &gt; \\cdots &gt; Y_{(n)}\\). The latter is, for exceedances above \\(Y_{(n-k)} &gt;0\\), \\[\\begin{align}\nH_{n,k}(\\boldsymbol{Y}) = \\frac{1}{k}\\sum_{i=n-k+1}^{n} \\log Y_{(i)} - \\log Y_{(n-k)}, \\qquad (j=1, 2), \\label{eq:Hillest}\n\\end{align}\\]\nUnder a second order regular variation assumption and provided \\(\\lim_{k \\to \\infty} k^{1/2}A(n/k) = \\lambda\\in \\mathbb{R}\\), Hill’s estimator is asymptotically normal with \\[\\begin{align*}\nk^{1/2}(H_{n,k} - \\xi) \\to \\mathsf{No}\\{\\lambda/(1-\\rho), \\xi^2\\}, \\qquad \\xi&gt;0, \\rho \\leq 0;\n\\end{align*}\\] the asymptotic bias term is dictated by the rate at which the number of extreme observations grows relative to the total sample size and depends on the unknown second-order auxiliary function \\(A(\\cdot)\\) which is distribution-specific. The estimator is consistent for \\(\\xi\\) provided \\(k=k(n)\\) is an intermediate sequence satisfying \\(k/n \\to 0\\) as \\(k, n \\to \\infty\\) such that \\(\\lambda \\to 0\\). Note the asymptotic variance, to be constrasted with that of the maximum likelihood estimator of the shape for the generalized Pareto shape, which is \\((1+\\xi)^2\\)."
  },
  {
    "objectID": "content/semiparametric.html#threshold-selection-for-hills-estimator",
    "href": "content/semiparametric.html#threshold-selection-for-hills-estimator",
    "title": "Semiparametric methods and Hill estimation",
    "section": "Threshold selection for Hill’s estimator",
    "text": "Threshold selection for Hill’s estimator\n\ndata(frwind, package = \"mev\")\nlyon &lt;- sort(frwind$S2, decreasing = TRUE)\nremotes::install_github(\"lbelzile/rbm\", quiet = TRUE)\n# Fit Hill's estimator\nhill_est &lt;- rbm::hill(data_array = lyon,\n                      idx = 1:400)\nplot(hill_est)\n\n\n\n\nWe can see that the Hill estimator values are quite stable (which is seldom the case) and however around 0.1. However, maximum likelihood estimators of the shape parameter \\(\\xi\\) are much closer to zero and negative for other wind speed series. The noticeable tilted staircase pattern is an artefact of rounding.\nA simple graphical method for choosing the number of order statistics to keep is a plot of the rank against shape parameters, termed Hill plots. Practical recommendations are that (1) the number of order statistics should be restricted (say 20 to 500), (2) the graph is more easily interpreted when the \\(x\\)-axis shows normalized log ranks \\(\\log(k)/\\log(n)\\), and (3) parameters should be smoothed using a moving window estimator, as the sample path of the Hill estimator are analogous to a Brownian motion.\nNote that Hill estimator is not location invariant. The pointwise confidence intervals reported by must methods are based on exact Pareto tails, so are at best approximate.\n\nevmix::hillplot(data = lyon[1:300],\n                hill.type = \"SmooHill\", \n                r = 3L, \n                x.theta = TRUE)\n\n\n\n\nThere are multiple alternative estimators of the shape parameter: extensive simulation studies show that the threshold selection performance using the random block maxima estimator of Wager (2014) is competitive. The latter is a \\(U\\) statistic and has \\(\\mathcal{C}^{\\infty}\\) sample paths: the selection is based on empirical risk minimization using a finite-difference approximation to the squared derivative of the process, subject to a penalty term. Although the computational cost is higher than Hill’s estimator, it can be kept reasonable by restricting attention to only largest exceedances.\n\nrbm_est &lt;- rbm::rbm.point_estimate(lyon[1:300])\nplot &lt;- rbm::rbm.plot(lyon[1:300])\n\n\n\n\nOther threshold selection methods, including the minimization of the asymptotic mean squared error of the Hill estimator highlighted in Section 2 of Caeiro & Gomes (2016), also works well but can fail catastrophically in some settings. Here, the answer (in terms of the tail index \\(\\alpha=1/\\xi\\)), is similar to other packages\n\nest_AMSE &lt;- tea::dAMSE(lyon[1:300])\n\nMany such estimators are available from the tea package. Extensions that deal with censoring and conditional estimators (ReIns), time series (extremefit), etc. but we will not attempt to cover those."
  },
  {
    "objectID": "content/semiparametric.html#quantile-estimator",
    "href": "content/semiparametric.html#quantile-estimator",
    "title": "Semiparametric methods and Hill estimation",
    "section": "Quantile estimator",
    "text": "Quantile estimator\nGiven an estimate of a positive shape parameter, we can get quantile estimates through Weissman (1978) formula. The estimator of the quantile at level \\(1-p\\), for small \\(p\\), is \\[\\begin{align*}\nQ^W_{k,n}(1-p) = Y_{(n-k)} \\left\\{ \\frac{k+1}{p(n+1)} \\right\\}^{H_{k,n}},                                                                     \\end{align*}\\] where \\(H_{k,n}\\) is the Hill estimator of the shape parameter and \\(Y_{(n-k)}\\) is the \\((n-k)\\)th order statistic, acting as threshold. While there are software that return these quantities, including the ReIns, evt0 and extremefit packages, they are easily coded.\nBelow, we show how to estimate the 0.999 quantile of the distribution by extrapolating the shape\n\nqweissman &lt;- function(n, p, k, thresh, shape){\n  thresh * ((k + 1) / (1 - p)/ (n + 1))^shape\n}\nquants &lt;- qweissman(n = length(lyon), \n          p = seq(0.99, 0.999, length.out = 101), \n          thresh = est_AMSE$threshold,\n          k = est_AMSE$k0, \n          shape = 1/est_AMSE$tail.index)\n\nUncertainty statements, if any, could be obtained by bootstrap methods or using asymptotic normality, but given the sampling distribution of the quantile estimator is strongly asymmetric, Wald-type (symmetric) confidence intervals are bound to give poor coverage."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Description",
    "section": "",
    "text": "Description\nThis satellite workshop will review R implementations of a variety of techniques for statistical analysis of extreme values. The focus of the first part of the workshop will be on univariate extremes, including likelihood-based, Bayesian and nonparametric methods for both peaks-over-threshold and block maxima approaches, with a foray into nonstationary extremes. The second part of the workshop will concentrate on conditional extremes and time series.\nThe tutorial takes place Friday, June 30th, 2023, from 14:00 until 18:15.\n\n\nCourse content\nUnivariate extremes:\n\nLikelihood-based modelling\nThreshold selection for peaks over threshold\nSemiparametric methods\nBayesian inference\nNonstationary extremes\n\nMultivariate extremes and time series:\n\nConditional extremes model\nTime series\n\n\n\nInstructions\nWe will be using multiple R packages from the Comprehensive R Archive Network, as well as development versions of some packages.\nIf you plan on using your own laptop, download and install R (current version 4.3.0, nicknamed “Already Tomorrow”) and an integrated development environment such as RStudio.\n\ndevpkgs &lt;- c(\"lbelzile/mev\", \"lbelzile/rbm\")\ncranpkgs &lt;- c(\"lite\", \"revdbayes\", \"texmex\",\n              \"threshr\", \"tea\", \"evt0\", \"remotes\",\n              \"xts\", \"lubridate\", \"ggplot2\")\ninstall.packages(cranpkgs)\nremotes::install_github(devpkgs)\n\n\n\nInstructors\n\nThomas Opitz, INRAE\nLéo Belzile, HEC Montréal"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#plan-for-today",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#plan-for-today",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Plan for today",
    "text": "Plan for today\n\n\n\nUnivariate extremes\n\nMaximum likelihood\nBayesian\nSemiparametric methods\n\nRegression models\n\n\n\nConditional extremes\nModels for time series"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#extremal-type-theorem",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#extremal-type-theorem",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Extremal type theorem",
    "text": "Extremal type theorem\nConsider \\(Y_i\\) \\((i=1,2,\\ldots)\\) i.i.d. with distribution \\(F\\).\nIf there exist normalizing sequences \\(a_n&gt;0\\) and \\(b_n \\in \\mathbb{R}\\) such that \\[\\begin{align}\n\\lim_{n \\to \\infty} \\Pr\\left(\\frac{\\max_{i=1}^n Y_i - b_n}{a_n} \\leq x \\right) = G(x),\n\\label{eq:gevconv}\n\\end{align}\\] for \\(G\\) a non-degenerate distribution, then \\(G\\) must be generalized extreme value (GEV)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#generalized-extreme-value-distribution",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#generalized-extreme-value-distribution",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Generalized extreme value distribution",
    "text": "Generalized extreme value distribution\nWith location \\(\\mu \\in \\mathbb{R}\\), scale \\(\\sigma \\in \\mathbb{R}_{+}\\) and shape \\(\\xi \\in \\mathbb{R}\\) parameters, the distribution function is \\[\\begin{align*}\nG(x) =\\begin{cases}\n\\exp\\left\\{-\\left(1+\\xi \\frac{x-\\mu}{\\sigma}\\right)^{-1/\\xi}_{+}\\right\\}, & \\xi \\neq 0;\\\\\n\\exp\\left\\{-\\exp\\left(-\\frac{x-\\mu}{\\sigma}\\right)\\right\\}, & \\xi = 0,\n\\end{cases}\n\\end{align*}\\] where \\(x_{+} = \\max\\{x, 0\\}\\).\nThe support is \\(\\{x \\in \\mathbb{R}: \\xi(x-\\mu)/\\sigma &gt; -1\\}\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#max-stability-property",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#max-stability-property",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Max-stability property",
    "text": "Max-stability property\nIf \\(Y_i \\sim \\mathsf{GEV}(\\mu, \\sigma, \\xi)\\) are independent, then \\[\\max_{i=1}^N Y_i \\sim \\mathsf{GEV}(\\mu_N, \\sigma_N, \\xi),\\] where\n\n\\(\\mu_N = \\mu + \\sigma(N^\\xi-1)/\\xi\\)\n\\(\\sigma_N = \\sigma N^\\xi\\)\n\n(case \\(\\xi=0\\) defined by continuity)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#block-maximum",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#block-maximum",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Block maximum",
    "text": "Block maximum\nWe can\n\npartition data into blocks of roughly equal size \\(m\\) and\nfit a GEV distribution to the maximum of the blocks.\n\n\nlibrary(mev)\nlibrary(xts)\nlibrary(lubridate)\ndata(frwinds, package = \"mev\")\nlyon &lt;- with(frwind, \n             xts(x = S2, order.by = date))\n# Create series of yearly maximum\nymax &lt;- apply.yearly(lyon, max)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#basics-of-likelihoods",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#basics-of-likelihoods",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Basics of likelihoods",
    "text": "Basics of likelihoods\n\nDenote by \\(\\boldsymbol{\\theta} \\in \\mathcal{S} \\subseteq \\mathbb{R}^p\\) the parameter vector.\nAssume data has joint density \\(f(\\boldsymbol{y}; \\boldsymbol{\\theta})\\).\nThe log likelihood is \\(\\ell(\\boldsymbol{\\theta}) = \\log f(\\boldsymbol{y}; \\boldsymbol{\\theta})\\).\nIf the \\(n\\) observations are independent with density or mass function \\(f_i\\), then \\(\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\log f_i(y_i; \\boldsymbol{\\theta})\\).\nThe maximum likelihood estimate \\(\\widehat{\\boldsymbol{\\theta}}\\) is found by maximizing (numerically) \\(\\ell(\\boldsymbol{\\theta})\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#fitting-gev-using-mev-package",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#fitting-gev-using-mev-package",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Fitting GEV using mev package",
    "text": "Fitting GEV using mev package\n\nopt_gev &lt;- mev::fit.gev(xdat = ymax, show = TRUE)\n## Log-likelihood: -142 \n## \n## Estimates\n##     loc    scale    shape  \n## 36.1845   3.9429  -0.0112  \n## \n## Standard Errors\n##   loc  scale  shape  \n## 0.659  0.488  0.132  \n## \n## Optimization Information\n##   Convergence: successful \n##   Function Evaluations: 27 \n##   Gradient Evaluations: 11\nmle &lt;- coef(opt_gev)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#goodness-of-fit-diagnostics",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#goodness-of-fit-diagnostics",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Goodness-of-fit diagnostics",
    "text": "Goodness-of-fit diagnostics\nCustom methods (print, plot, coef, etc.) are defined\n\nmethods(class = \"mev_gev\")\n## [1] anova  coef   logLik nobs   plot   print  vcov  \n## see '?methods' for accessing help and source code\npar(mfrow = c(1,2))\nplot(opt_gev)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#numerical-tricks",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#numerical-tricks",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Numerical tricks",
    "text": "Numerical tricks\n\nStandardize observations (e.g., scale) to facilitate optimization — the GEV is a location-scale family.\nEven if the limit is continuous and well defined at \\(\\xi=0\\), the log likelihood and it’s derivatives involves terms of the form \\(\\log(1+\\xi x)\\), which are numerically unstable when \\(\\xi \\to 0\\).\n\nPro tip: do not code the likelihood yourself! Otherwise,\n\nuse high precision arithmetic, e.g., log1p\nreplace the terms that blow up by Taylor series expansion near \\(\\xi=0\\) (interpolation)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#why-use-maximum-likelihood",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#why-use-maximum-likelihood",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Why use maximum likelihood?",
    "text": "Why use maximum likelihood?\n\nEasy to generalize to complex settings (nonstationarity, regression models, censoring, rounding, etc.)\nKnown to be asymptotically most efficient (Cramér-Rao bound), even if they can be biased.\nPoint estimators, etc. are invariant to reparametrization."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#invariance-property-of-maximum-likelihood",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#invariance-property-of-maximum-likelihood",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Invariance property of maximum likelihood",
    "text": "Invariance property of maximum likelihood\nIf \\(h\\) is a mapping, then \\(h(\\widehat{\\boldsymbol{\\theta}})\\) is the MLE of \\(h(\\boldsymbol{\\theta})\\).\nFor example, the expected maximum for \\(n=50\\) years of data, assuming annual maximum are exactly GEV and \\(\\xi &lt; 1\\), is \\[\\begin{align*}\\mathfrak{e}_N = h(\\mu, \\sigma, \\xi) = \\mu_N + \\sigma_N{\\Gamma(1-\\xi)-1\\}/\\xi.\n\\end{align*}\\]\nThus, the MLE \\(\\widehat{\\mathfrak{e}}_N=h(\\widehat{\\mu}, \\widehat{\\sigma}, \\widehat{\\xi})\\).\n\n# MLE of expectation of maximum of 50 blocks\ngev.mle(xdat = ymax, args = \"Nmean\", N = 50)\n## Nmean \n##  53.4"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#score-vector",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#score-vector",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Score vector",
    "text": "Score vector\nWhen the log likelihood is differentiable, the MLE is the root of the score equation, meaning \\(\\ell_{\\boldsymbol{\\theta}}(\\widehat{\\boldsymbol{\\theta}}) = \\partial \\ell(\\boldsymbol{\\theta}) / \\partial \\boldsymbol{\\theta} = \\boldsymbol{0}_p\\).\n\nmev::gev.score(par = mle, dat = ymax) # score\n## [1]  4.24e-08 -5.94e-08 -1.91e-07\n\n\nGradient-based algorithms exploit this feature for optimization\nbut beware of support constraints!\n\nBest to reparametrize so that the parameter space is \\(\\mathbb{R}^p\\) if possible."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#information-matrix-and-standard-errors",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#information-matrix-and-standard-errors",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Information matrix and standard errors",
    "text": "Information matrix and standard errors\nWe can extract standard errors by taking the square root of the diagonal elements of the inverse of either\n\nthe Fisher information, \\(\\imath(\\boldsymbol{\\theta}) = \\mathsf{Cov}\\{\\ell_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta})\\}\\) or\nthe observed information \\(\\jmath(\\boldsymbol{\\theta}) = - \\partial^2 \\ell(\\boldsymbol{\\theta})/ \\partial \\boldsymbol{\\theta} \\partial \\boldsymbol{\\theta}^\\top\\),\n\nboth evaluated at the MLE \\(\\widehat{\\boldsymbol{\\theta}}\\).\n\n# Compute observed information matrix\njmat &lt;- mev::gev.infomat(par = mle, dat = ymax)\n# Compute standard errors\nsqrt(diag(solve(jmat)))\n##   loc scale shape \n## 0.659 0.488 0.132\n# Compare with opt$std.err"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#some-remarks",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#some-remarks",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Some remarks",
    "text": "Some remarks\nWe may compute \\(j(\\widehat{\\boldsymbol{\\theta}})\\) (the negative Hessian of log likelihood) numerically through finite differences.\nMany software implementations compute MLE via Nelder–Mead simplex algorithm:\n\ncheck the gradient and/or\nthe log likelihood differences\n\nto make sure the optimisation was successful."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#properties-of-mle",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#properties-of-mle",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Properties of MLE",
    "text": "Properties of MLE\n\nMaximum likelihood estimators are asymptotically Gaussian whenever \\(\\xi &gt; -1/2\\) with data in domain of attraction of extreme value distribution.\nConsistency requires that one increases block size, etc. as \\(n\\) increases at a particular rate depending on \\(F\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#regularity-conditions",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#regularity-conditions",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Regularity conditions",
    "text": "Regularity conditions\nSome cumulants (moments of derivatives of the log likelihood) of extreme value models do not exist.\n\nthe MLE does not solve the score equation if \\(\\widehat{\\xi} \\leq -1\\)\nMLE is not unique for \\(\\xi &lt; -1\\) (some combinations of \\(\\mu\\) and \\(\\sigma\\) yield infinite log likelihood).\n\nrestrict the parameter space to \\(\\{\\boldsymbol{\\theta}: y_1, \\ldots, y_n \\in \\mathrm{supp}(\\boldsymbol{\\theta}), \\xi \\geq -1\\}\\)\nFor GEV, MLE at boundary is \\((\\widehat{\\mu}=\\overline{y}, \\widehat{\\sigma} = \\max(y) - \\overline{y}, \\xi=-1)\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#regularity-conditions-1",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#regularity-conditions-1",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Regularity conditions",
    "text": "Regularity conditions\nIf \\(\\widehat{\\xi} &lt; -1/2\\), cannot evaluate the information matrix.\n\nRegularity assumptions do not apply! reported std. errors are misleading.\nTypically faster convergence, joint limit not asymptotically normal (Smith, 1985).\n\nIn applications, shape is typically close to zero, so authors sometimes restrict \\(\\xi \\in (-0.5, 0.5)\\).\nPenalization of the shape helps ensure that we get reasonable estimates in small samples."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#profile-log-likelihood",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#profile-log-likelihood",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Profile log likelihood",
    "text": "Profile log likelihood\nConsider a functional of interest \\(\\psi\\) and other parameters \\(\\boldsymbol{\\lambda}\\), treated as nuisance.\nWe reparametrize the log likelihood in terms of \\((\\psi, \\boldsymbol{\\lambda})\\) and compute the profile log likelihood \\[\\begin{align*}\n\\ell_{\\mathrm{p}}(\\psi) = \\max_{\\boldsymbol{\\lambda}} \\ell(\\psi, \\boldsymbol{\\lambda})\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#plot-of-profile",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#plot-of-profile",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Plot of profile",
    "text": "Plot of profile\n\nprof &lt;- mev::gev.pll(param = \"Nmean\", dat = ymax, N = 50)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#confidence-intervals",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#confidence-intervals",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nUnder regularity conditions, the likelihood ratio statistic \\[\\begin{align*}\n2 \\{\\ell_{\\mathrm{p}}(\\widehat{\\psi}) - \\ell_{\\mathrm{p}}(\\psi_0)\\} \\stackrel{\\cdot}{\\sim} \\chi^2_1\n\\end{align*}\\] For the hypothesis \\(\\psi = \\psi_0\\), a \\((1-\\alpha)\\) confidence interval based on the profile likelihood ratio test is \\[\\begin{align*}\n\\{\\psi: 2\\{\\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta}_{\\psi})\\} \\leq  \\chi^2_1(1-\\alpha)\\}.\n\\end{align*}\\]\n\n(confint(prof))\n## Estimate Lower CI Upper CI \n##     53.4     47.8     73.6"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#generalized-pareto",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#generalized-pareto",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Generalized Pareto",
    "text": "Generalized Pareto\nIf extremal type theorem applies, then threshold exceedances \\(Y-u \\mid Y&gt;u\\) follow, as \\(u\\) tends to the upper endpoint of \\(F\\), a generalized Pareto distribution.\nThe generalized Pareto distribution is \\[\\begin{align*}\nH(y; \\tau, \\xi) &=\n\\begin{cases}\n1-\\left(1+\\xi {y}/{\\tau}\\right)_{+}^{-1/\\xi}, & \\xi \\neq 0,\\\\ 1-\n\\exp \\left(-{y}/{\\tau}\\right)_{+},& \\xi = 0,\n\\end{cases} \\label{eq:gpdist}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#preprocess-data",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#preprocess-data",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Preprocess data",
    "text": "Preprocess data\n\nChoose a threshold \\(u\\) (either an order statistic or a fixed quantity) and extract exceedances\nUse Grimshaw (1993) algorithm to reduce the 2D optimization problem to a line search.\n\n\nwindlyon &lt;- with(frwind, S2[month(date) &lt;= 4 | month(date) &gt;= 9])\nqulev &lt;- 1-100/nrow(windlyon)\nu &lt;- quantile(windlyon, 1-100/length(windlyon))"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#fitting-the-generalized-pareto-model",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#fitting-the-generalized-pareto-model",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Fitting the generalized Pareto model",
    "text": "Fitting the generalized Pareto model\n\nopt_gp &lt;- mev::fit.gpd(\n  xdat = windlyon, threshold = u, show = TRUE)\n## Method: Grimshaw \n## Log-likelihood: -208 \n## \n## Threshold: 33.8 \n## Number Above: 90 \n## Proportion Above: 0.008 \n## \n## Estimates\n##  scale   shape  \n## 3.5786  0.0309  \n## \n## Standard Errors\n## scale  shape  \n## 0.609  0.134  \n## \n## Optimization Information\n##   Convergence: successful"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#modelling-bulk",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#modelling-bulk",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Modelling bulk",
    "text": "Modelling bulk\nThe generalized Pareto only describes what happens above the threshold, but we can use the empirical distribution below: \\[\\begin{align*}\n\\widehat{\\Pr}(Y_i \\le x) = \\sum_{i=1}^n \\mathsf{I}(Y_i \\le x)/n, \\qquad x \\leq u.\n\\end{align*}\\]\nMany splicing models propose a (semi)parametric model for the bulk; see evmix package for examples"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#binomial---generalized-pareto-model",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#binomial---generalized-pareto-model",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Binomial - generalized Pareto model",
    "text": "Binomial - generalized Pareto model\n\nThe binomial-generalized Pareto model includes a likelihood contribution for \\(\\mathsf{I}(Y_i &gt;u) \\sim \\mathsf{Bin}(1, \\zeta_u)\\), where \\(\\zeta_u = \\Pr(Y_i &gt;u)\\).\nThis third parameter is orthogonal to the others, and there is a closed-form solution for the MLE."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#block-maximum-vs-threshold-exceedances",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#block-maximum-vs-threshold-exceedances",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Block maximum vs threshold exceedances",
    "text": "Block maximum vs threshold exceedances\n\nSuppose we fit a \\(\\mathsf{GP}(\\tau, \\xi)\\) distribution to exceedances above \\(u\\).\nIf there are on average \\(N_y\\) observations per year, the distribution of the \\(N\\)-year maximum conditional on exceeding \\(u\\) is approximately \\(H^{\\zeta_uNN_y}\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#threshold-stability",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#threshold-stability",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Threshold stability",
    "text": "Threshold stability\nMathematical basis for extrapolation.\nIf \\[\\begin{align*}\nY - u \\mid Y&gt;u \\sim \\mathsf{GP}(\\tau, \\xi),\n\\end{align*}\\] then for \\(\\{v &gt;u\\in \\mathbb{R}_{+}: \\tau+\\xi (u-v)&gt;0\\}\\),\n\\[\\begin{align*}\nY-v \\mid Y&gt;v \\sim \\mathsf{GP}\\{\\tau + \\xi (u-v), \\xi\\},\n\\end{align*}\\] and \\(\\zeta_v = \\{1+\\xi(v-u)/\\tau\\}^{-1/\\xi}\\zeta_u\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#threshold-stability-plots",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#threshold-stability-plots",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Threshold stability plots",
    "text": "Threshold stability plots\nAssuming data are exactly generalized Pareto, expect shape parameters to be constant (up to sampling variability).\n\nuseq &lt;- quantile(windlyon, seq(0.9, 0.99, by = 0.01))\ntstab.gpd(windlyon, \n          method = \"profile\",\n          thresh = useq)"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#inhomogeneous-point-process",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#inhomogeneous-point-process",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Inhomogeneous point process",
    "text": "Inhomogeneous point process\nLet \\(Y_i\\) i.i.d. from \\(F\\) with lower endpoint \\(x^*\\).\nConsider \\(a_n&gt;0\\) and \\(b_n \\in \\mathbb{R}\\) such that the distribution of the bidimensional point process \\[\\begin{align*}\nP_n =\\left\\{ \\frac{i}{n}, \\frac{Y_i-b_n}{a_n}, i = 1, \\ldots, n\\right\\}\n\\end{align*}\\] converges to an inhomogeneous Poisson point process on sets of the form \\((a, b) \\times (z, \\infty)\\) for \\(0 \\leq a \\leq b \\leq 1\\) and \\(z&gt;z_*=\\lim_{n \\to \\infty} \\{(x_*-b_n)/a_n\\}\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#intensity-of-inhomogeneous-poisson-process",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#intensity-of-inhomogeneous-poisson-process",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Intensity of inhomogeneous Poisson process",
    "text": "Intensity of inhomogeneous Poisson process\nThe intensity measure of the limiting point process, which gives the expected number of points falling in a set is \\[\\begin{align*}\n&\\Lambda\\{(a, b) \\times (z, \\infty)\\}\n\\\\&\\quad  = (b-a)\\left(1+ \\xi \\frac{z-\\mu}{\\sigma}\\right)_{+}^{-1/\\xi} \\label{eq:pp_conv}\n\\end{align*}\\] for \\(\\xi \\neq 0\\)."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#likelihood-of-the-point-process",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#likelihood-of-the-point-process",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Likelihood of the point process",
    "text": "Likelihood of the point process\n\\[\\begin{align*}\n\\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{y}) &=  (c\\sigma)^{n_u} \\prod_{i=1}^{n_u} \\left(1+\\xi\\frac{y_i-\\mu}{\\sigma}\\right)^{-1-1/\\xi}_{+} \\\\& \\times \\exp\\left\\{- c \\left(1+ \\xi \\frac{u-\\mu}{\\sigma}\\right)^{-1/\\xi}_{+}\\right\\},\n\\end{align*}\\] The constant \\(c\\) is introduced as a way to relate the parameters of the point process likelihood to those of the GEV fitted to blocks of size \\(m\\) observations, e.g., \\(c=n/m\\).\nMoins et al. (2023) propose a orthogonal reparametrization."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#link-between-parametrizations",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#link-between-parametrizations",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Link between parametrizations",
    "text": "Link between parametrizations\nUnder the Poisson approximation to the binomial, the expected number of observations above the threshold is \\[\\begin{align*}\nc \\left\\{1+ \\xi \\left( \\frac{u-\\mu}{\\sigma}\\right)\\right\\}^{-1/\\xi} \\approx n_u. \\end{align*}\\] We can thus relate \\(\\mathsf{GP}(\\tau, \\xi)\\) with Poisson, where MLE is \\[\\begin{align*}\n\\mu_0 & \\approx u - \\sigma_0\\{(n_u/c)^{-\\widehat{\\xi}}-1\\}/\\widehat{\\xi}, \\\\\\sigma_0 &\\approx \\widehat{\\sigma}_u (n_u/c)^{\\widehat{\\xi}}, \\qquad \\xi_0 = \\widehat{\\xi}.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#return-levels",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#return-levels",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "Return levels",
    "text": "Return levels\n\nThe probability \\(p_l\\) that a \\(N\\)-year return level is exceeded \\(l\\) times in \\(N\\) years of independent annual maxima is \\(\\mathsf{Bin}(N, 1/N)\\).\nFor large \\(N\\), a Poisson approximation yields \\(p_0=p_1=0.368\\), \\(p_2=0.184\\), \\(p_3=0.061\\), etc.\n\nThe probability of at least one exceedance over \\(N\\) years is in fact roughly \\(0.63\\).\nThe return level corresponds to the 0.368 quantile of the \\(N\\)-year maximum distribution."
  },
  {
    "objectID": "slides/EVA2023-Rsoftware-likelihood.html#references",
    "href": "slides/EVA2023-Rsoftware-likelihood.html#references",
    "title": "Tutorial on Statistical Computing for Extremes with R",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nGrimshaw, S. D. (1993). Computing maximum likelihood estimates for the generalized Pareto distribution. Technometrics, 35(2), 185–191. https://doi.org/10.1080/00401706.1993.10485040\n\n\nMoins, T., Arbel, J., Girard, S., & Dutfoy, A. (2023). Reparameterization of extreme value framework for improved Bayesian workflow. Computational Statistics and Data Analysis, to appear. https://doi.org/10.48550/ARXIV.2210.05224\n\n\nSmith, R. L. (1985). Maximum likelihood estimation in a class of nonregular cases. Biometrika, 72(1), 67–90. https://doi.org/10.1093/biomet/72.1.67"
  }
]