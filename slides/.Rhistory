y = ymax,
data = data.frame(
syear = scale(1976:2023),
ymax = ymax),
family = "gev",
method = "simulate",
burn = 1000L,
chain = 4L,
iter  = 1.1e4,
proposal.dist = "cauchy",
mu = ~ syear,
verbose = FALSE)
library(texmex)
post_reg <- texmex::evm(
y = ymax,
data = data.frame(
syear = scale(1976:2023),
ymax = ymax),
family = texmex::gev,
method = "simulate",
burn = 1000L,
chain = 4L,
iter  = 1.1e4,
proposal.dist = "cauchy",
mu = ~ syear,
verbose = FALSE)
post_reg
summary(post_reg)
methods(post_reg)
class(post_reg)
methods("evmSim")
coef(post_reg)
plot(post_reg)
library(texmex)
post_reg <- texmex::evm(
y = ymax,
data = data.frame(
syear = scale(1976:2023),
ymax = ymax),
family = texmex::gev,
method = "simulate",
burn = 1000L,
chain = 4L,
iter  = 1.1e4,
proposal.dist = "normal",
mu = ~ syear,
verbose = FALSE)
1.1e4
library(texmex)
post_reg <- texmex::evm(
y = ymax,
data = data.frame(
syear = scale(1976:2023),
ymax = ymax),
family = texmex::gev,
method = "simulate",
burn = 1000L,
chain = 4L,
iter  = 1.1e4,
proposal.dist = "gaussian",
mu = ~ syear,
verbose = FALSE)
summary(post_reg)
par(mfrow = c(3,4)); plot(post_reg)
plot(post_reg, which.plots = 2, chain = 1:4)
plot(post_reg, which.plots = 2, chain = 4)
plot(post_reg, which.plots = 2, chain = 3)
library(texmex)
post_reg <- texmex::evm(
y = ymax,
data = data.frame(
syear = scale(1976:2023),
ymax = ymax),
family = texmex::gev,
method = "simulate",
burn = 1000L,
chains = 4L,
iter  = 1.1e4,
proposal.dist = "gaussian",
mu = ~ syear,
verbose = FALSE)
summary(post_reg)
#| eval: false
# Density, Markov chains and correlograms
plot(post_reg, which.plots = 2, chains = 1:2)
#| eval: false
# Density, Markov chains and correlograms
plot(post_reg, which.plots = 2, chains = 1)
#| eval: false
# Density, Markov chains and correlograms
autoplot(post_reg, which.plots = 2, chains = 1)
library(texmex)
autoplot(post_reg, which.plots = 2, chains = 1)
rlang::last_trace()
library(ggplot2)
#| eval: false
# Density, Markov chains and correlograms
ggplot(post_reg, which.plots = 2, chains = 1)
?grid.arrange
#| eval: false
# Density, Markov chains and correlograms
library(gridExtra)
ggplot(post_reg, which.plots = 2, chains = 1)
post_reg$param
post_reg$chains]
post_reg$chains
test <- mcmc::as.mcmc(post_reg$chains)
install.packages("mcmc")
test <- mcmc::as.mcmc(post_reg$chains)
library(coda)
install.packages("coda")
library(coda)
test <- coda::as.mcmc(post_reg$chains)
test
coda(test)
test <- coda::as.mcmc.list(post_reg$chains)
test <- lapply(post_reg$chains, coda::as.mcmc)
test
plot(test)
coda::as.mcmc.list(test)
test <- coda::as.mcmc.list(test)
plot(test)
coda::traceplot(test)
coda::traceplot(test,smooth = TRUE)
chains <- coda::as.mcmc.list(lapply(post_reg$chains, coda::as.mcmc))
coda::plot.mcmc(chains, density = FALSE)
plot(chains, density = FALSE)
coda::effectiveSize(chains)
# we recommend running this is a fresh R session or restarting your current session
install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
cmdstanr::install_cmdstan()
install.packages(c("loo","psis"))
?loo::
plot(chains, density = FALSE, auto.layout = FALSE)
ggplot(post_reg)
coda::batchSE(x = chains)
coda::acfplot(chains)
coda::gelman.diag(chains)
coda::geweke.diag(chains)
coda::codamenu(chains)
coda::codamenu
coda::codamenu()
mean(chains)
summary(chains)
install.packages("fda")
library(fda)
oldpar <- par(no.readonly=TRUE)
##
## 1.  b-spline
##
# set up the b-spline basis for the lip data, using 23 basis functions,
#   order 4 (cubic), and equally spaced knots.
#  There will be 23 - 4 = 19 interior knots at 0.05, ..., 0.95
lipbasis <- create.bspline.basis(c(0,1), 23)
# plot the basis functions
plot(lipbasis)
oldpar <- par(no.readonly=TRUE)
##
## 1.  b-spline
##
# set up the b-spline basis for the lip data, using 23 basis functions,
#   order 4 (cubic), and equally spaced knots.
#  There will be 23 - 4 = 19 interior knots at 0.05, ..., 0.95
lipbasis <- create.bspline.basis(c(0,1), 23)
# plot the basis functions
plot(lipbasis,axes = NULL)
oldpar <- par(no.readonly=TRUE)
##
## 1.  b-spline
##
# set up the b-spline basis for the lip data, using 23 basis functions,
#   order 4 (cubic), and equally spaced knots.
#  There will be 23 - 4 = 19 interior knots at 0.05, ..., 0.95
lipbasis <- create.bspline.basis(c(0,1), 23)
# plot the basis functions
plot(lipbasis,axes = NULL,knots = FALSE)
lipbasis$basisvalues
lipbasis$values
lipbasis$params
?lipbasis
?create.bspline.basis
install.packages("gratia")
load_mgcv()
library(gratia)
load_mgcv()
df <- data_sim("eg4", n = 400, seed = 42)
bf <- basis(s(x0), data = df)
draw(bf)
bf
bf <- basis(object = s(x0), bs = "tp")
bf <- basis(object = s(x0), bs = "cr")
bf <- basis(object = s(x0, bs = "cr"))
bf <- basis(object = s(x0, bs = "cr"), data = df)
plot(bf)
draw(bf)
draw(bf) + theme_classic()
draw(bf) + ggplot2::theme_classic()
bf <- basis(object = s(x0, bs = "tp"), data = df)
draw(bf) + ggplot2::theme_classic()
load_mgcv()
df <- data_sim("eg4", n = 400, seed = 42)
bf <- basis(s(x0), data = df)
draw(bf)
df
gam <- mgcv::gam(y ~ s(x0), data = df)
draw(gam)
gratia:::draw.basis(gam)
gratia:::draw_concurvity(gam)
install.packages("simstudy")
# Load packages and data
library(mev)
library(xts)
library(lubridate)
data(frwinds, package = "mev")
# Extract time series of mean wind speed for Lyon
lyon <- with(frwind,
xts(x = S2, order.by = date))
# Create series of yearly maximum
ymax <- apply.yearly(lyon, max)
# Fit a GEV distribution to annual max
# Using maximum likelihood
# 'show' = TRUE prints the result to the console
opt_gev <- mev::fit.gev(xdat = ymax, show = TRUE)
# Extract maximum likelihood estimates
mle <- coef(opt_gev)
# check which S3 methods are available
methods(class = "mev_gev")
# Quantile-quantile and probability-probability plots
par(mfrow = c(1,2))
plot(opt_gev)
# Parameter invariance - compute MLE,
# MLE of expectation of maximum of 50 blocks
gev.mle(xdat = ymax, args = "Nmean", N = 50)
# Check that gradient of log likelihood is zero at MLE
mev::gev.score(par = mle, dat = ymax)
# Compute observed information matrix
jmat <- mev::gev.infomat(par = mle, dat = ymax)
# Compute standard errors
sqrt(diag(solve(jmat)))
# Confidence intervals for functional of interest
# via profile likelihood
prof <- mev::gev.pll(
param = "Nmean", # functional of interest
dat = ymax,
N = 50) # number of 'years'
# Obtain confidence intervals based on chi-square (1)
# asymptotic distribution
(confint(prof))
prof2 <- mev::gev.pll(
param = "Nquant",
dat = ymax,
p = 0.5, # probability level (here median)
N = 50) # number of 'years'
prof2 <- mev::gev.pll(
param = "Nquant",
dat = ymax,
q = 0.5, # probability level (here median)
N = 50) # number of 'years'
# Peaks-over-threshold approach
plot(frwind)
summary(prof2)
print(prof2)
confint(prof2)
confint(prof2, print = TRUE)
# Peaks-over-threshold approach
plot(frwind)
# Peaks-over-threshold approach
# First plot the time series
plot(lyon)
plot(x = yday(frwind$date),
y = frwind$S2,
xlab = "day of year",
ylab = "mean wind speed at Lyon (km/h)")
# Extract data for some months to deal with seasonality
windlyon <- with(frwind, S2[month(date) <= 4 | month(date) >= 9])
# Pick a quantile level - there won't be 100 obs due to ties
qulev <- 1-100/nrow(windlyon)
# Extract (interpolated) empirical quantile for threshold
u <- quantile(windlyon, 1-100/length(windlyon))
# Fit a generalized Pareto distribution to exceedances above u
opt_gp <- mev::fit.gpd(
xdat = windlyon,
threshold = u,
show = TRUE)
# In practice, we need a threshold such that we have
# approximate threshold stability (needed for extrapolation)
useq <- quantile(windlyon, seq(0.9, 0.99, by = 0.01))
tstab.gpd(windlyon,
method = "profile",
thresh = useq)
tstab <- tstab.gpd(windlyon,
method = "profile",
thresh = useq)
# Read from right: find lowest threshold such that later estimates
# are included inside confidence intervals
plot(tstab, which = 2)
# Read from right: find lowest threshold such that later estimates
# are included inside confidence intervals
par(mfrow = c(1,1))
plot(tstab, which = 2)
plot(tstab, which = 2, sub = "", main = "")
W.diag(windlyon, u = useq)
W.diag(xdat = windlyon, u = useq)
NC.diag(xdat = windlyon, u = useq)
tstab(opt.pp)
opt.pp <- fit.pp(
xdat = windlyon,
threshold = u,
show = TRUE,
# 'np' = number of periods of data
np = diff(range(lubridate::year(frwind$date))))
gev.Nyr(par = coef(opt.pp),nobs = nobs(opt.pp), type = retlev, p = 1/50)
gev.Nyr(par = coef(opt.pp),
nobs = nobs(opt.pp),
type = "retlev",
p = 1/50)
nobs(opt.pp)
gev.Nyr(par = coef(opt.pp),
nobs = nobs(opt.pp),
type = "retlev",
p = 1/50)
opt.pp
gev.Nyr(par = coef(opt.pp),
nobs = opt.pp$nat, # number above threshold
type = "retlev",
p = 1/50)
# Load packages and data
library(evgam)
library(lubridate)
library(ggplot2)
data(frwind, package = "mev")
# Create time series of wind speed at Lyon
lyon <- with(frwind,
xts::xts(x = S2, order.by = date))
# Extract annual maximum
ymax <- xts::apply.yearly(lyon, max)
ymax <- ymax[-length(ymax)] # Remove 2023 (incomplete year)
# Fit a GEV model using 'evgam'
# cubic spline for location based on year
# constant scale and shape
opt_gev <- evgam::evgam(
# data frame with response and covariates
data = data.frame(
year = year(ymax),
ymax = ymax),
# formula is list of formulae for loc, scale, shape
formula = list(ymax ~ s(year, bs = "cr"),
~ 1,
~ 1),
family = "gev") # extreme value distribution
# see ?evgam::evgam
## Summary with coefficient estimates
summary(opt_gev)
# plot of fitted spline for location parameter
# of generalized extreme value distribution as
# a function of day of year (scaled to unit interval)."
plot(opt_gev)
# Careful! - default is type='link' (log scale)
# Response here is mu, sigma, xi
# Based on normal approximation to posterior
sim_post <- simulate(object = opt_gev,
nsim = 1000L,
type = "response")
# Simulated parameter for each combo
# averaged across observed values of
# covariates in original data frame.
retlev <- with(sim_post,
evgam::qev(p = 1-1/50,
loc = location,
scale = scale,
shape = shape,
family = "gev"))
# Compute point estimate
mean(retlev)
# More straightforwardly, simulate directly quantiles
mean(simulate(opt_gev,
nsim = 1000L,
type = "quantile",
probs = 1-1/50))
# Another model, this time restricting the number of knots
opt_gev_spl <- evgam::evgam(
data = data.frame(
syear = scale(years),
ymax = ymax),
# k controls the knots
formula = list(ymax ~ s(syear, k = 5, bs = "cr"),
~ 1,
~ 1),
family = "gev")
# Load packages and data
library(evgam)
library(lubridate)
library(ggplot2)
data(frwind, package = "mev")
# Create time series of wind speed at Lyon
lyon <- with(frwind,
xts::xts(x = S2, order.by = date))
# Extract annual maximum
ymax <- xts::apply.yearly(lyon, max)
ymax <- ymax[-length(ymax)] # Remove 2023 (incomplete year)
# Fit a GEV model using 'evgam'
# cubic spline for location based on year
# constant scale and shape
opt_gev <- evgam::evgam(
# data frame with response and covariates
data = data.frame(
year = year(ymax),
ymax = ymax),
# formula is list of formulae for loc, scale, shape
formula = list(ymax ~ s(year, bs = "cr"),
~ 1,
~ 1),
family = "gev") # extreme value distribution
# see ?evgam::evgam
## Summary with coefficient estimates
summary(opt_gev)
# plot of fitted spline for location parameter
# of generalized extreme value distribution as
# a function of day of year (scaled to unit interval)."
plot(opt_gev)
# Careful! - default is type='link' (log scale)
# Response here is mu, sigma, xi
# Based on normal approximation to posterior
sim_post <- simulate(object = opt_gev,
nsim = 1000L,
type = "response")
# Simulated parameter for each combo
# averaged across observed values of
# covariates in original data frame.
retlev <- with(sim_post,
evgam::qev(p = 1-1/50,
loc = location,
scale = scale,
shape = shape,
family = "gev"))
# Compute point estimate
mean(retlev)
# More straightforwardly, simulate directly quantiles
mean(simulate(opt_gev,
nsim = 1000L,
type = "quantile",
probs = 1-1/50))
# Another model, this time restricting the number of knots
opt_gev_spl <- evgam::evgam(
data = data.frame(
syear = scale(year(ymax)),
ymax = ymax),
# k controls the knots
formula = list(ymax ~ s(syear, k = 5, bs = "cr"),
~ 1,
~ 1),
family = "gev")
## Summary with coefficients
post_sim <- simulate(opt_gev_spl, nsim = 1000L, seed = 2023)
# Plot posterior of location function
# note the varying uncertainty as we move
# towards higher mean value
ggplot(
data = data.frame(
location = c(post_sim$location),
year = factor(rep(1:length(years),
length.out = prod(dim(post_sim$location))))),
mapping = aes(x = location,
color = year,
group = year)) +
geom_density() +
theme_minimal() +
viridis::scale_color_viridis(discrete = TRUE) +
theme(legend.position = "none")
## Fit a nonstationary threshold using
# asymmetric Laplace distribution to get
# a working likelihood with a modified check function
qreg <- evgam::evgam(
# Cubic cyclic splines
formula = list(wind ~ s(tday, bs = "cc"),
~ s(tday, bs = "cc")),
data = data.frame(wind = as.numeric(lyon),
tday = yday(lyon)),
family = "ald",
ald.args = list(tau = 0.95)) # quantile level
plot(qreg)
# Now extract the fitted quantiles (correspond to loc)
# to use as threshold
u <- fitted(qreg)[,'location']
# Create a data frame containing
# exceedance, thresholds and covariates
df <- data.frame(exc = as.numeric(lyon - u),
yday = yday(lyon),
u = u)
# Extract exceedances from the latter
df <- df[lyon >u, ]
# Fit a generalized Pareto distribution
opt_gpd <- evgam::evgam(
formula = list(exc ~ 1, ~1), # scale shape formulaes - constant
data = df,
family = "gpd")
# Simulate parameters of generalized Pareto
post_sim <- simulate(opt_gpd,
type = "response")
# Compute average number of exceedances per year
npy <- diff(range(date(lyon))) / nrow(df)
# Compute return levels - must be high enough
# for empirical probability component to be zero
retlev_gpd <- qev(
p = 1-1/50, # quantile level for return level
m = as.numeric(npy), # number of observations per year (average)
loc = df$u, # threshold
scale = post_sim$scale,
shape = post_sim$shape,
family = "gpd")
# Compute average return level
mean(retlev_gpd)
